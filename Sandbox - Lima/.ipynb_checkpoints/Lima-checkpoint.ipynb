{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Challenge @ ITA 2022</font>\n",
    "# <font color='blue'>Equipe DIOMGIS</font>\n",
    "\n",
    "## <font color='blue'>Fase 1</font>\n",
    "\n",
    "### <font color='blue'>TEMA DO DESAFIO</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](..\\data\\image\\logo.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "#!pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "#!pip install -q -U watermark\n",
    "\n",
    "# Instala o pacote tensorboard-plugin-profile. \n",
    "# Esse pacote é usado para incrementar funcioalidades no Tensorboard.\n",
    "#!pip install -U tensorboard-plugin-profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau \n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 25\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Equipe DIOMGIS\n",
      "\n",
      "seaborn   : 0.11.2\n",
      "tensorflow: 2.10.0\n",
      "matplotlib: 3.5.1\n",
      "pandas    : 1.4.2\n",
      "numpy     : 1.22.3\n",
      "keras     : 2.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Equipe DIOMGIS\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Confirme se o TensorFlow pode acessar a GPU.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  9 13:40:43 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.94       Driver Version: 516.94       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:65:00.0  On |                  N/A |\n",
      "|  0%   50C    P2    30W / 220W |   1022MiB /  8192MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1876    C+G   ...txyewy\\MiniSearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      2788    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A      3736    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A      5192    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A      5560    C+G   ...370.37\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      5608    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9672    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9688    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10308    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     10812    C+G   ...370.34\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     11320    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12376    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     13160    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     13828      C   ...ucas\\anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     15180    C+G   ...x64__pc75e8sa7ep4e\\XD.exe    N/A      |\n",
      "|    0   N/A  N/A     15548    C+G   ...o Webcam\\GoPro Webcam.exe    N/A      |\n",
      "|    0   N/A  N/A     16676    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     20476    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     20808    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     22548    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     23520    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     26148    C+G   ...\\app-1.0.9006\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     27372    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados sintéticos\n",
    "size_sample = 200000\n",
    "\n",
    "# Dados de Treino e teste\n",
    "# x\n",
    "x1 = np.random.randint(0, 100, size_sample)\n",
    "x2 = np.random.randint(0, 100, size_sample)\n",
    "x_treino = np.dstack((x1, x2))[0]\n",
    "\n",
    "# y\n",
    "y_treino = 3*(x1**(1/2)) + 2*(x2**2)\n",
    "\n",
    "# Dados de Validação\n",
    "# x\n",
    "x1 = np.random.randint(0, 100, int(0.1 * size_sample))\n",
    "x2 = np.random.randint(0, 100, int(0.1 * size_sample))\n",
    "x_teste = np.dstack((x1, x2))[0]\n",
    "\n",
    "# y\n",
    "y_teste = 3*(x1**(1/2)) + 2*(x2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "batch_size = 128\n",
    "nKFold = 5\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer,\n",
    "                 n_dense1,\n",
    "                 n_dense2,\n",
    "                 n_dense3,\n",
    "                 activation1,\n",
    "                 activation2,\n",
    "                 activation3,\n",
    "                 dropout):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_dense1, input_shape = (2,) , activation = activation1))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_dense2, activation = activation2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_dense3, activation = activation3))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[\"mse\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = \"saveModel/bestModel\", \n",
    "                             monitor='val_mse',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             verbose = verbose)\n",
    "    \n",
    "tensorboard_callback = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mse',\n",
    "                              min_delta=0,\n",
    "                              patience=20,\n",
    "                              verbose = verbose,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mse',\n",
    "                              factor=0.25,\n",
    "                              patience=5,\n",
    "                              mode=\"min\",\n",
    "                              verbose = verbose,\n",
    "                              min_delta=0.00001)\n",
    "\n",
    "callbacks = [tensorboard_callback, earlystop, reduce_lr] # checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "model = KerasRegressor(build_fn = create_model,\n",
    "                        verbose = verbose,\n",
    "                        callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "steps = [(\"model\", model)]\n",
    "\n",
    "estimator = Pipeline(steps, verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos parametros (GridSearch)\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "opt_SGD = SGD(\n",
    "    learning_rate = learning_rate,\n",
    "    momentum = 0.0,\n",
    "    nesterov = False)\n",
    "\n",
    "opt_RMSprop = RMSprop(\n",
    "    learning_rate = learning_rate,\n",
    "    rho = 0.9,\n",
    "    momentum = 0.0,\n",
    "    epsilon = 1e-07,\n",
    "    centered = False)\n",
    "\n",
    "opt_Adam = Adam(\n",
    "    learning_rate = learning_rate,\n",
    "    beta_1 = 0.9,\n",
    "    beta_2 = 0.999,\n",
    "    epsilon = 1e-07,\n",
    "    amsgrad = False)\n",
    "\n",
    "opt_Adadelta = Adadelta(\n",
    "    learning_rate = learning_rate,\n",
    "    rho = 0.95,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Adagrad = Adagrad(\n",
    "    learning_rate = learning_rate,\n",
    "    initial_accumulator_value = 0.1,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Adamax = Adamax(\n",
    "    learning_rate = learning_rate,\n",
    "    beta_1 = 0.9,\n",
    "    beta_2 = 0.999,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Nadam = Nadam(\n",
    "    learning_rate = learning_rate,\n",
    "    beta_1 = 0.9,\n",
    "    beta_2 = 0.999,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Ftrl = Ftrl(\n",
    "    learning_rate = learning_rate,\n",
    "    learning_rate_power = -0.5,\n",
    "    initial_accumulator_value = 0.1,\n",
    "    l1_regularization_strength = 0.0,\n",
    "    l2_regularization_strength = 0.0,\n",
    "    l2_shrinkage_regularization_strength = 0.0,\n",
    "    beta = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outros parametros\n",
    "params_grid = {\n",
    "    'model__optimizer': [opt_SGD, opt_RMSprop, opt_Adam, opt_Adadelta, opt_Adagrad, opt_Adamax, opt_Nadam, opt_Ftrl],\n",
    "    'model__n_dense1': [128],\n",
    "    'model__n_dense2': [128],\n",
    "    'model__n_dense3': [128], \n",
    "    'model__activation1': ['relu'],\n",
    "    'model__activation2': ['relu'],\n",
    "    'model__activation3': ['relu'],\n",
    "    'model__dropout': [0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator = estimator,  \n",
    "                    verbose = verbose,\n",
    "                    return_train_score = True,\n",
    "                    cv = nKFold,\n",
    "                    param_grid = params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13440), started 1 day, 2:24:26 ago. (Use '!kill 13440' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d91dad2ab8f9a324\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d91dad2ab8f9a324\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monitoramento de Otimização\n",
    "%tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Epoch 1/300\n",
      "1250/1250 - 5s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 3ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19/300\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  57.7s\n",
      "1250/1250 - 2s - loss: nan - mse: nan - 2s/epoch - 2ms/step\n",
      "5000/5000 - 10s - loss: nan - mse: nan - 10s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x00000125F19B0E20>; total time= 1.0min\n",
      "Epoch 1/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19/300\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  55.0s\n",
      "1250/1250 - 2s - loss: nan - mse: nan - 2s/epoch - 2ms/step\n",
      "5000/5000 - 9s - loss: nan - mse: nan - 9s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x00000125F19B0E20>; total time=  57.2s\n",
      "Epoch 1/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19/300\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  54.3s\n",
      "1250/1250 - 2s - loss: nan - mse: nan - 2s/epoch - 2ms/step\n",
      "5000/5000 - 8s - loss: nan - mse: nan - 8s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x00000125F19B0E20>; total time=  56.3s\n",
      "Epoch 1/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19/300\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  52.3s\n",
      "1250/1250 - 2s - loss: nan - mse: nan - 2s/epoch - 2ms/step\n",
      "5000/5000 - 9s - loss: nan - mse: nan - 9s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x00000125F19B0E20>; total time=  54.4s\n",
      "Epoch 1/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 6.2500e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19/300\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1250/1250 - 3s - loss: nan - mse: nan - val_loss: nan - val_mse: nan - lr: 1.5625e-04 - 3s/epoch - 2ms/step\n",
      "Epoch 19: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  52.3s\n",
      "1250/1250 - 2s - loss: nan - mse: nan - 2s/epoch - 2ms/step\n",
      "5000/5000 - 9s - loss: nan - mse: nan - 9s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x00000125F19B0E20>; total time=  54.4s\n",
      "Epoch 1/300\n",
      "1250/1250 - 5s - loss: 3095620.2500 - mse: 3095620.2500 - val_loss: 1021225.6875 - val_mse: 1021225.6875 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 4s - loss: 1488805.5000 - mse: 1488805.5000 - val_loss: 398722.7500 - val_mse: 398722.7500 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 4s - loss: 1065382.5000 - mse: 1065382.5000 - val_loss: 674609.0625 - val_mse: 674609.0625 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 4s - loss: 848735.6250 - mse: 848735.6250 - val_loss: 1348849.1250 - val_mse: 1348849.1250 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 5/300\n",
      "1250/1250 - 4s - loss: 709859.5625 - mse: 709859.5625 - val_loss: 200954.5469 - val_mse: 200954.5469 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 4s - loss: 603549.4375 - mse: 603549.4375 - val_loss: 1333554.1250 - val_mse: 1333554.1250 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 4s - loss: 525634.8125 - mse: 525634.8125 - val_loss: 1111951.0000 - val_mse: 1111951.0000 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 4s - loss: 462347.8750 - mse: 462347.8750 - val_loss: 471209.9375 - val_mse: 471209.9375 - lr: 0.0100 - 4s/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300\n",
      "1250/1250 - 4s - loss: 415071.7812 - mse: 415071.7812 - val_loss: 92213.7656 - val_mse: 92213.7656 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 10/300\n",
      "1250/1250 - 4s - loss: 370678.6562 - mse: 370678.6562 - val_loss: 492271.9375 - val_mse: 492271.9375 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 4s - loss: 336490.3125 - mse: 336490.3125 - val_loss: 41498.0273 - val_mse: 41498.0273 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 5s - loss: 309095.4375 - mse: 309095.4375 - val_loss: 165245.6719 - val_mse: 165245.6719 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 4s - loss: 291805.5312 - mse: 291805.5312 - val_loss: 260129.0312 - val_mse: 260129.0312 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 4s - loss: 270839.8125 - mse: 270839.8125 - val_loss: 173370.5781 - val_mse: 173370.5781 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 15/300\n",
      "1250/1250 - 4s - loss: 248916.0469 - mse: 248916.0469 - val_loss: 189353.5000 - val_mse: 189353.5000 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 4s - loss: 236166.7344 - mse: 236166.7344 - val_loss: 879429.9375 - val_mse: 879429.9375 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 4s - loss: 19268.2227 - mse: 19268.2227 - val_loss: 15439.5537 - val_mse: 15439.5537 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 4s - loss: 17882.8594 - mse: 17882.8594 - val_loss: 894.6043 - val_mse: 894.6043 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 19/300\n",
      "1250/1250 - 4s - loss: 17699.7930 - mse: 17699.7930 - val_loss: 5578.4834 - val_mse: 5578.4834 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 20/300\n",
      "1250/1250 - 4s - loss: 17727.5625 - mse: 17727.5625 - val_loss: 2040.1147 - val_mse: 2040.1147 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 21/300\n",
      "1250/1250 - 4s - loss: 17447.0664 - mse: 17447.0664 - val_loss: 1267.8188 - val_mse: 1267.8188 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 22/300\n",
      "1250/1250 - 4s - loss: 17363.9238 - mse: 17363.9238 - val_loss: 74738.4375 - val_mse: 74738.4375 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 4s - loss: 17193.8008 - mse: 17193.8008 - val_loss: 40779.3086 - val_mse: 40779.3086 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 24/300\n",
      "1250/1250 - 4s - loss: 1351.6366 - mse: 1351.6366 - val_loss: 508.2488 - val_mse: 508.2488 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 25/300\n",
      "1250/1250 - 4s - loss: 1295.3610 - mse: 1295.3610 - val_loss: 320.2686 - val_mse: 320.2686 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 26/300\n",
      "1250/1250 - 4s - loss: 1282.9073 - mse: 1282.9073 - val_loss: 1036.2563 - val_mse: 1036.2563 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 27/300\n",
      "1250/1250 - 4s - loss: 1282.7607 - mse: 1282.7607 - val_loss: 3432.2920 - val_mse: 3432.2920 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 28/300\n",
      "1250/1250 - 4s - loss: 1222.0449 - mse: 1222.0449 - val_loss: 1918.7303 - val_mse: 1918.7303 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 29/300\n",
      "1250/1250 - 4s - loss: 1215.8158 - mse: 1215.8158 - val_loss: 2926.3423 - val_mse: 2926.3423 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 4s - loss: 1213.1956 - mse: 1213.1956 - val_loss: 4777.9370 - val_mse: 4777.9370 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 31/300\n",
      "1250/1250 - 4s - loss: 194.3574 - mse: 194.3574 - val_loss: 117.9234 - val_mse: 117.9234 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 32/300\n",
      "1250/1250 - 4s - loss: 185.6166 - mse: 185.6166 - val_loss: 138.2779 - val_mse: 138.2779 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 33/300\n",
      "1250/1250 - 4s - loss: 183.6871 - mse: 183.6871 - val_loss: 141.9610 - val_mse: 141.9610 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 34/300\n",
      "1250/1250 - 4s - loss: 176.3679 - mse: 176.3679 - val_loss: 259.2596 - val_mse: 259.2596 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 35/300\n",
      "1250/1250 - 4s - loss: 169.1159 - mse: 169.1159 - val_loss: 214.7152 - val_mse: 214.7152 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "1250/1250 - 4s - loss: 168.1015 - mse: 168.1015 - val_loss: 143.8722 - val_mse: 143.8722 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 37/300\n",
      "1250/1250 - 4s - loss: 102.7208 - mse: 102.7208 - val_loss: 104.0985 - val_mse: 104.0985 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 38/300\n",
      "1250/1250 - 4s - loss: 102.5214 - mse: 102.5214 - val_loss: 101.0596 - val_mse: 101.0596 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 39/300\n",
      "1250/1250 - 4s - loss: 101.4766 - mse: 101.4766 - val_loss: 95.9870 - val_mse: 95.9870 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 40/300\n",
      "1250/1250 - 4s - loss: 100.4462 - mse: 100.4462 - val_loss: 96.6942 - val_mse: 96.6942 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 41/300\n",
      "1250/1250 - 4s - loss: 99.8118 - mse: 99.8118 - val_loss: 96.1230 - val_mse: 96.1230 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 42/300\n",
      "1250/1250 - 4s - loss: 99.7791 - mse: 99.7791 - val_loss: 95.7379 - val_mse: 95.7379 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 43/300\n",
      "1250/1250 - 4s - loss: 98.5396 - mse: 98.5396 - val_loss: 99.6697 - val_mse: 99.6697 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 44/300\n",
      "1250/1250 - 4s - loss: 97.7023 - mse: 97.7023 - val_loss: 95.3738 - val_mse: 95.3738 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 45/300\n",
      "1250/1250 - 4s - loss: 98.4535 - mse: 98.4535 - val_loss: 104.0568 - val_mse: 104.0568 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 46/300\n",
      "1250/1250 - 4s - loss: 97.5231 - mse: 97.5231 - val_loss: 100.2311 - val_mse: 100.2311 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 47/300\n",
      "1250/1250 - 4s - loss: 96.4322 - mse: 96.4322 - val_loss: 114.1617 - val_mse: 114.1617 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 48/300\n",
      "1250/1250 - 4s - loss: 96.4425 - mse: 96.4425 - val_loss: 92.4868 - val_mse: 92.4868 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 49/300\n",
      "1250/1250 - 4s - loss: 95.8763 - mse: 95.8763 - val_loss: 92.5434 - val_mse: 92.5434 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 50/300\n",
      "1250/1250 - 4s - loss: 94.9199 - mse: 94.9199 - val_loss: 105.1443 - val_mse: 105.1443 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 51/300\n",
      "1250/1250 - 4s - loss: 94.1632 - mse: 94.1632 - val_loss: 92.9425 - val_mse: 92.9425 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 52/300\n",
      "1250/1250 - 4s - loss: 92.8236 - mse: 92.8236 - val_loss: 88.5501 - val_mse: 88.5501 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 53/300\n",
      "1250/1250 - 4s - loss: 93.4019 - mse: 93.4019 - val_loss: 89.1297 - val_mse: 89.1297 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 54/300\n",
      "1250/1250 - 4s - loss: 93.5478 - mse: 93.5478 - val_loss: 95.5210 - val_mse: 95.5210 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 55/300\n",
      "1250/1250 - 4s - loss: 92.5091 - mse: 92.5091 - val_loss: 95.0328 - val_mse: 95.0328 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 56/300\n",
      "1250/1250 - 4s - loss: 91.9229 - mse: 91.9229 - val_loss: 89.5172 - val_mse: 89.5172 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 57/300\n",
      "1250/1250 - 4s - loss: 90.5416 - mse: 90.5416 - val_loss: 86.7716 - val_mse: 86.7716 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 58/300\n",
      "1250/1250 - 4s - loss: 90.1973 - mse: 90.1973 - val_loss: 86.7206 - val_mse: 86.7206 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 59/300\n",
      "1250/1250 - 4s - loss: 90.3755 - mse: 90.3755 - val_loss: 88.1500 - val_mse: 88.1500 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 60/300\n",
      "1250/1250 - 4s - loss: 89.6505 - mse: 89.6505 - val_loss: 92.0092 - val_mse: 92.0092 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 61/300\n",
      "1250/1250 - 4s - loss: 89.0984 - mse: 89.0984 - val_loss: 84.9302 - val_mse: 84.9302 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 62/300\n",
      "1250/1250 - 4s - loss: 88.7720 - mse: 88.7720 - val_loss: 100.2527 - val_mse: 100.2527 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 63/300\n",
      "1250/1250 - 4s - loss: 88.7801 - mse: 88.7801 - val_loss: 84.5502 - val_mse: 84.5502 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 64/300\n",
      "1250/1250 - 4s - loss: 87.7700 - mse: 87.7700 - val_loss: 95.2643 - val_mse: 95.2643 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300\n",
      "1250/1250 - 4s - loss: 87.8524 - mse: 87.8524 - val_loss: 84.5637 - val_mse: 84.5637 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 66/300\n",
      "1250/1250 - 4s - loss: 87.5938 - mse: 87.5938 - val_loss: 96.2955 - val_mse: 96.2955 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 67/300\n",
      "1250/1250 - 4s - loss: 86.4829 - mse: 86.4829 - val_loss: 86.2460 - val_mse: 86.2460 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "1250/1250 - 4s - loss: 87.9364 - mse: 87.9364 - val_loss: 105.6187 - val_mse: 105.6187 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 69/300\n",
      "1250/1250 - 4s - loss: 82.4309 - mse: 82.4309 - val_loss: 81.4288 - val_mse: 81.4288 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 70/300\n",
      "1250/1250 - 4s - loss: 81.8087 - mse: 81.8087 - val_loss: 81.5376 - val_mse: 81.5376 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 71/300\n",
      "1250/1250 - 5s - loss: 81.5277 - mse: 81.5277 - val_loss: 81.0568 - val_mse: 81.0568 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 72/300\n",
      "1250/1250 - 4s - loss: 81.3535 - mse: 81.3535 - val_loss: 83.3381 - val_mse: 83.3381 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 73/300\n",
      "1250/1250 - 4s - loss: 81.3980 - mse: 81.3980 - val_loss: 84.6916 - val_mse: 84.6916 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 74/300\n",
      "1250/1250 - 4s - loss: 81.2293 - mse: 81.2293 - val_loss: 81.7731 - val_mse: 81.7731 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 75/300\n",
      "1250/1250 - 4s - loss: 81.0652 - mse: 81.0652 - val_loss: 81.5327 - val_mse: 81.5327 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 76/300\n",
      "1250/1250 - 4s - loss: 81.0384 - mse: 81.0384 - val_loss: 80.1079 - val_mse: 80.1079 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 77/300\n",
      "1250/1250 - 4s - loss: 80.7368 - mse: 80.7368 - val_loss: 82.2937 - val_mse: 82.2937 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 78/300\n",
      "1250/1250 - 4s - loss: 80.9917 - mse: 80.9917 - val_loss: 80.5063 - val_mse: 80.5063 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 79/300\n",
      "1250/1250 - 4s - loss: 80.6776 - mse: 80.6776 - val_loss: 82.4064 - val_mse: 82.4064 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 80/300\n",
      "1250/1250 - 4s - loss: 79.9594 - mse: 79.9594 - val_loss: 78.6580 - val_mse: 78.6580 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 81/300\n",
      "1250/1250 - 4s - loss: 79.5504 - mse: 79.5504 - val_loss: 79.2073 - val_mse: 79.2073 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 82/300\n",
      "1250/1250 - 4s - loss: 79.5975 - mse: 79.5975 - val_loss: 79.7841 - val_mse: 79.7841 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 83/300\n",
      "1250/1250 - 4s - loss: 79.8287 - mse: 79.8287 - val_loss: 79.6575 - val_mse: 79.6575 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 84/300\n",
      "1250/1250 - 4s - loss: 79.4796 - mse: 79.4796 - val_loss: 82.7682 - val_mse: 82.7682 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "1250/1250 - 4s - loss: 79.4774 - mse: 79.4774 - val_loss: 80.0986 - val_mse: 80.0986 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 86/300\n",
      "1250/1250 - 4s - loss: 78.7772 - mse: 78.7772 - val_loss: 79.1727 - val_mse: 79.1727 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 87/300\n",
      "1250/1250 - 4s - loss: 79.0617 - mse: 79.0617 - val_loss: 79.4400 - val_mse: 79.4400 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 88/300\n",
      "1250/1250 - 4s - loss: 79.2927 - mse: 79.2927 - val_loss: 79.4107 - val_mse: 79.4107 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 89/300\n",
      "1250/1250 - 4s - loss: 79.3410 - mse: 79.3410 - val_loss: 79.8556 - val_mse: 79.8556 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "1250/1250 - 4s - loss: 79.2502 - mse: 79.2502 - val_loss: 79.5676 - val_mse: 79.5676 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 91/300\n",
      "1250/1250 - 4s - loss: 79.0741 - mse: 79.0741 - val_loss: 79.2087 - val_mse: 79.2087 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 92/300\n",
      "1250/1250 - 4s - loss: 79.0133 - mse: 79.0133 - val_loss: 79.1887 - val_mse: 79.1887 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 93/300\n",
      "1250/1250 - 4s - loss: 78.9896 - mse: 78.9896 - val_loss: 79.3870 - val_mse: 79.3870 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 94/300\n",
      "1250/1250 - 4s - loss: 78.9898 - mse: 78.9898 - val_loss: 79.2878 - val_mse: 79.2878 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "1250/1250 - 4s - loss: 78.9335 - mse: 78.9335 - val_loss: 79.3882 - val_mse: 79.3882 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 96/300\n",
      "1250/1250 - 4s - loss: 78.9033 - mse: 78.9033 - val_loss: 79.0984 - val_mse: 79.0984 - lr: 1.5259e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 97/300\n",
      "1250/1250 - 4s - loss: 78.8669 - mse: 78.8669 - val_loss: 79.1464 - val_mse: 79.1464 - lr: 1.5259e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 98/300\n",
      "1250/1250 - 4s - loss: 78.9029 - mse: 78.9029 - val_loss: 79.1066 - val_mse: 79.1066 - lr: 1.5259e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 99/300\n",
      "Restoring model weights from the end of the best epoch: 80.\n",
      "1250/1250 - 4s - loss: 78.9093 - mse: 78.9093 - val_loss: 79.1884 - val_mse: 79.1884 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 99: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 7.3min\n",
      "1250/1250 - 2s - loss: 81.9994 - mse: 81.9994 - 2s/epoch - 2ms/step\n",
      "5000/5000 - 8s - loss: 79.5106 - mse: 79.5106 - 8s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x00000125F19B0760>; total time= 7.3min\n",
      "Epoch 1/300\n",
      "1250/1250 - 5s - loss: 3246781.7500 - mse: 3246781.7500 - val_loss: 5786259.5000 - val_mse: 5786259.5000 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 4s - loss: 1527082.7500 - mse: 1527082.7500 - val_loss: 356101.1562 - val_mse: 356101.1562 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 4s - loss: 1099227.5000 - mse: 1099227.5000 - val_loss: 2786164.0000 - val_mse: 2786164.0000 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 4s - loss: 851924.1250 - mse: 851924.1250 - val_loss: 171818.2188 - val_mse: 171818.2188 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 5/300\n",
      "1250/1250 - 4s - loss: 677889.6250 - mse: 677889.6250 - val_loss: 164483.8906 - val_mse: 164483.8906 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 4s - loss: 555065.1250 - mse: 555065.1250 - val_loss: 818371.1250 - val_mse: 818371.1250 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 4s - loss: 486200.7812 - mse: 486200.7812 - val_loss: 205921.7188 - val_mse: 205921.7188 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 4s - loss: 430530.3438 - mse: 430530.3438 - val_loss: 400849.9688 - val_mse: 400849.9688 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 4s - loss: 388727.4688 - mse: 388727.4688 - val_loss: 368863.5312 - val_mse: 368863.5312 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 10/300\n",
      "1250/1250 - 4s - loss: 351296.1250 - mse: 351296.1250 - val_loss: 130178.3906 - val_mse: 130178.3906 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 4s - loss: 329893.9688 - mse: 329893.9688 - val_loss: 660939.2500 - val_mse: 660939.2500 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 4s - loss: 306523.5625 - mse: 306523.5625 - val_loss: 279792.3438 - val_mse: 279792.3438 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 4s - loss: 280592.8750 - mse: 280592.8750 - val_loss: 27217.3223 - val_mse: 27217.3223 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 4s - loss: 260793.5000 - mse: 260793.5000 - val_loss: 43920.6719 - val_mse: 43920.6719 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 15/300\n",
      "1250/1250 - 4s - loss: 247317.4062 - mse: 247317.4062 - val_loss: 653100.3125 - val_mse: 653100.3125 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 4s - loss: 234725.9375 - mse: 234725.9375 - val_loss: 424370.2188 - val_mse: 424370.2188 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 4s - loss: 227719.9375 - mse: 227719.9375 - val_loss: 214276.5781 - val_mse: 214276.5781 - lr: 0.0100 - 4s/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 4s - loss: 216131.6562 - mse: 216131.6562 - val_loss: 236883.2031 - val_mse: 236883.2031 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 19/300\n",
      "1250/1250 - 4s - loss: 16968.1777 - mse: 16968.1777 - val_loss: 19796.0547 - val_mse: 19796.0547 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 20/300\n",
      "1250/1250 - 4s - loss: 16165.8398 - mse: 16165.8398 - val_loss: 19251.5703 - val_mse: 19251.5703 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 21/300\n",
      "1250/1250 - 4s - loss: 15971.0254 - mse: 15971.0254 - val_loss: 16174.4414 - val_mse: 16174.4414 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 22/300\n",
      "1250/1250 - 4s - loss: 15728.1006 - mse: 15728.1006 - val_loss: 18703.8984 - val_mse: 18703.8984 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 23/300\n",
      "1250/1250 - 4s - loss: 15702.0430 - mse: 15702.0430 - val_loss: 3893.9836 - val_mse: 3893.9836 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 24/300\n",
      "1250/1250 - 4s - loss: 15797.1533 - mse: 15797.1533 - val_loss: 3862.6929 - val_mse: 3862.6929 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 25/300\n",
      "1250/1250 - 4s - loss: 15668.9326 - mse: 15668.9326 - val_loss: 7101.1895 - val_mse: 7101.1895 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 26/300\n",
      "1250/1250 - 4s - loss: 15462.5234 - mse: 15462.5234 - val_loss: 3384.7148 - val_mse: 3384.7148 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 27/300\n",
      "1250/1250 - 4s - loss: 15500.2100 - mse: 15500.2100 - val_loss: 27563.1543 - val_mse: 27563.1543 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 28/300\n",
      "1250/1250 - 4s - loss: 15352.2676 - mse: 15352.2676 - val_loss: 4118.8838 - val_mse: 4118.8838 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 29/300\n",
      "1250/1250 - 4s - loss: 15245.7793 - mse: 15245.7793 - val_loss: 18309.6309 - val_mse: 18309.6309 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 30/300\n",
      "1250/1250 - 4s - loss: 15336.5566 - mse: 15336.5566 - val_loss: 15413.8145 - val_mse: 15413.8145 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 31/300\n",
      "1250/1250 - 4s - loss: 15187.7422 - mse: 15187.7422 - val_loss: 1369.0991 - val_mse: 1369.0991 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 32/300\n",
      "1250/1250 - 4s - loss: 15179.4229 - mse: 15179.4229 - val_loss: 3749.0508 - val_mse: 3749.0508 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 33/300\n",
      "1250/1250 - 4s - loss: 14972.1250 - mse: 14972.1250 - val_loss: 28977.1816 - val_mse: 28977.1816 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 34/300\n",
      "1250/1250 - 4s - loss: 15132.9395 - mse: 15132.9395 - val_loss: 42867.8867 - val_mse: 42867.8867 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 35/300\n",
      "1250/1250 - 4s - loss: 14899.5967 - mse: 14899.5967 - val_loss: 4103.2617 - val_mse: 4103.2617 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 4s - loss: 14848.7061 - mse: 14848.7061 - val_loss: 5203.7095 - val_mse: 5203.7095 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 37/300\n",
      "1250/1250 - 4s - loss: 1033.6962 - mse: 1033.6962 - val_loss: 637.6436 - val_mse: 637.6436 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 38/300\n",
      "1250/1250 - 4s - loss: 1014.6587 - mse: 1014.6587 - val_loss: 2190.4165 - val_mse: 2190.4165 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 39/300\n",
      "1250/1250 - 4s - loss: 1014.9286 - mse: 1014.9286 - val_loss: 2076.9128 - val_mse: 2076.9128 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 40/300\n",
      "1250/1250 - 4s - loss: 999.8752 - mse: 999.8752 - val_loss: 69.7538 - val_mse: 69.7538 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 41/300\n",
      "1250/1250 - 4s - loss: 986.9157 - mse: 986.9157 - val_loss: 3238.6147 - val_mse: 3238.6147 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 42/300\n",
      "1250/1250 - 4s - loss: 972.2243 - mse: 972.2243 - val_loss: 701.0977 - val_mse: 701.0977 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 43/300\n",
      "1250/1250 - 4s - loss: 964.0078 - mse: 964.0078 - val_loss: 74.0050 - val_mse: 74.0050 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 44/300\n",
      "1250/1250 - 4s - loss: 973.1471 - mse: 973.1471 - val_loss: 1701.1786 - val_mse: 1701.1786 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 45/300\n",
      "1250/1250 - 8s - loss: 970.4459 - mse: 970.4459 - val_loss: 63.4701 - val_mse: 63.4701 - lr: 6.2500e-04 - 8s/epoch - 6ms/step\n",
      "Epoch 46/300\n",
      "1250/1250 - 5s - loss: 977.3304 - mse: 977.3304 - val_loss: 1126.2019 - val_mse: 1126.2019 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 47/300\n",
      "1250/1250 - 5s - loss: 991.2318 - mse: 991.2318 - val_loss: 1646.6525 - val_mse: 1646.6525 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 48/300\n",
      "1250/1250 - 5s - loss: 980.6407 - mse: 980.6407 - val_loss: 136.1296 - val_mse: 136.1296 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 49/300\n",
      "1250/1250 - 5s - loss: 993.5054 - mse: 993.5054 - val_loss: 2205.4097 - val_mse: 2205.4097 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 5s - loss: 990.4800 - mse: 990.4800 - val_loss: 280.2310 - val_mse: 280.2310 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 51/300\n",
      "1250/1250 - 5s - loss: 93.0175 - mse: 93.0175 - val_loss: 328.0701 - val_mse: 328.0701 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 52/300\n",
      "1250/1250 - 5s - loss: 93.1314 - mse: 93.1314 - val_loss: 37.5308 - val_mse: 37.5308 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 53/300\n",
      "1250/1250 - 5s - loss: 93.2753 - mse: 93.2753 - val_loss: 177.7810 - val_mse: 177.7810 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 54/300\n",
      "1250/1250 - 5s - loss: 89.9352 - mse: 89.9352 - val_loss: 73.4850 - val_mse: 73.4850 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 55/300\n",
      "1250/1250 - 4s - loss: 89.0461 - mse: 89.0461 - val_loss: 60.3115 - val_mse: 60.3115 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 56/300\n",
      "1250/1250 - 4s - loss: 90.1381 - mse: 90.1381 - val_loss: 32.3331 - val_mse: 32.3331 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 57/300\n",
      "1250/1250 - 5s - loss: 91.8746 - mse: 91.8746 - val_loss: 133.5736 - val_mse: 133.5736 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 58/300\n",
      "1250/1250 - 4s - loss: 95.2124 - mse: 95.2124 - val_loss: 50.1428 - val_mse: 50.1428 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 59/300\n",
      "1250/1250 - 4s - loss: 95.6820 - mse: 95.6820 - val_loss: 88.2543 - val_mse: 88.2543 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 60/300\n",
      "1250/1250 - 4s - loss: 97.5769 - mse: 97.5769 - val_loss: 108.9488 - val_mse: 108.9488 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "1250/1250 - 4s - loss: 99.5077 - mse: 99.5077 - val_loss: 64.6696 - val_mse: 64.6696 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 62/300\n",
      "1250/1250 - 4s - loss: 31.3870 - mse: 31.3870 - val_loss: 29.3362 - val_mse: 29.3362 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 63/300\n",
      "1250/1250 - 5s - loss: 31.2455 - mse: 31.2455 - val_loss: 39.4848 - val_mse: 39.4848 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 64/300\n",
      "1250/1250 - 5s - loss: 30.8943 - mse: 30.8943 - val_loss: 38.3510 - val_mse: 38.3510 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 65/300\n",
      "1250/1250 - 5s - loss: 30.8663 - mse: 30.8663 - val_loss: 35.9765 - val_mse: 35.9765 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 66/300\n",
      "1250/1250 - 4s - loss: 31.3417 - mse: 31.3417 - val_loss: 37.9558 - val_mse: 37.9558 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 67/300\n",
      "1250/1250 - 5s - loss: 31.2114 - mse: 31.2114 - val_loss: 26.2889 - val_mse: 26.2889 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 68/300\n",
      "1250/1250 - 5s - loss: 30.9751 - mse: 30.9751 - val_loss: 30.8015 - val_mse: 30.8015 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 69/300\n",
      "1250/1250 - 6s - loss: 31.1620 - mse: 31.1620 - val_loss: 26.5542 - val_mse: 26.5542 - lr: 3.9062e-05 - 6s/epoch - 5ms/step\n",
      "Epoch 70/300\n",
      "1250/1250 - 8s - loss: 30.7319 - mse: 30.7319 - val_loss: 42.9056 - val_mse: 42.9056 - lr: 3.9062e-05 - 8s/epoch - 7ms/step\n",
      "Epoch 71/300\n",
      "1250/1250 - 6s - loss: 29.7054 - mse: 29.7054 - val_loss: 30.8029 - val_mse: 30.8029 - lr: 3.9062e-05 - 6s/epoch - 5ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "1250/1250 - 6s - loss: 29.6843 - mse: 29.6843 - val_loss: 34.9628 - val_mse: 34.9628 - lr: 3.9062e-05 - 6s/epoch - 5ms/step\n",
      "Epoch 73/300\n",
      "1250/1250 - 5s - loss: 25.0422 - mse: 25.0422 - val_loss: 26.2405 - val_mse: 26.2405 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300\n",
      "1250/1250 - 6s - loss: 24.9635 - mse: 24.9635 - val_loss: 26.2361 - val_mse: 26.2361 - lr: 9.7656e-06 - 6s/epoch - 5ms/step\n",
      "Epoch 75/300\n",
      "1250/1250 - 5s - loss: 24.9487 - mse: 24.9487 - val_loss: 25.9589 - val_mse: 25.9589 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 76/300\n",
      "1250/1250 - 5s - loss: 24.9234 - mse: 24.9234 - val_loss: 26.1657 - val_mse: 26.1657 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 77/300\n",
      "1250/1250 - 5s - loss: 24.9192 - mse: 24.9192 - val_loss: 25.3755 - val_mse: 25.3755 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 78/300\n",
      "1250/1250 - 5s - loss: 24.8623 - mse: 24.8623 - val_loss: 25.8588 - val_mse: 25.8588 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 79/300\n",
      "1250/1250 - 5s - loss: 24.8149 - mse: 24.8149 - val_loss: 26.2034 - val_mse: 26.2034 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 80/300\n",
      "1250/1250 - 4s - loss: 24.7968 - mse: 24.7968 - val_loss: 27.2720 - val_mse: 27.2720 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 81/300\n",
      "1250/1250 - 4s - loss: 24.8117 - mse: 24.8117 - val_loss: 25.2015 - val_mse: 25.2015 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 82/300\n",
      "1250/1250 - 4s - loss: 24.7675 - mse: 24.7675 - val_loss: 26.0678 - val_mse: 26.0678 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 83/300\n",
      "1250/1250 - 5s - loss: 24.7395 - mse: 24.7395 - val_loss: 24.9355 - val_mse: 24.9355 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 84/300\n",
      "1250/1250 - 6s - loss: 24.6214 - mse: 24.6214 - val_loss: 25.1333 - val_mse: 25.1333 - lr: 9.7656e-06 - 6s/epoch - 4ms/step\n",
      "Epoch 85/300\n",
      "1250/1250 - 5s - loss: 24.5829 - mse: 24.5829 - val_loss: 26.9068 - val_mse: 26.9068 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 86/300\n",
      "1250/1250 - 5s - loss: 24.4528 - mse: 24.4528 - val_loss: 25.2900 - val_mse: 25.2900 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 87/300\n",
      "1250/1250 - 4s - loss: 24.4160 - mse: 24.4160 - val_loss: 25.4995 - val_mse: 25.4995 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "1250/1250 - 4s - loss: 24.3625 - mse: 24.3625 - val_loss: 26.5909 - val_mse: 26.5909 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 89/300\n",
      "1250/1250 - 4s - loss: 23.7904 - mse: 23.7904 - val_loss: 24.8895 - val_mse: 24.8895 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 90/300\n",
      "1250/1250 - 4s - loss: 23.7869 - mse: 23.7869 - val_loss: 24.8892 - val_mse: 24.8892 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 91/300\n",
      "1250/1250 - 4s - loss: 23.8837 - mse: 23.8837 - val_loss: 24.8353 - val_mse: 24.8353 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 92/300\n",
      "1250/1250 - 5s - loss: 23.8357 - mse: 23.8357 - val_loss: 24.6312 - val_mse: 24.6312 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 93/300\n",
      "1250/1250 - 5s - loss: 23.7486 - mse: 23.7486 - val_loss: 24.5228 - val_mse: 24.5228 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 94/300\n",
      "1250/1250 - 5s - loss: 23.7159 - mse: 23.7159 - val_loss: 24.5143 - val_mse: 24.5143 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 95/300\n",
      "1250/1250 - 5s - loss: 23.7842 - mse: 23.7842 - val_loss: 24.6597 - val_mse: 24.6597 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 96/300\n",
      "1250/1250 - 5s - loss: 23.7944 - mse: 23.7944 - val_loss: 24.6668 - val_mse: 24.6668 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 97/300\n",
      "1250/1250 - 5s - loss: 23.7684 - mse: 23.7684 - val_loss: 25.4038 - val_mse: 25.4038 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 98/300\n",
      "1250/1250 - 5s - loss: 23.6934 - mse: 23.6934 - val_loss: 24.8207 - val_mse: 24.8207 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "1250/1250 - 4s - loss: 23.6209 - mse: 23.6209 - val_loss: 24.5483 - val_mse: 24.5483 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 100/300\n",
      "1250/1250 - 4s - loss: 23.5657 - mse: 23.5657 - val_loss: 24.6960 - val_mse: 24.6960 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 101/300\n",
      "1250/1250 - 4s - loss: 23.5535 - mse: 23.5535 - val_loss: 24.6154 - val_mse: 24.6154 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 102/300\n",
      "1250/1250 - 4s - loss: 23.5763 - mse: 23.5763 - val_loss: 24.6266 - val_mse: 24.6266 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 103/300\n",
      "1250/1250 - 4s - loss: 23.5845 - mse: 23.5845 - val_loss: 24.4800 - val_mse: 24.4800 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 104/300\n",
      "1250/1250 - 4s - loss: 23.5529 - mse: 23.5529 - val_loss: 24.5998 - val_mse: 24.5998 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 105/300\n",
      "1250/1250 - 5s - loss: 23.5704 - mse: 23.5704 - val_loss: 24.7129 - val_mse: 24.7129 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 106/300\n",
      "1250/1250 - 5s - loss: 23.5360 - mse: 23.5360 - val_loss: 24.5401 - val_mse: 24.5401 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 107/300\n",
      "1250/1250 - 5s - loss: 23.5365 - mse: 23.5365 - val_loss: 24.4392 - val_mse: 24.4392 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 108/300\n",
      "1250/1250 - 5s - loss: 23.5286 - mse: 23.5286 - val_loss: 24.4965 - val_mse: 24.4965 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 109/300\n",
      "1250/1250 - 5s - loss: 23.5316 - mse: 23.5316 - val_loss: 24.3612 - val_mse: 24.3612 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 110/300\n",
      "1250/1250 - 5s - loss: 23.4994 - mse: 23.4994 - val_loss: 24.3239 - val_mse: 24.3239 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 111/300\n",
      "1250/1250 - 5s - loss: 23.4145 - mse: 23.4145 - val_loss: 24.3251 - val_mse: 24.3251 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 112/300\n",
      "1250/1250 - 5s - loss: 23.4359 - mse: 23.4359 - val_loss: 24.2887 - val_mse: 24.2887 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 113/300\n",
      "1250/1250 - 5s - loss: 23.3673 - mse: 23.3673 - val_loss: 24.4288 - val_mse: 24.4288 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 114/300\n",
      "1250/1250 - 5s - loss: 23.3316 - mse: 23.3316 - val_loss: 24.2636 - val_mse: 24.2636 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 115/300\n",
      "1250/1250 - 5s - loss: 23.3304 - mse: 23.3304 - val_loss: 24.2601 - val_mse: 24.2601 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 116/300\n",
      "1250/1250 - 5s - loss: 23.3168 - mse: 23.3168 - val_loss: 24.2668 - val_mse: 24.2668 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 117/300\n",
      "1250/1250 - 5s - loss: 23.3448 - mse: 23.3448 - val_loss: 24.4631 - val_mse: 24.4631 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 118/300\n",
      "1250/1250 - 5s - loss: 23.3481 - mse: 23.3481 - val_loss: 24.2333 - val_mse: 24.2333 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 119/300\n",
      "1250/1250 - 5s - loss: 23.2891 - mse: 23.2891 - val_loss: 24.2492 - val_mse: 24.2492 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 120/300\n",
      "1250/1250 - 5s - loss: 23.3047 - mse: 23.3047 - val_loss: 24.2741 - val_mse: 24.2741 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 121/300\n",
      "1250/1250 - 5s - loss: 23.2863 - mse: 23.2863 - val_loss: 24.2805 - val_mse: 24.2805 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 122/300\n",
      "1250/1250 - 5s - loss: 23.2898 - mse: 23.2898 - val_loss: 24.2953 - val_mse: 24.2953 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 123/300\n",
      "1250/1250 - 5s - loss: 23.2838 - mse: 23.2838 - val_loss: 24.2157 - val_mse: 24.2157 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 124/300\n",
      "1250/1250 - 5s - loss: 23.2873 - mse: 23.2873 - val_loss: 24.2368 - val_mse: 24.2368 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 125/300\n",
      "1250/1250 - 5s - loss: 23.3066 - mse: 23.3066 - val_loss: 24.2187 - val_mse: 24.2187 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 126/300\n",
      "1250/1250 - 5s - loss: 23.2673 - mse: 23.2673 - val_loss: 24.1651 - val_mse: 24.1651 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 127/300\n",
      "1250/1250 - 4s - loss: 23.2709 - mse: 23.2709 - val_loss: 24.2363 - val_mse: 24.2363 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 128/300\n",
      "1250/1250 - 5s - loss: 23.2919 - mse: 23.2919 - val_loss: 24.2009 - val_mse: 24.2009 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 129/300\n",
      "1250/1250 - 5s - loss: 23.2949 - mse: 23.2949 - val_loss: 24.4739 - val_mse: 24.4739 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 130/300\n",
      "1250/1250 - 5s - loss: 23.3094 - mse: 23.3094 - val_loss: 24.3602 - val_mse: 24.3602 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "1250/1250 - 5s - loss: 23.3500 - mse: 23.3500 - val_loss: 24.2247 - val_mse: 24.2247 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/300\n",
      "1250/1250 - 4s - loss: 23.3268 - mse: 23.3268 - val_loss: 24.2094 - val_mse: 24.2094 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 133/300\n",
      "1250/1250 - 5s - loss: 23.3106 - mse: 23.3106 - val_loss: 24.3127 - val_mse: 24.3127 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 134/300\n",
      "1250/1250 - 5s - loss: 23.3230 - mse: 23.3230 - val_loss: 24.2725 - val_mse: 24.2725 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 135/300\n",
      "1250/1250 - 4s - loss: 23.3226 - mse: 23.3226 - val_loss: 24.2749 - val_mse: 24.2749 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "1250/1250 - 4s - loss: 23.3019 - mse: 23.3019 - val_loss: 24.2728 - val_mse: 24.2728 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 137/300\n",
      "1250/1250 - 4s - loss: 23.3357 - mse: 23.3357 - val_loss: 24.3866 - val_mse: 24.3866 - lr: 3.8147e-08 - 4s/epoch - 4ms/step\n",
      "Epoch 138/300\n",
      "1250/1250 - 4s - loss: 23.2385 - mse: 23.2385 - val_loss: 24.2029 - val_mse: 24.2029 - lr: 3.8147e-08 - 4s/epoch - 4ms/step\n",
      "Epoch 139/300\n",
      "1250/1250 - 5s - loss: 23.2369 - mse: 23.2369 - val_loss: 24.2370 - val_mse: 24.2370 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 140/300\n",
      "1250/1250 - 4s - loss: 23.2111 - mse: 23.2111 - val_loss: 24.2118 - val_mse: 24.2118 - lr: 3.8147e-08 - 4s/epoch - 4ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "1250/1250 - 5s - loss: 23.2032 - mse: 23.2032 - val_loss: 24.1926 - val_mse: 24.1926 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 142/300\n",
      "1250/1250 - 5s - loss: 23.1905 - mse: 23.1905 - val_loss: 24.1665 - val_mse: 24.1665 - lr: 9.5367e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 143/300\n",
      "1250/1250 - 5s - loss: 23.1675 - mse: 23.1675 - val_loss: 24.1398 - val_mse: 24.1398 - lr: 9.5367e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 144/300\n",
      "1250/1250 - 4s - loss: 23.1516 - mse: 23.1516 - val_loss: 24.1431 - val_mse: 24.1431 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 145/300\n",
      "1250/1250 - 5s - loss: 23.1393 - mse: 23.1393 - val_loss: 24.1287 - val_mse: 24.1287 - lr: 9.5367e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 146/300\n",
      "1250/1250 - 5s - loss: 23.1434 - mse: 23.1434 - val_loss: 24.1463 - val_mse: 24.1463 - lr: 9.5367e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 147/300\n",
      "1250/1250 - 4s - loss: 23.1484 - mse: 23.1484 - val_loss: 24.1312 - val_mse: 24.1312 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 148/300\n",
      "1250/1250 - 4s - loss: 23.1480 - mse: 23.1480 - val_loss: 24.1299 - val_mse: 24.1299 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 149/300\n",
      "1250/1250 - 4s - loss: 23.1478 - mse: 23.1478 - val_loss: 24.1530 - val_mse: 24.1530 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "1250/1250 - 4s - loss: 23.1554 - mse: 23.1554 - val_loss: 24.1601 - val_mse: 24.1601 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 151/300\n",
      "1250/1250 - 4s - loss: 23.1676 - mse: 23.1676 - val_loss: 24.1595 - val_mse: 24.1595 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 152/300\n",
      "1250/1250 - 4s - loss: 23.1693 - mse: 23.1693 - val_loss: 24.1595 - val_mse: 24.1595 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 153/300\n",
      "1250/1250 - 4s - loss: 23.1678 - mse: 23.1678 - val_loss: 24.1574 - val_mse: 24.1574 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 154/300\n",
      "1250/1250 - 4s - loss: 23.1667 - mse: 23.1667 - val_loss: 24.1514 - val_mse: 24.1514 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "1250/1250 - 5s - loss: 23.1641 - mse: 23.1641 - val_loss: 24.1523 - val_mse: 24.1523 - lr: 2.3842e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 156/300\n",
      "1250/1250 - 4s - loss: 23.1649 - mse: 23.1649 - val_loss: 24.1546 - val_mse: 24.1546 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 157/300\n",
      "1250/1250 - 4s - loss: 23.1644 - mse: 23.1644 - val_loss: 24.1525 - val_mse: 24.1525 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 158/300\n",
      "1250/1250 - 4s - loss: 23.1648 - mse: 23.1648 - val_loss: 24.1523 - val_mse: 24.1523 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 159/300\n",
      "1250/1250 - 4s - loss: 23.1643 - mse: 23.1643 - val_loss: 24.1522 - val_mse: 24.1522 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "1250/1250 - 4s - loss: 23.1637 - mse: 23.1637 - val_loss: 24.1527 - val_mse: 24.1527 - lr: 5.9605e-10 - 4s/epoch - 3ms/step\n",
      "Epoch 161/300\n",
      "1250/1250 - 4s - loss: 23.1652 - mse: 23.1652 - val_loss: 24.1527 - val_mse: 24.1527 - lr: 1.4901e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 162/300\n",
      "1250/1250 - 5s - loss: 23.1653 - mse: 23.1653 - val_loss: 24.1527 - val_mse: 24.1527 - lr: 1.4901e-10 - 5s/epoch - 4ms/step\n",
      "Epoch 163/300\n",
      "1250/1250 - 4s - loss: 23.1652 - mse: 23.1652 - val_loss: 24.1527 - val_mse: 24.1527 - lr: 1.4901e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 164/300\n",
      "Restoring model weights from the end of the best epoch: 145.\n",
      "1250/1250 - 4s - loss: 23.1652 - mse: 23.1652 - val_loss: 24.1527 - val_mse: 24.1527 - lr: 1.4901e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 164: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=12.8min\n",
      "1250/1250 - 2s - loss: 21.4709 - mse: 21.4709 - 2s/epoch - 2ms/step\n",
      "5000/5000 - 9s - loss: 21.2878 - mse: 21.2878 - 9s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x00000125F19B0760>; total time=12.9min\n",
      "Epoch 1/300\n",
      "1250/1250 - 5s - loss: 3313988.7500 - mse: 3313988.7500 - val_loss: 2922920.0000 - val_mse: 2922920.0000 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 5s - loss: 1555865.7500 - mse: 1555865.7500 - val_loss: 1355015.1250 - val_mse: 1355015.1250 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 4s - loss: 1108076.8750 - mse: 1108076.8750 - val_loss: 417619.0312 - val_mse: 417619.0312 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 5s - loss: 859333.4375 - mse: 859333.4375 - val_loss: 556494.0000 - val_mse: 556494.0000 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 5/300\n",
      "1250/1250 - 5s - loss: 702705.1875 - mse: 702705.1875 - val_loss: 566862.7500 - val_mse: 566862.7500 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 4s - loss: 605555.3750 - mse: 605555.3750 - val_loss: 920221.5000 - val_mse: 920221.5000 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 4s - loss: 518773.4062 - mse: 518773.4062 - val_loss: 609392.1875 - val_mse: 609392.1875 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 4s - loss: 454250.8438 - mse: 454250.8438 - val_loss: 953456.3125 - val_mse: 953456.3125 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 4s - loss: 38240.0352 - mse: 38240.0352 - val_loss: 18615.9336 - val_mse: 18615.9336 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 10/300\n",
      "1250/1250 - 4s - loss: 35502.1680 - mse: 35502.1680 - val_loss: 4391.7422 - val_mse: 4391.7422 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 4s - loss: 34929.3945 - mse: 34929.3945 - val_loss: 155324.9844 - val_mse: 155324.9844 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 5s - loss: 34285.3516 - mse: 34285.3516 - val_loss: 25663.3340 - val_mse: 25663.3340 - lr: 0.0025 - 5s/epoch - 4ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 5s - loss: 33831.6719 - mse: 33831.6719 - val_loss: 43549.3203 - val_mse: 43549.3203 - lr: 0.0025 - 5s/epoch - 4ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 4s - loss: 33166.0078 - mse: 33166.0078 - val_loss: 17066.6504 - val_mse: 17066.6504 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 5s - loss: 32887.8438 - mse: 32887.8438 - val_loss: 50866.0469 - val_mse: 50866.0469 - lr: 0.0025 - 5s/epoch - 4ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 5s - loss: 2438.4597 - mse: 2438.4597 - val_loss: 3217.0923 - val_mse: 3217.0923 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 5s - loss: 2319.5527 - mse: 2319.5527 - val_loss: 1023.6182 - val_mse: 1023.6182 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 5s - loss: 2314.6384 - mse: 2314.6384 - val_loss: 351.8275 - val_mse: 351.8275 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300\n",
      "1250/1250 - 5s - loss: 2283.1829 - mse: 2283.1829 - val_loss: 3925.2080 - val_mse: 3925.2080 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 20/300\n",
      "1250/1250 - 5s - loss: 2310.2441 - mse: 2310.2441 - val_loss: 236.0239 - val_mse: 236.0239 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 21/300\n",
      "1250/1250 - 5s - loss: 2278.8721 - mse: 2278.8721 - val_loss: 4848.8550 - val_mse: 4848.8550 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 22/300\n",
      "1250/1250 - 5s - loss: 2267.4404 - mse: 2267.4404 - val_loss: 5119.8662 - val_mse: 5119.8662 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 23/300\n",
      "1250/1250 - 5s - loss: 2259.1433 - mse: 2259.1433 - val_loss: 1478.0574 - val_mse: 1478.0574 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 24/300\n",
      "1250/1250 - 5s - loss: 2264.4810 - mse: 2264.4810 - val_loss: 183.9195 - val_mse: 183.9195 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 25/300\n",
      "1250/1250 - 5s - loss: 2245.8271 - mse: 2245.8271 - val_loss: 1783.7766 - val_mse: 1783.7766 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 26/300\n",
      "1250/1250 - 5s - loss: 2277.2441 - mse: 2277.2441 - val_loss: 620.3834 - val_mse: 620.3834 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 27/300\n",
      "1250/1250 - 5s - loss: 2266.6523 - mse: 2266.6523 - val_loss: 2192.9661 - val_mse: 2192.9661 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 28/300\n",
      "1250/1250 - 5s - loss: 2257.1133 - mse: 2257.1133 - val_loss: 3173.5818 - val_mse: 3173.5818 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 5s - loss: 2235.6169 - mse: 2235.6169 - val_loss: 7471.8159 - val_mse: 7471.8159 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 30/300\n",
      "1250/1250 - 5s - loss: 202.7473 - mse: 202.7473 - val_loss: 453.5723 - val_mse: 453.5723 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 31/300\n",
      "1250/1250 - 5s - loss: 191.9038 - mse: 191.9038 - val_loss: 266.1633 - val_mse: 266.1633 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 32/300\n",
      "1250/1250 - 4s - loss: 192.6874 - mse: 192.6874 - val_loss: 59.2584 - val_mse: 59.2584 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 33/300\n",
      "1250/1250 - 4s - loss: 192.3396 - mse: 192.3396 - val_loss: 62.4389 - val_mse: 62.4389 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 34/300\n",
      "1250/1250 - 4s - loss: 190.1853 - mse: 190.1853 - val_loss: 274.7583 - val_mse: 274.7583 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 35/300\n",
      "1250/1250 - 4s - loss: 190.2463 - mse: 190.2463 - val_loss: 169.1742 - val_mse: 169.1742 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 36/300\n",
      "1250/1250 - 4s - loss: 189.4181 - mse: 189.4181 - val_loss: 219.7817 - val_mse: 219.7817 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 37/300\n",
      "1250/1250 - 4s - loss: 187.2796 - mse: 187.2796 - val_loss: 56.1950 - val_mse: 56.1950 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 38/300\n",
      "1250/1250 - 4s - loss: 185.1707 - mse: 185.1707 - val_loss: 325.1679 - val_mse: 325.1679 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 39/300\n",
      "1250/1250 - 4s - loss: 183.3304 - mse: 183.3304 - val_loss: 55.4148 - val_mse: 55.4148 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 40/300\n",
      "1250/1250 - 4s - loss: 180.4668 - mse: 180.4668 - val_loss: 155.5660 - val_mse: 155.5660 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 41/300\n",
      "1250/1250 - 4s - loss: 180.7361 - mse: 180.7361 - val_loss: 359.1650 - val_mse: 359.1650 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 42/300\n",
      "1250/1250 - 4s - loss: 181.4439 - mse: 181.4439 - val_loss: 50.5095 - val_mse: 50.5095 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 43/300\n",
      "1250/1250 - 4s - loss: 185.4604 - mse: 185.4604 - val_loss: 108.0897 - val_mse: 108.0897 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 44/300\n",
      "1250/1250 - 4s - loss: 186.3352 - mse: 186.3352 - val_loss: 116.6440 - val_mse: 116.6440 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 45/300\n",
      "1250/1250 - 4s - loss: 191.1740 - mse: 191.1740 - val_loss: 266.0621 - val_mse: 266.0621 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 46/300\n",
      "1250/1250 - 4s - loss: 191.4023 - mse: 191.4023 - val_loss: 390.2909 - val_mse: 390.2909 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "1250/1250 - 4s - loss: 190.2931 - mse: 190.2931 - val_loss: 173.8031 - val_mse: 173.8031 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 48/300\n",
      "1250/1250 - 4s - loss: 52.8628 - mse: 52.8628 - val_loss: 54.8239 - val_mse: 54.8239 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 49/300\n",
      "1250/1250 - 4s - loss: 52.9653 - mse: 52.9653 - val_loss: 43.8615 - val_mse: 43.8615 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 50/300\n",
      "1250/1250 - 4s - loss: 52.6967 - mse: 52.6967 - val_loss: 60.8724 - val_mse: 60.8724 - lr: 3.9062e-05 - 4s/epoch - 3ms/step\n",
      "Epoch 51/300\n",
      "1250/1250 - 4s - loss: 52.1383 - mse: 52.1383 - val_loss: 76.4668 - val_mse: 76.4668 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 52/300\n",
      "1250/1250 - 4s - loss: 52.6143 - mse: 52.6143 - val_loss: 43.9658 - val_mse: 43.9658 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 53/300\n",
      "1250/1250 - 4s - loss: 51.9284 - mse: 51.9284 - val_loss: 45.9113 - val_mse: 45.9113 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "1250/1250 - 4s - loss: 51.3202 - mse: 51.3202 - val_loss: 60.7869 - val_mse: 60.7869 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 55/300\n",
      "1250/1250 - 4s - loss: 43.0739 - mse: 43.0739 - val_loss: 42.2824 - val_mse: 42.2824 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 56/300\n",
      "1250/1250 - 4s - loss: 42.9675 - mse: 42.9675 - val_loss: 42.2884 - val_mse: 42.2884 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 57/300\n",
      "1250/1250 - 4s - loss: 42.8934 - mse: 42.8934 - val_loss: 42.6077 - val_mse: 42.6077 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 58/300\n",
      "1250/1250 - 4s - loss: 42.7352 - mse: 42.7352 - val_loss: 41.8323 - val_mse: 41.8323 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 59/300\n",
      "1250/1250 - 4s - loss: 42.5797 - mse: 42.5797 - val_loss: 41.5896 - val_mse: 41.5896 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 60/300\n",
      "1250/1250 - 4s - loss: 42.5453 - mse: 42.5453 - val_loss: 45.2212 - val_mse: 45.2212 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 61/300\n",
      "1250/1250 - 4s - loss: 42.4230 - mse: 42.4230 - val_loss: 41.5975 - val_mse: 41.5975 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 62/300\n",
      "1250/1250 - 4s - loss: 42.4519 - mse: 42.4519 - val_loss: 42.5466 - val_mse: 42.5466 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 63/300\n",
      "1250/1250 - 4s - loss: 42.3068 - mse: 42.3068 - val_loss: 41.2306 - val_mse: 41.2306 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 64/300\n",
      "1250/1250 - 4s - loss: 42.1130 - mse: 42.1130 - val_loss: 41.0066 - val_mse: 41.0066 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 65/300\n",
      "1250/1250 - 4s - loss: 41.8885 - mse: 41.8885 - val_loss: 42.4893 - val_mse: 42.4893 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 66/300\n",
      "1250/1250 - 4s - loss: 41.7646 - mse: 41.7646 - val_loss: 41.9064 - val_mse: 41.9064 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 67/300\n",
      "1250/1250 - 4s - loss: 41.6437 - mse: 41.6437 - val_loss: 40.9022 - val_mse: 40.9022 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 68/300\n",
      "1250/1250 - 4s - loss: 41.6011 - mse: 41.6011 - val_loss: 40.7027 - val_mse: 40.7027 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 69/300\n",
      "1250/1250 - 4s - loss: 41.4857 - mse: 41.4857 - val_loss: 41.2214 - val_mse: 41.2214 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 70/300\n",
      "1250/1250 - 4s - loss: 41.3619 - mse: 41.3619 - val_loss: 41.7760 - val_mse: 41.7760 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 71/300\n",
      "1250/1250 - 4s - loss: 41.3491 - mse: 41.3491 - val_loss: 42.8820 - val_mse: 42.8820 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 72/300\n",
      "1250/1250 - 4s - loss: 41.2254 - mse: 41.2254 - val_loss: 40.7178 - val_mse: 40.7178 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 73/300\n",
      "1250/1250 - 4s - loss: 41.2335 - mse: 41.2335 - val_loss: 40.3150 - val_mse: 40.3150 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 74/300\n",
      "1250/1250 - 4s - loss: 41.1435 - mse: 41.1435 - val_loss: 40.9667 - val_mse: 40.9667 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 75/300\n",
      "1250/1250 - 4s - loss: 41.0794 - mse: 41.0794 - val_loss: 40.6009 - val_mse: 40.6009 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 76/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 - 4s - loss: 40.9788 - mse: 40.9788 - val_loss: 40.0134 - val_mse: 40.0134 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 77/300\n",
      "1250/1250 - 5s - loss: 40.7704 - mse: 40.7704 - val_loss: 39.9294 - val_mse: 39.9294 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 78/300\n",
      "1250/1250 - 4s - loss: 40.6687 - mse: 40.6687 - val_loss: 39.6838 - val_mse: 39.6838 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 79/300\n",
      "1250/1250 - 4s - loss: 40.6104 - mse: 40.6104 - val_loss: 40.8049 - val_mse: 40.8049 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 80/300\n",
      "1250/1250 - 4s - loss: 40.4430 - mse: 40.4430 - val_loss: 39.3130 - val_mse: 39.3130 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 81/300\n",
      "1250/1250 - 4s - loss: 40.4712 - mse: 40.4712 - val_loss: 43.2159 - val_mse: 43.2159 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 82/300\n",
      "1250/1250 - 4s - loss: 40.2438 - mse: 40.2438 - val_loss: 40.8540 - val_mse: 40.8540 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 83/300\n",
      "1250/1250 - 4s - loss: 40.1090 - mse: 40.1090 - val_loss: 40.2588 - val_mse: 40.2588 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 84/300\n",
      "1250/1250 - 4s - loss: 40.1135 - mse: 40.1135 - val_loss: 39.6175 - val_mse: 39.6175 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "1250/1250 - 4s - loss: 40.0859 - mse: 40.0859 - val_loss: 40.2377 - val_mse: 40.2377 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 86/300\n",
      "1250/1250 - 4s - loss: 39.1218 - mse: 39.1218 - val_loss: 38.9710 - val_mse: 38.9710 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 87/300\n",
      "1250/1250 - 4s - loss: 39.0805 - mse: 39.0805 - val_loss: 38.7200 - val_mse: 38.7200 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 88/300\n",
      "1250/1250 - 4s - loss: 39.0656 - mse: 39.0656 - val_loss: 38.7033 - val_mse: 38.7033 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 89/300\n",
      "1250/1250 - 4s - loss: 39.1044 - mse: 39.1044 - val_loss: 38.6730 - val_mse: 38.6730 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 90/300\n",
      "1250/1250 - 4s - loss: 39.0087 - mse: 39.0087 - val_loss: 38.8327 - val_mse: 38.8327 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 91/300\n",
      "1250/1250 - 4s - loss: 39.0060 - mse: 39.0060 - val_loss: 38.7522 - val_mse: 38.7522 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 92/300\n",
      "1250/1250 - 4s - loss: 39.0305 - mse: 39.0305 - val_loss: 38.9274 - val_mse: 38.9274 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 93/300\n",
      "1250/1250 - 4s - loss: 38.9495 - mse: 38.9495 - val_loss: 38.5645 - val_mse: 38.5645 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 94/300\n",
      "1250/1250 - 4s - loss: 38.9460 - mse: 38.9460 - val_loss: 38.6665 - val_mse: 38.6665 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 95/300\n",
      "1250/1250 - 4s - loss: 38.8923 - mse: 38.8923 - val_loss: 38.5643 - val_mse: 38.5643 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 96/300\n",
      "1250/1250 - 4s - loss: 38.8438 - mse: 38.8438 - val_loss: 38.6716 - val_mse: 38.6716 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 97/300\n",
      "1250/1250 - 4s - loss: 38.8766 - mse: 38.8766 - val_loss: 38.4382 - val_mse: 38.4382 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 98/300\n",
      "1250/1250 - 4s - loss: 38.9044 - mse: 38.9044 - val_loss: 38.6399 - val_mse: 38.6399 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 99/300\n",
      "1250/1250 - 4s - loss: 38.8869 - mse: 38.8869 - val_loss: 38.6680 - val_mse: 38.6680 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 100/300\n",
      "1250/1250 - 4s - loss: 38.8436 - mse: 38.8436 - val_loss: 39.0178 - val_mse: 39.0178 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 101/300\n",
      "1250/1250 - 4s - loss: 38.8411 - mse: 38.8411 - val_loss: 38.6161 - val_mse: 38.6161 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "1250/1250 - 4s - loss: 38.7224 - mse: 38.7224 - val_loss: 38.4480 - val_mse: 38.4480 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 103/300\n",
      "1250/1250 - 4s - loss: 38.5709 - mse: 38.5709 - val_loss: 38.3743 - val_mse: 38.3743 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 104/300\n",
      "1250/1250 - 4s - loss: 38.5769 - mse: 38.5769 - val_loss: 38.3566 - val_mse: 38.3566 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 105/300\n",
      "1250/1250 - 4s - loss: 38.5812 - mse: 38.5812 - val_loss: 38.3302 - val_mse: 38.3302 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 106/300\n",
      "1250/1250 - 4s - loss: 38.5651 - mse: 38.5651 - val_loss: 38.2746 - val_mse: 38.2746 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 107/300\n",
      "1250/1250 - 4s - loss: 38.5398 - mse: 38.5398 - val_loss: 38.3763 - val_mse: 38.3763 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 108/300\n",
      "1250/1250 - 4s - loss: 38.5508 - mse: 38.5508 - val_loss: 38.2761 - val_mse: 38.2761 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 109/300\n",
      "1250/1250 - 4s - loss: 38.4920 - mse: 38.4920 - val_loss: 38.1784 - val_mse: 38.1784 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 110/300\n",
      "1250/1250 - 4s - loss: 38.5002 - mse: 38.5002 - val_loss: 38.2168 - val_mse: 38.2168 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 111/300\n",
      "1250/1250 - 4s - loss: 38.4592 - mse: 38.4592 - val_loss: 38.4676 - val_mse: 38.4676 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 112/300\n",
      "1250/1250 - 5s - loss: 38.4658 - mse: 38.4658 - val_loss: 38.2633 - val_mse: 38.2633 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 113/300\n",
      "1250/1250 - 5s - loss: 38.4742 - mse: 38.4742 - val_loss: 38.1393 - val_mse: 38.1393 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 114/300\n",
      "1250/1250 - 5s - loss: 38.4745 - mse: 38.4745 - val_loss: 38.1607 - val_mse: 38.1607 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 115/300\n",
      "1250/1250 - 5s - loss: 38.4729 - mse: 38.4729 - val_loss: 38.2019 - val_mse: 38.2019 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 116/300\n",
      "1250/1250 - 5s - loss: 38.4629 - mse: 38.4629 - val_loss: 38.1724 - val_mse: 38.1724 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 117/300\n",
      "1250/1250 - 5s - loss: 38.4907 - mse: 38.4907 - val_loss: 38.2168 - val_mse: 38.2168 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 118/300\n",
      "1250/1250 - 5s - loss: 38.4610 - mse: 38.4610 - val_loss: 38.1282 - val_mse: 38.1282 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 119/300\n",
      "1250/1250 - 5s - loss: 38.4971 - mse: 38.4971 - val_loss: 38.1764 - val_mse: 38.1764 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 120/300\n",
      "1250/1250 - 5s - loss: 38.4828 - mse: 38.4828 - val_loss: 38.1047 - val_mse: 38.1047 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 121/300\n",
      "1250/1250 - 4s - loss: 38.4805 - mse: 38.4805 - val_loss: 38.0656 - val_mse: 38.0656 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 122/300\n",
      "1250/1250 - 5s - loss: 38.4957 - mse: 38.4957 - val_loss: 38.0864 - val_mse: 38.0864 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 123/300\n",
      "1250/1250 - 5s - loss: 38.4720 - mse: 38.4720 - val_loss: 38.1592 - val_mse: 38.1592 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 124/300\n",
      "1250/1250 - 8s - loss: 38.5116 - mse: 38.5116 - val_loss: 38.0833 - val_mse: 38.0833 - lr: 6.1035e-07 - 8s/epoch - 6ms/step\n",
      "Epoch 125/300\n",
      "1250/1250 - 5s - loss: 38.5214 - mse: 38.5214 - val_loss: 38.1297 - val_mse: 38.1297 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "1250/1250 - 5s - loss: 38.5215 - mse: 38.5215 - val_loss: 38.0902 - val_mse: 38.0902 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 127/300\n",
      "1250/1250 - 5s - loss: 38.4716 - mse: 38.4716 - val_loss: 38.1400 - val_mse: 38.1400 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 128/300\n",
      "1250/1250 - 6s - loss: 38.4616 - mse: 38.4616 - val_loss: 38.0426 - val_mse: 38.0426 - lr: 1.5259e-07 - 6s/epoch - 4ms/step\n",
      "Epoch 129/300\n",
      "1250/1250 - 5s - loss: 38.4694 - mse: 38.4694 - val_loss: 38.1041 - val_mse: 38.1041 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 130/300\n",
      "1250/1250 - 4s - loss: 38.4524 - mse: 38.4524 - val_loss: 38.1258 - val_mse: 38.1258 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 131/300\n",
      "1250/1250 - 5s - loss: 38.4306 - mse: 38.4306 - val_loss: 38.0925 - val_mse: 38.0925 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 132/300\n",
      "1250/1250 - 5s - loss: 38.4285 - mse: 38.4285 - val_loss: 38.0734 - val_mse: 38.0734 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 133/300\n",
      "1250/1250 - 4s - loss: 38.4400 - mse: 38.4400 - val_loss: 38.0208 - val_mse: 38.0208 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/300\n",
      "1250/1250 - 4s - loss: 38.4137 - mse: 38.4137 - val_loss: 38.0336 - val_mse: 38.0336 - lr: 1.5259e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 135/300\n",
      "1250/1250 - 4s - loss: 38.4237 - mse: 38.4237 - val_loss: 38.0434 - val_mse: 38.0434 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 136/300\n",
      "1250/1250 - 4s - loss: 38.4139 - mse: 38.4139 - val_loss: 38.1512 - val_mse: 38.1512 - lr: 1.5259e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 137/300\n",
      "1250/1250 - 4s - loss: 38.4137 - mse: 38.4137 - val_loss: 38.0379 - val_mse: 38.0379 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "1250/1250 - 4s - loss: 38.4052 - mse: 38.4052 - val_loss: 38.1086 - val_mse: 38.1086 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 139/300\n",
      "1250/1250 - 4s - loss: 38.4017 - mse: 38.4017 - val_loss: 38.1200 - val_mse: 38.1200 - lr: 3.8147e-08 - 4s/epoch - 3ms/step\n",
      "Epoch 140/300\n",
      "1250/1250 - 4s - loss: 38.3941 - mse: 38.3941 - val_loss: 38.0876 - val_mse: 38.0876 - lr: 3.8147e-08 - 4s/epoch - 3ms/step\n",
      "Epoch 141/300\n",
      "1250/1250 - 4s - loss: 38.4167 - mse: 38.4167 - val_loss: 38.1061 - val_mse: 38.1061 - lr: 3.8147e-08 - 4s/epoch - 3ms/step\n",
      "Epoch 142/300\n",
      "1250/1250 - 4s - loss: 38.4208 - mse: 38.4208 - val_loss: 38.1486 - val_mse: 38.1486 - lr: 3.8147e-08 - 4s/epoch - 3ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "1250/1250 - 4s - loss: 38.4155 - mse: 38.4155 - val_loss: 38.1412 - val_mse: 38.1412 - lr: 3.8147e-08 - 4s/epoch - 4ms/step\n",
      "Epoch 144/300\n",
      "1250/1250 - 4s - loss: 38.4089 - mse: 38.4089 - val_loss: 38.1274 - val_mse: 38.1274 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 145/300\n",
      "1250/1250 - 4s - loss: 38.4294 - mse: 38.4294 - val_loss: 38.1827 - val_mse: 38.1827 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 146/300\n",
      "1250/1250 - 4s - loss: 38.4186 - mse: 38.4186 - val_loss: 38.2016 - val_mse: 38.2016 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 147/300\n",
      "1250/1250 - 4s - loss: 38.4503 - mse: 38.4503 - val_loss: 38.1977 - val_mse: 38.1977 - lr: 9.5367e-09 - 4s/epoch - 3ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "1250/1250 - 4s - loss: 38.4199 - mse: 38.4199 - val_loss: 38.1694 - val_mse: 38.1694 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 149/300\n",
      "1250/1250 - 4s - loss: 38.4255 - mse: 38.4255 - val_loss: 38.1718 - val_mse: 38.1718 - lr: 2.3842e-09 - 4s/epoch - 3ms/step\n",
      "Epoch 150/300\n",
      "1250/1250 - 4s - loss: 38.4253 - mse: 38.4253 - val_loss: 38.1455 - val_mse: 38.1455 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 151/300\n",
      "1250/1250 - 4s - loss: 38.4008 - mse: 38.4008 - val_loss: 38.1402 - val_mse: 38.1402 - lr: 2.3842e-09 - 4s/epoch - 3ms/step\n",
      "Epoch 152/300\n",
      "Restoring model weights from the end of the best epoch: 133.\n",
      "1250/1250 - 4s - loss: 38.3996 - mse: 38.3996 - val_loss: 38.1373 - val_mse: 38.1373 - lr: 2.3842e-09 - 4s/epoch - 3ms/step\n",
      "Epoch 152: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=11.5min\n",
      "1250/1250 - 2s - loss: 37.6364 - mse: 37.6364 - 2s/epoch - 2ms/step\n",
      "5000/5000 - 8s - loss: 37.1382 - mse: 37.1382 - 8s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x00000125F19B0760>; total time=11.6min\n",
      "Epoch 1/300\n",
      "1250/1250 - 5s - loss: 3252590.5000 - mse: 3252590.5000 - val_loss: 1012994.6875 - val_mse: 1012994.6875 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 4s - loss: 1490389.0000 - mse: 1490389.0000 - val_loss: 622830.6250 - val_mse: 622830.6250 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 4s - loss: 1049751.5000 - mse: 1049751.5000 - val_loss: 206173.5625 - val_mse: 206173.5625 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 4s - loss: 806688.6875 - mse: 806688.6875 - val_loss: 437366.0625 - val_mse: 437366.0625 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 5/300\n",
      "1250/1250 - 4s - loss: 651545.1875 - mse: 651545.1875 - val_loss: 339269.8750 - val_mse: 339269.8750 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 4s - loss: 561825.6250 - mse: 561825.6250 - val_loss: 466678.6250 - val_mse: 466678.6250 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 4s - loss: 484765.5000 - mse: 484765.5000 - val_loss: 260116.2969 - val_mse: 260116.2969 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 4s - loss: 425346.6250 - mse: 425346.6250 - val_loss: 212846.5938 - val_mse: 212846.5938 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 4s - loss: 35551.4219 - mse: 35551.4219 - val_loss: 3633.4663 - val_mse: 3633.4663 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 10/300\n",
      "1250/1250 - 4s - loss: 34140.5234 - mse: 34140.5234 - val_loss: 67296.3438 - val_mse: 67296.3438 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 4s - loss: 33576.3789 - mse: 33576.3789 - val_loss: 42167.4922 - val_mse: 42167.4922 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 4s - loss: 33055.1523 - mse: 33055.1523 - val_loss: 19446.8867 - val_mse: 19446.8867 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 4s - loss: 32723.9004 - mse: 32723.9004 - val_loss: 6918.2446 - val_mse: 6918.2446 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 4s - loss: 32390.9570 - mse: 32390.9570 - val_loss: 47251.7812 - val_mse: 47251.7812 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 15/300\n",
      "1250/1250 - 4s - loss: 2387.4585 - mse: 2387.4585 - val_loss: 5169.3862 - val_mse: 5169.3862 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 4s - loss: 2315.7998 - mse: 2315.7998 - val_loss: 1520.5144 - val_mse: 1520.5144 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 4s - loss: 2294.1240 - mse: 2294.1240 - val_loss: 6180.1274 - val_mse: 6180.1274 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 4s - loss: 2298.7917 - mse: 2298.7917 - val_loss: 1976.7180 - val_mse: 1976.7180 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 19/300\n",
      "1250/1250 - 4s - loss: 2292.5613 - mse: 2292.5613 - val_loss: 434.9915 - val_mse: 434.9915 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 20/300\n",
      "1250/1250 - 4s - loss: 2252.8174 - mse: 2252.8174 - val_loss: 362.6819 - val_mse: 362.6819 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 21/300\n",
      "1250/1250 - 4s - loss: 2263.8179 - mse: 2263.8179 - val_loss: 1797.9210 - val_mse: 1797.9210 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 22/300\n",
      "1250/1250 - 4s - loss: 2253.3181 - mse: 2253.3181 - val_loss: 198.9981 - val_mse: 198.9981 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 23/300\n",
      "1250/1250 - 4s - loss: 2234.8267 - mse: 2234.8267 - val_loss: 569.0788 - val_mse: 569.0788 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 24/300\n",
      "1250/1250 - 4s - loss: 2236.8967 - mse: 2236.8967 - val_loss: 590.1151 - val_mse: 590.1151 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 25/300\n",
      "1250/1250 - 4s - loss: 2239.4741 - mse: 2239.4741 - val_loss: 329.9778 - val_mse: 329.9778 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 26/300\n",
      "1250/1250 - 4s - loss: 2240.6907 - mse: 2240.6907 - val_loss: 2091.9849 - val_mse: 2091.9849 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 4s - loss: 2232.8640 - mse: 2232.8640 - val_loss: 629.5526 - val_mse: 629.5526 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 28/300\n",
      "1250/1250 - 4s - loss: 197.8286 - mse: 197.8286 - val_loss: 182.1809 - val_mse: 182.1809 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 29/300\n",
      "1250/1250 - 4s - loss: 196.1783 - mse: 196.1783 - val_loss: 313.0345 - val_mse: 313.0345 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 30/300\n",
      "1250/1250 - 4s - loss: 190.5723 - mse: 190.5723 - val_loss: 84.4830 - val_mse: 84.4830 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 31/300\n",
      "1250/1250 - 4s - loss: 187.1637 - mse: 187.1637 - val_loss: 58.6161 - val_mse: 58.6161 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 32/300\n",
      "1250/1250 - 4s - loss: 188.8628 - mse: 188.8628 - val_loss: 62.4698 - val_mse: 62.4698 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "1250/1250 - 5s - loss: 185.8466 - mse: 185.8466 - val_loss: 123.3424 - val_mse: 123.3424 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 34/300\n",
      "1250/1250 - 5s - loss: 181.8931 - mse: 181.8931 - val_loss: 426.5801 - val_mse: 426.5801 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 35/300\n",
      "1250/1250 - 5s - loss: 176.1539 - mse: 176.1539 - val_loss: 51.5285 - val_mse: 51.5285 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 36/300\n",
      "1250/1250 - 4s - loss: 177.3313 - mse: 177.3313 - val_loss: 172.1820 - val_mse: 172.1820 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 37/300\n",
      "1250/1250 - 5s - loss: 179.0349 - mse: 179.0349 - val_loss: 99.9526 - val_mse: 99.9526 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 38/300\n",
      "1250/1250 - 5s - loss: 179.1052 - mse: 179.1052 - val_loss: 267.2476 - val_mse: 267.2476 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 39/300\n",
      "1250/1250 - 5s - loss: 176.0932 - mse: 176.0932 - val_loss: 132.3463 - val_mse: 132.3463 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "1250/1250 - 5s - loss: 181.5517 - mse: 181.5517 - val_loss: 108.0749 - val_mse: 108.0749 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 41/300\n",
      "1250/1250 - 4s - loss: 52.0396 - mse: 52.0396 - val_loss: 65.6602 - val_mse: 65.6602 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 42/300\n",
      "1250/1250 - 4s - loss: 51.8941 - mse: 51.8941 - val_loss: 44.6304 - val_mse: 44.6304 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 43/300\n",
      "1250/1250 - 4s - loss: 50.6412 - mse: 50.6412 - val_loss: 71.1693 - val_mse: 71.1693 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 44/300\n",
      "1250/1250 - 5s - loss: 49.6410 - mse: 49.6410 - val_loss: 49.1830 - val_mse: 49.1830 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 45/300\n",
      "1250/1250 - 4s - loss: 49.6944 - mse: 49.6944 - val_loss: 45.2294 - val_mse: 45.2294 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 46/300\n",
      "1250/1250 - 4s - loss: 49.6909 - mse: 49.6909 - val_loss: 46.4948 - val_mse: 46.4948 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "1250/1250 - 4s - loss: 50.7820 - mse: 50.7820 - val_loss: 55.3765 - val_mse: 55.3765 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 48/300\n",
      "1250/1250 - 5s - loss: 41.6476 - mse: 41.6476 - val_loss: 42.4312 - val_mse: 42.4312 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 49/300\n",
      "1250/1250 - 5s - loss: 41.4295 - mse: 41.4295 - val_loss: 40.6988 - val_mse: 40.6988 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 50/300\n",
      "1250/1250 - 4s - loss: 41.2493 - mse: 41.2493 - val_loss: 41.3795 - val_mse: 41.3795 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 51/300\n",
      "1250/1250 - 4s - loss: 40.9076 - mse: 40.9076 - val_loss: 40.7871 - val_mse: 40.7871 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 52/300\n",
      "1250/1250 - 4s - loss: 40.8978 - mse: 40.8978 - val_loss: 41.8193 - val_mse: 41.8193 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 53/300\n",
      "1250/1250 - 4s - loss: 40.6745 - mse: 40.6745 - val_loss: 40.8935 - val_mse: 40.8935 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 54/300\n",
      "1250/1250 - 5s - loss: 40.5031 - mse: 40.5031 - val_loss: 40.1658 - val_mse: 40.1658 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 55/300\n",
      "1250/1250 - 4s - loss: 40.4872 - mse: 40.4872 - val_loss: 41.2857 - val_mse: 41.2857 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 56/300\n",
      "1250/1250 - 4s - loss: 40.3930 - mse: 40.3930 - val_loss: 41.4191 - val_mse: 41.4191 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 57/300\n",
      "1250/1250 - 4s - loss: 40.2479 - mse: 40.2479 - val_loss: 40.1821 - val_mse: 40.1821 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 58/300\n",
      "1250/1250 - 4s - loss: 40.1938 - mse: 40.1938 - val_loss: 40.1237 - val_mse: 40.1237 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 59/300\n",
      "1250/1250 - 5s - loss: 40.1230 - mse: 40.1230 - val_loss: 40.2361 - val_mse: 40.2361 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 60/300\n",
      "1250/1250 - 5s - loss: 40.0634 - mse: 40.0634 - val_loss: 41.3424 - val_mse: 41.3424 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 61/300\n",
      "1250/1250 - 4s - loss: 39.9348 - mse: 39.9348 - val_loss: 39.4959 - val_mse: 39.4959 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 62/300\n",
      "1250/1250 - 5s - loss: 39.8382 - mse: 39.8382 - val_loss: 40.2677 - val_mse: 40.2677 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 63/300\n",
      "1250/1250 - 5s - loss: 39.7355 - mse: 39.7355 - val_loss: 39.3472 - val_mse: 39.3472 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 64/300\n",
      "1250/1250 - 5s - loss: 39.6334 - mse: 39.6334 - val_loss: 39.5373 - val_mse: 39.5373 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 65/300\n",
      "1250/1250 - 5s - loss: 39.5834 - mse: 39.5834 - val_loss: 39.7239 - val_mse: 39.7239 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 66/300\n",
      "1250/1250 - 6s - loss: 39.5544 - mse: 39.5544 - val_loss: 40.1144 - val_mse: 40.1144 - lr: 9.7656e-06 - 6s/epoch - 5ms/step\n",
      "Epoch 67/300\n",
      "1250/1250 - 7s - loss: 39.6748 - mse: 39.6748 - val_loss: 40.0296 - val_mse: 40.0296 - lr: 9.7656e-06 - 7s/epoch - 5ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "1250/1250 - 7s - loss: 39.4860 - mse: 39.4860 - val_loss: 41.3707 - val_mse: 41.3707 - lr: 9.7656e-06 - 7s/epoch - 6ms/step\n",
      "Epoch 69/300\n",
      "1250/1250 - 6s - loss: 38.6583 - mse: 38.6583 - val_loss: 39.0759 - val_mse: 39.0759 - lr: 2.4414e-06 - 6s/epoch - 5ms/step\n",
      "Epoch 70/300\n",
      "1250/1250 - 5s - loss: 38.5907 - mse: 38.5907 - val_loss: 39.1580 - val_mse: 39.1580 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 71/300\n",
      "1250/1250 - 5s - loss: 38.5963 - mse: 38.5963 - val_loss: 38.8790 - val_mse: 38.8790 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 72/300\n",
      "1250/1250 - 5s - loss: 38.4339 - mse: 38.4339 - val_loss: 38.8490 - val_mse: 38.8490 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 73/300\n",
      "1250/1250 - 5s - loss: 38.4886 - mse: 38.4886 - val_loss: 39.0089 - val_mse: 39.0089 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 74/300\n",
      "1250/1250 - 4s - loss: 38.4339 - mse: 38.4339 - val_loss: 38.9173 - val_mse: 38.9173 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 75/300\n",
      "1250/1250 - 5s - loss: 38.4845 - mse: 38.4845 - val_loss: 38.7342 - val_mse: 38.7342 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 76/300\n",
      "1250/1250 - 5s - loss: 38.3463 - mse: 38.3463 - val_loss: 38.6932 - val_mse: 38.6932 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 77/300\n",
      "1250/1250 - 5s - loss: 38.4270 - mse: 38.4270 - val_loss: 38.5869 - val_mse: 38.5869 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 78/300\n",
      "1250/1250 - 5s - loss: 38.4230 - mse: 38.4230 - val_loss: 38.9417 - val_mse: 38.9417 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 79/300\n",
      "1250/1250 - 4s - loss: 38.3738 - mse: 38.3738 - val_loss: 38.5632 - val_mse: 38.5632 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 80/300\n",
      "1250/1250 - 4s - loss: 38.3237 - mse: 38.3237 - val_loss: 38.2782 - val_mse: 38.2782 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 81/300\n",
      "1250/1250 - 4s - loss: 38.3373 - mse: 38.3373 - val_loss: 38.8013 - val_mse: 38.8013 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 82/300\n",
      "1250/1250 - 4s - loss: 38.3306 - mse: 38.3306 - val_loss: 38.6218 - val_mse: 38.6218 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 83/300\n",
      "1250/1250 - 4s - loss: 38.2826 - mse: 38.2826 - val_loss: 38.7909 - val_mse: 38.7909 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 84/300\n",
      "1250/1250 - 5s - loss: 38.1958 - mse: 38.1958 - val_loss: 39.1351 - val_mse: 39.1351 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 85/300\n",
      "1250/1250 - 4s - loss: 38.1840 - mse: 38.1840 - val_loss: 38.0825 - val_mse: 38.0825 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 86/300\n",
      "1250/1250 - 5s - loss: 38.0886 - mse: 38.0886 - val_loss: 38.2196 - val_mse: 38.2196 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 87/300\n",
      "1250/1250 - 5s - loss: 38.0995 - mse: 38.0995 - val_loss: 37.9932 - val_mse: 37.9932 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 88/300\n",
      "1250/1250 - 4s - loss: 38.0413 - mse: 38.0413 - val_loss: 37.9729 - val_mse: 37.9729 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 89/300\n",
      "1250/1250 - 5s - loss: 38.0848 - mse: 38.0848 - val_loss: 38.1534 - val_mse: 38.1534 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 90/300\n",
      "1250/1250 - 5s - loss: 38.0449 - mse: 38.0449 - val_loss: 38.1141 - val_mse: 38.1141 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/300\n",
      "1250/1250 - 5s - loss: 38.0542 - mse: 38.0542 - val_loss: 38.1236 - val_mse: 38.1236 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 92/300\n",
      "1250/1250 - 4s - loss: 38.0397 - mse: 38.0397 - val_loss: 38.2998 - val_mse: 38.2998 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "1250/1250 - 4s - loss: 38.0572 - mse: 38.0572 - val_loss: 38.2018 - val_mse: 38.2018 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 94/300\n",
      "1250/1250 - 4s - loss: 37.9294 - mse: 37.9294 - val_loss: 37.9833 - val_mse: 37.9833 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 95/300\n",
      "1250/1250 - 4s - loss: 37.8480 - mse: 37.8480 - val_loss: 38.0872 - val_mse: 38.0872 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 96/300\n",
      "1250/1250 - 4s - loss: 37.8834 - mse: 37.8834 - val_loss: 38.2015 - val_mse: 38.2015 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 97/300\n",
      "1250/1250 - 5s - loss: 37.8578 - mse: 37.8578 - val_loss: 38.1036 - val_mse: 38.1036 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "1250/1250 - 4s - loss: 37.8143 - mse: 37.8143 - val_loss: 38.0225 - val_mse: 38.0225 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 99/300\n",
      "1250/1250 - 4s - loss: 37.8252 - mse: 37.8252 - val_loss: 37.9752 - val_mse: 37.9752 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 100/300\n",
      "1250/1250 - 5s - loss: 37.7785 - mse: 37.7785 - val_loss: 37.9833 - val_mse: 37.9833 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 101/300\n",
      "1250/1250 - 4s - loss: 37.7832 - mse: 37.7832 - val_loss: 38.0046 - val_mse: 38.0046 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 102/300\n",
      "1250/1250 - 4s - loss: 37.7953 - mse: 37.7953 - val_loss: 37.9878 - val_mse: 37.9878 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "1250/1250 - 5s - loss: 37.7786 - mse: 37.7786 - val_loss: 38.0118 - val_mse: 38.0118 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 104/300\n",
      "1250/1250 - 5s - loss: 37.7859 - mse: 37.7859 - val_loss: 37.8955 - val_mse: 37.8955 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 105/300\n",
      "1250/1250 - 5s - loss: 37.7863 - mse: 37.7863 - val_loss: 37.9638 - val_mse: 37.9638 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 106/300\n",
      "1250/1250 - 5s - loss: 37.7911 - mse: 37.7911 - val_loss: 37.9230 - val_mse: 37.9230 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 107/300\n",
      "1250/1250 - 4s - loss: 37.8094 - mse: 37.8094 - val_loss: 37.9628 - val_mse: 37.9628 - lr: 3.8147e-08 - 4s/epoch - 4ms/step\n",
      "Epoch 108/300\n",
      "1250/1250 - 4s - loss: 37.8178 - mse: 37.8178 - val_loss: 37.9602 - val_mse: 37.9602 - lr: 3.8147e-08 - 4s/epoch - 3ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "1250/1250 - 5s - loss: 37.8151 - mse: 37.8151 - val_loss: 37.9641 - val_mse: 37.9641 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 110/300\n",
      "1250/1250 - 4s - loss: 37.7908 - mse: 37.7908 - val_loss: 37.9700 - val_mse: 37.9700 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 111/300\n",
      "1250/1250 - 4s - loss: 37.7946 - mse: 37.7946 - val_loss: 37.9698 - val_mse: 37.9698 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 112/300\n",
      "1250/1250 - 4s - loss: 37.7891 - mse: 37.7891 - val_loss: 37.9542 - val_mse: 37.9542 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 113/300\n",
      "1250/1250 - 4s - loss: 37.7922 - mse: 37.7922 - val_loss: 37.9727 - val_mse: 37.9727 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "1250/1250 - 4s - loss: 37.8033 - mse: 37.8033 - val_loss: 37.9498 - val_mse: 37.9498 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 115/300\n",
      "1250/1250 - 4s - loss: 37.7749 - mse: 37.7749 - val_loss: 37.9469 - val_mse: 37.9469 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 116/300\n",
      "1250/1250 - 4s - loss: 37.7765 - mse: 37.7765 - val_loss: 37.9505 - val_mse: 37.9505 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 117/300\n",
      "1250/1250 - 4s - loss: 37.7698 - mse: 37.7698 - val_loss: 37.9477 - val_mse: 37.9477 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 118/300\n",
      "1250/1250 - 4s - loss: 37.7709 - mse: 37.7709 - val_loss: 37.9495 - val_mse: 37.9495 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "1250/1250 - 4s - loss: 37.7698 - mse: 37.7698 - val_loss: 37.9392 - val_mse: 37.9392 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 120/300\n",
      "1250/1250 - 5s - loss: 37.7630 - mse: 37.7630 - val_loss: 37.9390 - val_mse: 37.9390 - lr: 5.9605e-10 - 5s/epoch - 4ms/step\n",
      "Epoch 121/300\n",
      "1250/1250 - 4s - loss: 37.7631 - mse: 37.7631 - val_loss: 37.9394 - val_mse: 37.9394 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 122/300\n",
      "1250/1250 - 4s - loss: 37.7631 - mse: 37.7631 - val_loss: 37.9392 - val_mse: 37.9392 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 123/300\n",
      "Restoring model weights from the end of the best epoch: 104.\n",
      "1250/1250 - 4s - loss: 37.7639 - mse: 37.7639 - val_loss: 37.9391 - val_mse: 37.9391 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 123: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 9.4min\n",
      "1250/1250 - 2s - loss: 37.0095 - mse: 37.0095 - 2s/epoch - 2ms/step\n",
      "5000/5000 - 9s - loss: 36.5960 - mse: 36.5960 - 9s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x00000125F19B0760>; total time= 9.4min\n",
      "Epoch 1/300\n",
      "1250/1250 - 5s - loss: 3318278.0000 - mse: 3318278.0000 - val_loss: 2156719.2500 - val_mse: 2156719.2500 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 4s - loss: 1516035.8750 - mse: 1516035.8750 - val_loss: 1210810.6250 - val_mse: 1210810.6250 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 5s - loss: 1072234.2500 - mse: 1072234.2500 - val_loss: 903735.3125 - val_mse: 903735.3125 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 4s - loss: 832876.0625 - mse: 832876.0625 - val_loss: 170997.5000 - val_mse: 170997.5000 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 5/300\n",
      "1250/1250 - 4s - loss: 687373.4375 - mse: 687373.4375 - val_loss: 936262.8750 - val_mse: 936262.8750 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 5s - loss: 569365.3750 - mse: 569365.3750 - val_loss: 845077.7500 - val_mse: 845077.7500 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 4s - loss: 489305.2500 - mse: 489305.2500 - val_loss: 1101165.7500 - val_mse: 1101165.7500 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 5s - loss: 432021.0938 - mse: 432021.0938 - val_loss: 341643.1250 - val_mse: 341643.1250 - lr: 0.0100 - 5s/epoch - 4ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 4s - loss: 395587.2500 - mse: 395587.2500 - val_loss: 440944.6875 - val_mse: 440944.6875 - lr: 0.0100 - 4s/epoch - 4ms/step\n",
      "Epoch 10/300\n",
      "1250/1250 - 4s - loss: 32495.8555 - mse: 32495.8555 - val_loss: 21929.8535 - val_mse: 21929.8535 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 4s - loss: 30468.3613 - mse: 30468.3613 - val_loss: 44419.4688 - val_mse: 44419.4688 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 4s - loss: 29985.5840 - mse: 29985.5840 - val_loss: 61759.3164 - val_mse: 61759.3164 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 4s - loss: 29447.2891 - mse: 29447.2891 - val_loss: 14377.7695 - val_mse: 14377.7695 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 4s - loss: 29044.9277 - mse: 29044.9277 - val_loss: 33262.4688 - val_mse: 33262.4688 - lr: 0.0025 - 4s/epoch - 3ms/step\n",
      "Epoch 15/300\n",
      "1250/1250 - 4s - loss: 28729.6836 - mse: 28729.6836 - val_loss: 2005.8054 - val_mse: 2005.8054 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 4s - loss: 28278.7422 - mse: 28278.7422 - val_loss: 7276.6650 - val_mse: 7276.6650 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 4s - loss: 28162.2402 - mse: 28162.2402 - val_loss: 28499.3438 - val_mse: 28499.3438 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 4s - loss: 27628.7383 - mse: 27628.7383 - val_loss: 34198.7891 - val_mse: 34198.7891 - lr: 0.0025 - 4s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300\n",
      "1250/1250 - 4s - loss: 27592.4414 - mse: 27592.4414 - val_loss: 103300.0078 - val_mse: 103300.0078 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1250/1250 - 4s - loss: 27290.4219 - mse: 27290.4219 - val_loss: 43306.2344 - val_mse: 43306.2344 - lr: 0.0025 - 4s/epoch - 4ms/step\n",
      "Epoch 21/300\n",
      "1250/1250 - 4s - loss: 2025.4240 - mse: 2025.4240 - val_loss: 683.1229 - val_mse: 683.1229 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 22/300\n",
      "1250/1250 - 4s - loss: 1941.3455 - mse: 1941.3455 - val_loss: 572.5577 - val_mse: 572.5577 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 23/300\n",
      "1250/1250 - 4s - loss: 1914.9138 - mse: 1914.9138 - val_loss: 878.2875 - val_mse: 878.2875 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 24/300\n",
      "1250/1250 - 4s - loss: 1885.3704 - mse: 1885.3704 - val_loss: 458.4901 - val_mse: 458.4901 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 25/300\n",
      "1250/1250 - 4s - loss: 1928.2152 - mse: 1928.2152 - val_loss: 1487.3523 - val_mse: 1487.3523 - lr: 6.2500e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 26/300\n",
      "1250/1250 - 4s - loss: 1958.5828 - mse: 1958.5828 - val_loss: 1034.1782 - val_mse: 1034.1782 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 27/300\n",
      "1250/1250 - 4s - loss: 1921.9489 - mse: 1921.9489 - val_loss: 248.3835 - val_mse: 248.3835 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 28/300\n",
      "1250/1250 - 4s - loss: 1867.7087 - mse: 1867.7087 - val_loss: 899.5322 - val_mse: 899.5322 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 29/300\n",
      "1250/1250 - 5s - loss: 1860.0887 - mse: 1860.0887 - val_loss: 605.7722 - val_mse: 605.7722 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 30/300\n",
      "1250/1250 - 4s - loss: 1915.0522 - mse: 1915.0522 - val_loss: 378.2812 - val_mse: 378.2812 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 31/300\n",
      "1250/1250 - 4s - loss: 1932.8972 - mse: 1932.8972 - val_loss: 1215.0977 - val_mse: 1215.0977 - lr: 6.2500e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "1250/1250 - 5s - loss: 1920.2902 - mse: 1920.2902 - val_loss: 1399.4919 - val_mse: 1399.4919 - lr: 6.2500e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 33/300\n",
      "1250/1250 - 5s - loss: 180.4332 - mse: 180.4332 - val_loss: 99.1209 - val_mse: 99.1209 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 34/300\n",
      "1250/1250 - 4s - loss: 176.4188 - mse: 176.4188 - val_loss: 68.8396 - val_mse: 68.8396 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 35/300\n",
      "1250/1250 - 4s - loss: 171.8155 - mse: 171.8155 - val_loss: 87.4078 - val_mse: 87.4078 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 36/300\n",
      "1250/1250 - 4s - loss: 166.4105 - mse: 166.4105 - val_loss: 116.9647 - val_mse: 116.9647 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 37/300\n",
      "1250/1250 - 4s - loss: 169.0420 - mse: 169.0420 - val_loss: 513.1816 - val_mse: 513.1816 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 38/300\n",
      "1250/1250 - 4s - loss: 166.6116 - mse: 166.6116 - val_loss: 245.6037 - val_mse: 245.6037 - lr: 1.5625e-04 - 4s/epoch - 3ms/step\n",
      "Epoch 39/300\n",
      "1250/1250 - 4s - loss: 169.1198 - mse: 169.1198 - val_loss: 60.3343 - val_mse: 60.3343 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 40/300\n",
      "1250/1250 - 5s - loss: 167.1960 - mse: 167.1960 - val_loss: 66.8242 - val_mse: 66.8242 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 41/300\n",
      "1250/1250 - 5s - loss: 175.5383 - mse: 175.5383 - val_loss: 61.8623 - val_mse: 61.8623 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 42/300\n",
      "1250/1250 - 5s - loss: 176.4865 - mse: 176.4865 - val_loss: 87.3118 - val_mse: 87.3118 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 43/300\n",
      "1250/1250 - 4s - loss: 176.1912 - mse: 176.1912 - val_loss: 223.1566 - val_mse: 223.1566 - lr: 1.5625e-04 - 4s/epoch - 4ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "1250/1250 - 5s - loss: 177.2702 - mse: 177.2702 - val_loss: 121.5098 - val_mse: 121.5098 - lr: 1.5625e-04 - 5s/epoch - 4ms/step\n",
      "Epoch 45/300\n",
      "1250/1250 - 5s - loss: 58.8854 - mse: 58.8854 - val_loss: 98.2961 - val_mse: 98.2961 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 46/300\n",
      "1250/1250 - 5s - loss: 57.4094 - mse: 57.4094 - val_loss: 55.8584 - val_mse: 55.8584 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 47/300\n",
      "1250/1250 - 4s - loss: 57.5612 - mse: 57.5612 - val_loss: 79.1274 - val_mse: 79.1274 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 48/300\n",
      "1250/1250 - 4s - loss: 56.3882 - mse: 56.3882 - val_loss: 66.4780 - val_mse: 66.4780 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 49/300\n",
      "1250/1250 - 4s - loss: 55.8306 - mse: 55.8306 - val_loss: 50.0079 - val_mse: 50.0079 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 50/300\n",
      "1250/1250 - 4s - loss: 55.7301 - mse: 55.7301 - val_loss: 53.8760 - val_mse: 53.8760 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 51/300\n",
      "1250/1250 - 5s - loss: 55.2562 - mse: 55.2562 - val_loss: 65.9092 - val_mse: 65.9092 - lr: 3.9062e-05 - 5s/epoch - 4ms/step\n",
      "Epoch 52/300\n",
      "1250/1250 - 4s - loss: 55.3741 - mse: 55.3741 - val_loss: 66.6158 - val_mse: 66.6158 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 53/300\n",
      "1250/1250 - 4s - loss: 55.2780 - mse: 55.2780 - val_loss: 59.4859 - val_mse: 59.4859 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "1250/1250 - 4s - loss: 54.5953 - mse: 54.5953 - val_loss: 50.6598 - val_mse: 50.6598 - lr: 3.9062e-05 - 4s/epoch - 4ms/step\n",
      "Epoch 55/300\n",
      "1250/1250 - 4s - loss: 47.5089 - mse: 47.5089 - val_loss: 48.2047 - val_mse: 48.2047 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 56/300\n",
      "1250/1250 - 4s - loss: 47.2042 - mse: 47.2042 - val_loss: 47.8310 - val_mse: 47.8310 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 57/300\n",
      "1250/1250 - 4s - loss: 47.0753 - mse: 47.0753 - val_loss: 47.6325 - val_mse: 47.6325 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 58/300\n",
      "1250/1250 - 4s - loss: 47.0680 - mse: 47.0680 - val_loss: 47.4929 - val_mse: 47.4929 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 59/300\n",
      "1250/1250 - 5s - loss: 46.8631 - mse: 46.8631 - val_loss: 47.1687 - val_mse: 47.1687 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 60/300\n",
      "1250/1250 - 5s - loss: 46.8524 - mse: 46.8524 - val_loss: 47.1460 - val_mse: 47.1460 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 61/300\n",
      "1250/1250 - 4s - loss: 46.7190 - mse: 46.7190 - val_loss: 47.0290 - val_mse: 47.0290 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 62/300\n",
      "1250/1250 - 4s - loss: 46.7315 - mse: 46.7315 - val_loss: 46.9407 - val_mse: 46.9407 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 63/300\n",
      "1250/1250 - 4s - loss: 46.6289 - mse: 46.6289 - val_loss: 50.2884 - val_mse: 50.2884 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 64/300\n",
      "1250/1250 - 5s - loss: 46.5031 - mse: 46.5031 - val_loss: 46.8406 - val_mse: 46.8406 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 65/300\n",
      "1250/1250 - 4s - loss: 46.5003 - mse: 46.5003 - val_loss: 46.9623 - val_mse: 46.9623 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 66/300\n",
      "1250/1250 - 4s - loss: 46.4433 - mse: 46.4433 - val_loss: 46.8978 - val_mse: 46.8978 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 67/300\n",
      "1250/1250 - 5s - loss: 46.3535 - mse: 46.3535 - val_loss: 49.4249 - val_mse: 49.4249 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 68/300\n",
      "1250/1250 - 4s - loss: 46.2661 - mse: 46.2661 - val_loss: 46.1725 - val_mse: 46.1725 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 69/300\n",
      "1250/1250 - 4s - loss: 46.1797 - mse: 46.1797 - val_loss: 47.2242 - val_mse: 47.2242 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 70/300\n",
      "1250/1250 - 4s - loss: 46.1909 - mse: 46.1909 - val_loss: 46.4735 - val_mse: 46.4735 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 71/300\n",
      "1250/1250 - 4s - loss: 46.2363 - mse: 46.2363 - val_loss: 45.9677 - val_mse: 45.9677 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 72/300\n",
      "1250/1250 - 4s - loss: 46.0748 - mse: 46.0748 - val_loss: 46.2398 - val_mse: 46.2398 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 73/300\n",
      "1250/1250 - 5s - loss: 45.9891 - mse: 45.9891 - val_loss: 46.1126 - val_mse: 46.1126 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 74/300\n",
      "1250/1250 - 5s - loss: 45.8917 - mse: 45.8917 - val_loss: 47.0285 - val_mse: 47.0285 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 75/300\n",
      "1250/1250 - 4s - loss: 45.7364 - mse: 45.7364 - val_loss: 48.4372 - val_mse: 48.4372 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300\n",
      "1250/1250 - 4s - loss: 45.6684 - mse: 45.6684 - val_loss: 45.2907 - val_mse: 45.2907 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 77/300\n",
      "1250/1250 - 4s - loss: 45.4499 - mse: 45.4499 - val_loss: 47.2768 - val_mse: 47.2768 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 78/300\n",
      "1250/1250 - 4s - loss: 45.3510 - mse: 45.3510 - val_loss: 46.2005 - val_mse: 46.2005 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 79/300\n",
      "1250/1250 - 4s - loss: 45.4483 - mse: 45.4483 - val_loss: 45.6959 - val_mse: 45.6959 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 80/300\n",
      "1250/1250 - 4s - loss: 45.3341 - mse: 45.3341 - val_loss: 45.1496 - val_mse: 45.1496 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 81/300\n",
      "1250/1250 - 4s - loss: 45.1049 - mse: 45.1049 - val_loss: 46.5648 - val_mse: 46.5648 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 82/300\n",
      "1250/1250 - 4s - loss: 45.0713 - mse: 45.0713 - val_loss: 45.8406 - val_mse: 45.8406 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 83/300\n",
      "1250/1250 - 4s - loss: 45.0978 - mse: 45.0978 - val_loss: 47.3254 - val_mse: 47.3254 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 84/300\n",
      "1250/1250 - 5s - loss: 45.0708 - mse: 45.0708 - val_loss: 44.6517 - val_mse: 44.6517 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 85/300\n",
      "1250/1250 - 4s - loss: 44.8379 - mse: 44.8379 - val_loss: 46.1212 - val_mse: 46.1212 - lr: 9.7656e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 86/300\n",
      "1250/1250 - 4s - loss: 44.6897 - mse: 44.6897 - val_loss: 45.2445 - val_mse: 45.2445 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 87/300\n",
      "1250/1250 - 5s - loss: 44.7451 - mse: 44.7451 - val_loss: 45.2006 - val_mse: 45.2006 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 88/300\n",
      "1250/1250 - 5s - loss: 44.7617 - mse: 44.7617 - val_loss: 45.9851 - val_mse: 45.9851 - lr: 9.7656e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "1250/1250 - 4s - loss: 44.6171 - mse: 44.6171 - val_loss: 47.7573 - val_mse: 47.7573 - lr: 9.7656e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 90/300\n",
      "1250/1250 - 4s - loss: 43.9001 - mse: 43.9001 - val_loss: 44.7028 - val_mse: 44.7028 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 91/300\n",
      "1250/1250 - 5s - loss: 43.9537 - mse: 43.9537 - val_loss: 44.9922 - val_mse: 44.9922 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 92/300\n",
      "1250/1250 - 4s - loss: 43.8656 - mse: 43.8656 - val_loss: 44.7866 - val_mse: 44.7866 - lr: 2.4414e-06 - 4s/epoch - 3ms/step\n",
      "Epoch 93/300\n",
      "1250/1250 - 4s - loss: 43.8486 - mse: 43.8486 - val_loss: 44.5702 - val_mse: 44.5702 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 94/300\n",
      "1250/1250 - 4s - loss: 43.7805 - mse: 43.7805 - val_loss: 44.6898 - val_mse: 44.6898 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 95/300\n",
      "1250/1250 - 5s - loss: 43.7953 - mse: 43.7953 - val_loss: 45.4594 - val_mse: 45.4594 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 96/300\n",
      "1250/1250 - 4s - loss: 43.7694 - mse: 43.7694 - val_loss: 44.8645 - val_mse: 44.8645 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 97/300\n",
      "1250/1250 - 5s - loss: 43.8111 - mse: 43.8111 - val_loss: 45.4954 - val_mse: 45.4954 - lr: 2.4414e-06 - 5s/epoch - 4ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "1250/1250 - 4s - loss: 43.7351 - mse: 43.7351 - val_loss: 45.0807 - val_mse: 45.0807 - lr: 2.4414e-06 - 4s/epoch - 4ms/step\n",
      "Epoch 99/300\n",
      "1250/1250 - 4s - loss: 43.6529 - mse: 43.6529 - val_loss: 44.5258 - val_mse: 44.5258 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 100/300\n",
      "1250/1250 - 4s - loss: 43.6534 - mse: 43.6534 - val_loss: 44.5982 - val_mse: 44.5982 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 101/300\n",
      "1250/1250 - 4s - loss: 43.6239 - mse: 43.6239 - val_loss: 44.5773 - val_mse: 44.5773 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 102/300\n",
      "1250/1250 - 5s - loss: 43.5822 - mse: 43.5822 - val_loss: 44.4915 - val_mse: 44.4915 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 103/300\n",
      "1250/1250 - 5s - loss: 43.5876 - mse: 43.5876 - val_loss: 44.4634 - val_mse: 44.4634 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 104/300\n",
      "1250/1250 - 4s - loss: 43.5799 - mse: 43.5799 - val_loss: 44.6748 - val_mse: 44.6748 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 105/300\n",
      "1250/1250 - 4s - loss: 43.5884 - mse: 43.5884 - val_loss: 44.4763 - val_mse: 44.4763 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 106/300\n",
      "1250/1250 - 4s - loss: 43.6199 - mse: 43.6199 - val_loss: 44.5541 - val_mse: 44.5541 - lr: 6.1035e-07 - 4s/epoch - 3ms/step\n",
      "Epoch 107/300\n",
      "1250/1250 - 4s - loss: 43.5764 - mse: 43.5764 - val_loss: 44.4868 - val_mse: 44.4868 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 108/300\n",
      "1250/1250 - 4s - loss: 43.5631 - mse: 43.5631 - val_loss: 44.4244 - val_mse: 44.4244 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 109/300\n",
      "1250/1250 - 4s - loss: 43.5391 - mse: 43.5391 - val_loss: 44.4697 - val_mse: 44.4697 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 110/300\n",
      "1250/1250 - 4s - loss: 43.5501 - mse: 43.5501 - val_loss: 44.4293 - val_mse: 44.4293 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 111/300\n",
      "1250/1250 - 4s - loss: 43.5953 - mse: 43.5953 - val_loss: 44.5753 - val_mse: 44.5753 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 112/300\n",
      "1250/1250 - 5s - loss: 43.5527 - mse: 43.5527 - val_loss: 44.5505 - val_mse: 44.5505 - lr: 6.1035e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "1250/1250 - 4s - loss: 43.5603 - mse: 43.5603 - val_loss: 44.4585 - val_mse: 44.4585 - lr: 6.1035e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 114/300\n",
      "1250/1250 - 4s - loss: 43.5677 - mse: 43.5677 - val_loss: 44.5330 - val_mse: 44.5330 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 115/300\n",
      "1250/1250 - 4s - loss: 43.5566 - mse: 43.5566 - val_loss: 44.4574 - val_mse: 44.4574 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 116/300\n",
      "1250/1250 - 5s - loss: 43.5503 - mse: 43.5503 - val_loss: 44.4315 - val_mse: 44.4315 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 117/300\n",
      "1250/1250 - 5s - loss: 43.5671 - mse: 43.5671 - val_loss: 44.6039 - val_mse: 44.6039 - lr: 1.5259e-07 - 5s/epoch - 4ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "1250/1250 - 4s - loss: 43.5546 - mse: 43.5546 - val_loss: 44.4414 - val_mse: 44.4414 - lr: 1.5259e-07 - 4s/epoch - 4ms/step\n",
      "Epoch 119/300\n",
      "1250/1250 - 5s - loss: 43.5043 - mse: 43.5043 - val_loss: 44.4212 - val_mse: 44.4212 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 120/300\n",
      "1250/1250 - 4s - loss: 43.4994 - mse: 43.4994 - val_loss: 44.4214 - val_mse: 44.4214 - lr: 3.8147e-08 - 4s/epoch - 4ms/step\n",
      "Epoch 121/300\n",
      "1250/1250 - 5s - loss: 43.5249 - mse: 43.5249 - val_loss: 44.4385 - val_mse: 44.4385 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 122/300\n",
      "1250/1250 - 5s - loss: 43.4994 - mse: 43.4994 - val_loss: 44.4326 - val_mse: 44.4326 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 123/300\n",
      "1250/1250 - 5s - loss: 43.4942 - mse: 43.4942 - val_loss: 44.4390 - val_mse: 44.4390 - lr: 3.8147e-08 - 5s/epoch - 4ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "1250/1250 - 4s - loss: 43.4968 - mse: 43.4968 - val_loss: 44.4507 - val_mse: 44.4507 - lr: 3.8147e-08 - 4s/epoch - 4ms/step\n",
      "Epoch 125/300\n",
      "1250/1250 - 4s - loss: 43.5028 - mse: 43.5028 - val_loss: 44.4616 - val_mse: 44.4616 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 126/300\n",
      "1250/1250 - 5s - loss: 43.4941 - mse: 43.4941 - val_loss: 44.4654 - val_mse: 44.4654 - lr: 9.5367e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 127/300\n",
      "1250/1250 - 5s - loss: 43.4840 - mse: 43.4840 - val_loss: 44.4609 - val_mse: 44.4609 - lr: 9.5367e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 128/300\n",
      "1250/1250 - 5s - loss: 43.5005 - mse: 43.5005 - val_loss: 44.4240 - val_mse: 44.4240 - lr: 9.5367e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "1250/1250 - 4s - loss: 43.4856 - mse: 43.4856 - val_loss: 44.4398 - val_mse: 44.4398 - lr: 9.5367e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 130/300\n",
      "1250/1250 - 4s - loss: 43.5002 - mse: 43.5002 - val_loss: 44.4380 - val_mse: 44.4380 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 131/300\n",
      "1250/1250 - 4s - loss: 43.5063 - mse: 43.5063 - val_loss: 44.4446 - val_mse: 44.4446 - lr: 2.3842e-09 - 4s/epoch - 3ms/step\n",
      "Epoch 132/300\n",
      "1250/1250 - 4s - loss: 43.5082 - mse: 43.5082 - val_loss: 44.4184 - val_mse: 44.4184 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/300\n",
      "1250/1250 - 4s - loss: 43.4878 - mse: 43.4878 - val_loss: 44.4241 - val_mse: 44.4241 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 134/300\n",
      "1250/1250 - 4s - loss: 43.4838 - mse: 43.4838 - val_loss: 44.4021 - val_mse: 44.4021 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 135/300\n",
      "1250/1250 - 5s - loss: 43.4845 - mse: 43.4845 - val_loss: 44.4029 - val_mse: 44.4029 - lr: 2.3842e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 136/300\n",
      "1250/1250 - 5s - loss: 43.4846 - mse: 43.4846 - val_loss: 44.3976 - val_mse: 44.3976 - lr: 2.3842e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 137/300\n",
      "1250/1250 - 4s - loss: 43.4788 - mse: 43.4788 - val_loss: 44.3746 - val_mse: 44.3746 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 138/300\n",
      "1250/1250 - 5s - loss: 43.4668 - mse: 43.4668 - val_loss: 44.3771 - val_mse: 44.3771 - lr: 2.3842e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 139/300\n",
      "1250/1250 - 4s - loss: 43.4680 - mse: 43.4680 - val_loss: 44.3894 - val_mse: 44.3894 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 140/300\n",
      "1250/1250 - 4s - loss: 43.4501 - mse: 43.4501 - val_loss: 44.3737 - val_mse: 44.3737 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 141/300\n",
      "1250/1250 - 4s - loss: 43.4419 - mse: 43.4419 - val_loss: 44.3694 - val_mse: 44.3694 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 142/300\n",
      "1250/1250 - 4s - loss: 43.4445 - mse: 43.4445 - val_loss: 44.3673 - val_mse: 44.3673 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 143/300\n",
      "1250/1250 - 4s - loss: 43.4404 - mse: 43.4404 - val_loss: 44.3585 - val_mse: 44.3585 - lr: 2.3842e-09 - 4s/epoch - 3ms/step\n",
      "Epoch 144/300\n",
      "1250/1250 - 4s - loss: 43.4452 - mse: 43.4452 - val_loss: 44.3639 - val_mse: 44.3639 - lr: 2.3842e-09 - 4s/epoch - 3ms/step\n",
      "Epoch 145/300\n",
      "1250/1250 - 4s - loss: 43.4376 - mse: 43.4376 - val_loss: 44.3466 - val_mse: 44.3466 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 146/300\n",
      "1250/1250 - 4s - loss: 43.4331 - mse: 43.4331 - val_loss: 44.3468 - val_mse: 44.3468 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 147/300\n",
      "1250/1250 - 4s - loss: 43.4316 - mse: 43.4316 - val_loss: 44.3523 - val_mse: 44.3523 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 148/300\n",
      "1250/1250 - 5s - loss: 43.4221 - mse: 43.4221 - val_loss: 44.3571 - val_mse: 44.3571 - lr: 2.3842e-09 - 5s/epoch - 4ms/step\n",
      "Epoch 149/300\n",
      "1250/1250 - 4s - loss: 43.4373 - mse: 43.4373 - val_loss: 44.3577 - val_mse: 44.3577 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "1250/1250 - 4s - loss: 43.4436 - mse: 43.4436 - val_loss: 44.3582 - val_mse: 44.3582 - lr: 2.3842e-09 - 4s/epoch - 4ms/step\n",
      "Epoch 151/300\n",
      "1250/1250 - 5s - loss: 43.4516 - mse: 43.4516 - val_loss: 44.3609 - val_mse: 44.3609 - lr: 5.9605e-10 - 5s/epoch - 4ms/step\n",
      "Epoch 152/300\n",
      "1250/1250 - 5s - loss: 43.4506 - mse: 43.4506 - val_loss: 44.3590 - val_mse: 44.3590 - lr: 5.9605e-10 - 5s/epoch - 4ms/step\n",
      "Epoch 153/300\n",
      "1250/1250 - 4s - loss: 43.4510 - mse: 43.4510 - val_loss: 44.3570 - val_mse: 44.3570 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 154/300\n",
      "1250/1250 - 4s - loss: 43.4515 - mse: 43.4515 - val_loss: 44.3596 - val_mse: 44.3596 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "1250/1250 - 4s - loss: 43.4523 - mse: 43.4523 - val_loss: 44.3543 - val_mse: 44.3543 - lr: 5.9605e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 156/300\n",
      "1250/1250 - 5s - loss: 43.4423 - mse: 43.4423 - val_loss: 44.3543 - val_mse: 44.3543 - lr: 1.4901e-10 - 5s/epoch - 4ms/step\n",
      "Epoch 157/300\n",
      "1250/1250 - 5s - loss: 43.4423 - mse: 43.4423 - val_loss: 44.3542 - val_mse: 44.3542 - lr: 1.4901e-10 - 5s/epoch - 4ms/step\n",
      "Epoch 158/300\n",
      "1250/1250 - 4s - loss: 43.4422 - mse: 43.4422 - val_loss: 44.3542 - val_mse: 44.3542 - lr: 1.4901e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 159/300\n",
      "1250/1250 - 4s - loss: 43.4423 - mse: 43.4423 - val_loss: 44.3542 - val_mse: 44.3542 - lr: 1.4901e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "1250/1250 - 4s - loss: 43.4424 - mse: 43.4424 - val_loss: 44.3543 - val_mse: 44.3543 - lr: 1.4901e-10 - 4s/epoch - 4ms/step\n",
      "Epoch 161/300\n",
      "1250/1250 - 4s - loss: 43.4423 - mse: 43.4423 - val_loss: 44.3543 - val_mse: 44.3543 - lr: 3.7253e-11 - 4s/epoch - 4ms/step\n",
      "Epoch 162/300\n",
      "1250/1250 - 5s - loss: 43.4424 - mse: 43.4424 - val_loss: 44.3543 - val_mse: 44.3543 - lr: 3.7253e-11 - 5s/epoch - 4ms/step\n",
      "Epoch 163/300\n",
      "1250/1250 - 5s - loss: 43.4423 - mse: 43.4423 - val_loss: 44.3543 - val_mse: 44.3543 - lr: 3.7253e-11 - 5s/epoch - 4ms/step\n",
      "Epoch 164/300\n",
      "Restoring model weights from the end of the best epoch: 145.\n",
      "1250/1250 - 5s - loss: 43.4424 - mse: 43.4424 - val_loss: 44.3548 - val_mse: 44.3548 - lr: 3.7253e-11 - 5s/epoch - 4ms/step\n",
      "Epoch 164: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=12.3min\n",
      "1250/1250 - 2s - loss: 42.7331 - mse: 42.7331 - 2s/epoch - 2ms/step\n",
      "5000/5000 - 9s - loss: 42.4595 - mse: 42.4595 - 9s/epoch - 2ms/step\n",
      "[CV] END model__activation1=relu, model__activation2=relu, model__activation3=relu, model__dropout=0, model__n_dense1=128, model__n_dense2=128, model__n_dense3=128, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x00000125F19B0760>; total time=12.3min\n",
      "Epoch 1/300\n",
      "1250/1250 - 4s - loss: 1480461.0000 - mse: 1480461.0000 - val_loss: 6854.0977 - val_mse: 6854.0977 - lr: 0.0100 - 4s/epoch - 3ms/step\n",
      "Epoch 2/300\n",
      "1250/1250 - 3s - loss: 28865.2969 - mse: 28865.2969 - val_loss: 12094.4824 - val_mse: 12094.4824 - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 3/300\n",
      "1250/1250 - 3s - loss: 31159.0078 - mse: 31159.0078 - val_loss: 4457.0234 - val_mse: 4457.0234 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 4/300\n",
      "1250/1250 - 3s - loss: 43618.0508 - mse: 43618.0508 - val_loss: 1840.4490 - val_mse: 1840.4490 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 5/300\n",
      "1250/1250 - 3s - loss: 19022.8477 - mse: 19022.8477 - val_loss: 1663.4915 - val_mse: 1663.4915 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 6/300\n",
      "1250/1250 - 3s - loss: 21078.8379 - mse: 21078.8379 - val_loss: 49501.3555 - val_mse: 49501.3555 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 7/300\n",
      "1250/1250 - 3s - loss: 16073.0107 - mse: 16073.0107 - val_loss: 3311.7727 - val_mse: 3311.7727 - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 8/300\n",
      "1250/1250 - 3s - loss: 22055.6328 - mse: 22055.6328 - val_loss: 23151.8809 - val_mse: 23151.8809 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 9/300\n",
      "1250/1250 - 3s - loss: 10950.3848 - mse: 10950.3848 - val_loss: 85104.8516 - val_mse: 85104.8516 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 10/300\n",
      "1250/1250 - 3s - loss: 18757.0293 - mse: 18757.0293 - val_loss: 283.2462 - val_mse: 283.2462 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 11/300\n",
      "1250/1250 - 3s - loss: 16154.4111 - mse: 16154.4111 - val_loss: 8601.1074 - val_mse: 8601.1074 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 12/300\n",
      "1250/1250 - 3s - loss: 10784.0586 - mse: 10784.0586 - val_loss: 354.4124 - val_mse: 354.4124 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 13/300\n",
      "1250/1250 - 3s - loss: 14107.7412 - mse: 14107.7412 - val_loss: 49592.4375 - val_mse: 49592.4375 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 14/300\n",
      "1250/1250 - 3s - loss: 14517.7549 - mse: 14517.7549 - val_loss: 4087.8184 - val_mse: 4087.8184 - lr: 0.0100 - 3s/epoch - 2ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1250/1250 - 3s - loss: 12285.1396 - mse: 12285.1396 - val_loss: 617.7551 - val_mse: 617.7551 - lr: 0.0100 - 3s/epoch - 3ms/step\n",
      "Epoch 16/300\n",
      "1250/1250 - 3s - loss: 166.6823 - mse: 166.6823 - val_loss: 91.7558 - val_mse: 91.7558 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
      "Epoch 17/300\n",
      "1250/1250 - 3s - loss: 111.4816 - mse: 111.4816 - val_loss: 57.5823 - val_mse: 57.5823 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
      "Epoch 18/300\n",
      "1250/1250 - 3s - loss: 514.5021 - mse: 514.5021 - val_loss: 6950.2617 - val_mse: 6950.2617 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
      "Epoch 19/300\n",
      "1250/1250 - 3s - loss: 661.2816 - mse: 661.2816 - val_loss: 44.6966 - val_mse: 44.6966 - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 20/300\n",
      "1250/1250 - 3s - loss: 700.7605 - mse: 700.7605 - val_loss: 32.4514 - val_mse: 32.4514 - lr: 0.0025 - 3s/epoch - 2ms/step\n",
      "Epoch 21/300\n",
      "1250/1250 - 3s - loss: 1150.7715 - mse: 1150.7715 - val_loss: 53.9629 - val_mse: 53.9629 - lr: 0.0025 - 3s/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "1250/1250 - 3s - loss: 348.5977 - mse: 348.5977 - val_loss: 367.3542 - val_mse: 367.3542 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
      "Epoch 23/300\n",
      "1250/1250 - 3s - loss: 1579.4180 - mse: 1579.4180 - val_loss: 91.5431 - val_mse: 91.5431 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
      "Epoch 24/300\n",
      "1250/1250 - 3s - loss: 235.7442 - mse: 235.7442 - val_loss: 64.3589 - val_mse: 64.3589 - lr: 0.0025 - 3s/epoch - 3ms/step\n",
      "Epoch 25/300\n"
     ]
    }
   ],
   "source": [
    "# Treinamento\n",
    "fit_params = {\n",
    "    'model__batch_size': batch_size,\n",
    "    'model__epochs': epochs,\n",
    "    'model__verbose': verbose,\n",
    "    'model__validation_data': (x_teste, y_teste),\n",
    "    'model__shuffle': True,\n",
    "    'model__validation_steps': None,\n",
    "    'model__validation_freq': 1,\n",
    "}\n",
    "\n",
    "grid_result = grid.fit(x_treino, y_treino, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do melhor modelo por mais épocas\n",
    "\n",
    "fit_params = {\n",
    "    'model__batch_size': batch_size,\n",
    "    'model__epochs': epochs,\n",
    "    'model__verbose': verbose,\n",
    "    'model__validation_data': (x_teste, y_teste),\n",
    "    'model__shuffle': True,\n",
    "    'model__validation_steps': None,\n",
    "    'model__validation_freq': 1,\n",
    "}\n",
    "\n",
    "# add -> abaro\n",
    "best_params\n",
    "\n",
    "\n",
    "grid_result = estimator.fit(x_treino, y_treino, **fit_params)\n",
    "\n",
    "# OU: ...\n",
    "#grid_result = best_model.fit(x_treino, y_treino, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(np.array([[16, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o Conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise n - XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento de Dados Para Construção de Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Construção, Treinamento e Avaliação do Modelo 1 com Regressão Linear (Benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resíduos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Construção, Treinamento e Avaliação do Modelo n com XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
