{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Challenge @ ITA 2022</font>\n",
    "# <font color='blue'>Equipe DIOMGIS</font>\n",
    "\n",
    "## <font color='blue'>1º Fase</font>\n",
    "\n",
    "### <font color='blue'>Predição de pregões futuros de ativos que compõem o índice SP500.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data\\image\\logo.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_datareader in /home/lucas/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/lucas/anaconda3/lib/python3.9/site-packages (from pandas_datareader) (1.4.3)\n",
      "Requirement already satisfied: lxml in /home/lucas/anaconda3/lib/python3.9/site-packages (from pandas_datareader) (4.9.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from pandas_datareader) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/lucas/anaconda3/lib/python3.9/site-packages (from pandas>=0.23->pandas_datareader) (1.21.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->pandas_datareader) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/lucas/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_datareader) (1.16.0)\n",
      "Requirement already satisfied: tensorflow in /home/lucas/anaconda3/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: setuptools in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (63.4.1)\n",
      "Requirement already satisfied: packaging in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/lucas/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lucas/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/lucas/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/lucas/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas e Frameworks\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorboard import notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Equipe DIOMGIS\n",
      "\n",
      "pandas_datareader: 0.10.0\n",
      "keras            : 2.10.0\n",
      "seaborn          : 0.11.2\n",
      "matplotlib       : 3.5.2\n",
      "tensorboard      : 2.10.1\n",
      "pandas           : 1.4.3\n",
      "numpy            : 1.21.5\n",
      "tensorflow       : 2.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Equipe DIOMGIS\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "plt.rcParams['figure.figsize'] = (15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Confirma se o TensorFlow pode acessar a GPU\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 18 20:30:53 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.94       Driver Version: 516.94       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:65:00.0  On |                  N/A |\n",
      "|  0%   50C    P2    29W / 220W |    806MiB /  8192MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       732    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      2592    C+G   ...txyewy\\MiniSearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3516    C+G   ...o Webcam\\GoPro Webcam.exe    N/A      |\n",
      "|    0   N/A  N/A      7452    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     12876    C+G   ...bat\\acrocef_2\\AcroCEF.exe    N/A      |\n",
      "|    0   N/A  N/A     13256    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13784    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14424    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14448    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15244    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     16788    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     17300    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     20344    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20628    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20992    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     21560    C+G   ...persky VPN 5.7\\ksdeui.exe    N/A      |\n",
      "|    0   N/A  N/A     22980    C+G   ...x64__pc75e8sa7ep4e\\XD.exe    N/A      |\n",
      "|    0   N/A  N/A     30040    C+G   ...370.42\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     31324      C   ...ucas\\anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     42176    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     44224    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     45908    C+G   ...370.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     47076    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Estado da GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros fixos de treinamento\n",
    "\n",
    "verbose = 2\n",
    "seed = 25\n",
    "steps = 30\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "logRetPeriod = 20\n",
    "graphic = True\n",
    "downloadData = True\n",
    "trainModel = True\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = ['2022-10-24', '2022-10-25', '2022-10-26', '2022-10-27', '2022-10-28', \n",
    "            '2022-10-31', '2022-11-01', '2022-11-02', '2022-11-03', '2022-11-04', \n",
    "            '2022-11-07', '2022-11-08', '2022-11-09', '2022-11-10', '2022-11-11',\n",
    "            '2022-11-14', '2022-11-15', '2022-11-16', '2022-11-17', '2022-11-18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ativos = ['A', 'AAL', 'AAP', 'AAPL', 'ABBV', 'ABC', 'ABMD', 'ABT',\n",
    "          'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES',\n",
    "          'AFL', 'AIG', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL',\n",
    "          'ALLE', 'AMAT', 'AMCR', 'AMD', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN',\n",
    "          'ANET', 'ANSS', 'AON', 'AOS', 'APA', 'APD', 'APH', 'APTV', 'ARE',\n",
    "          'ATO', 'ATVI', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AZO', 'BA',\n",
    "          'BAC', 'BALL', 'BAX', 'BBWI', 'BBY', 'BDX', 'BEN', 'BF.B', 'BIIB',\n",
    "          'BIO', 'BK', 'BKNG', 'BKR', 'BLK', 'BMY', 'BR', 'BRK.B', 'BRO',\n",
    "          'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CARR', 'CAT', 'CB',\n",
    "          'CBOE', 'CBRE', 'CCI', 'CCL', 'CDAY', 'CDNS', 'CDW', 'CE', 'CEG',\n",
    "          'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX',\n",
    "          'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF',\n",
    "          'COO', 'COP', 'COST', 'CPB', 'CPRT', 'CPT', 'CRL', 'CRM', 'CSCO',\n",
    "          'CSGP', 'CSX', 'CTAS', 'CTLT', 'CTRA', 'CTSH', 'CTVA', 'CVS',\n",
    "          'CVX', 'CZR', 'D', 'DAL', 'DD', 'DE', 'DFS', 'DG', 'DGX', 'DHI',\n",
    "          'DHR', 'DIS', 'DISH', 'DLR', 'DLTR', 'DOV', 'DOW', 'DPZ', 'DRI',\n",
    "          'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'DXCM', 'EA', 'EBAY', 'ECL',\n",
    "          'ED', 'EFX', 'EIX', 'EL', 'ELV', 'EMN', 'EMR', 'ENPH', 'EOG',\n",
    "          'EPAM', 'EQIX', 'EQR', 'EQT', 'ES', 'ESS', 'ETN', 'ETR', 'ETSY',\n",
    "          'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG', 'FAST',\n",
    "          'FBHS', 'FCX', 'FDS', 'FDX', 'FE', 'FFIV', 'FIS', 'FISV', 'FITB',\n",
    "          'FLT', 'FMC', 'FOX', 'FOXA', 'FRC', 'FRT', 'FTNT', 'FTV', 'GD',\n",
    "          'GE', 'GILD', 'GIS', 'GL', 'GLW', 'GM', 'GNRC', 'GOOG', 'GOOGL',\n",
    "          'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HCA',\n",
    "          'HD', 'HES', 'HIG', 'HII', 'HLT', 'HOLX', 'HON', 'HPE', 'HPQ',\n",
    "          'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'HWM', 'IBM', 'ICE', 'IDXX',\n",
    "          'IEX', 'IFF', 'ILMN', 'INCY', 'INTC', 'INTU', 'INVH', 'IP', 'IPG',\n",
    "          'IQV', 'IR', 'IRM', 'ISRG', 'IT', 'ITW', 'IVZ', 'J', 'JBHT', 'JCI',\n",
    "          'JKHY', 'JNJ', 'JNPR', 'JPM', 'K', 'KDP', 'KEY', 'KEYS', 'KHC',\n",
    "          'KIM', 'KLAC', 'KMB', 'KMI', 'KMX', 'KO', 'KR', 'L', 'LDOS', 'LEN',\n",
    "          'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW',\n",
    "          'LRCX', 'LUMN', 'LUV', 'LVS', 'LW', 'LYB', 'LYV', 'MA', 'MAA',\n",
    "          'MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET',\n",
    "          'META', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST',\n",
    "          'MO', 'MOH', 'MOS', 'MPC', 'MPWR', 'MRK', 'MRNA', 'MRO', 'MS',\n",
    "          'MSCI', 'MSFT', 'MSI', 'MTB', 'MTCH', 'MTD', 'MU', 'NCLH', 'NDAQ',\n",
    "          'NDSN', 'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NLOK', 'NLSN', 'NOC',\n",
    "          'NOW', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWL',\n",
    "          'NWS', 'NWSA', 'NXPI', 'O', 'ODFL', 'OGN', 'OKE', 'OMC', 'ON',\n",
    "          'ORCL', 'ORLY', 'OTIS', 'OXY', 'PARA', 'PAYC', 'PAYX', 'PCAR',\n",
    "          'PCG', 'PEAK', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH',\n",
    "          'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'POOL',\n",
    "          'PPG', 'PPL', 'PRU', 'PSA', 'PSX', 'PTC', 'PWR', 'PXD', 'PYPL',\n",
    "          'QCOM', 'QRVO', 'RCL', 'RE', 'REG', 'REGN', 'RF', 'RHI', 'RJF',\n",
    "          'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX', 'SBAC',\n",
    "          'SBNY', 'SBUX', 'SCHW', 'SEDG', 'SEE', 'SHW', 'SIVB', 'SJM', 'SLB',\n",
    "          'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE', 'STE', 'STT', 'STX',\n",
    "          'STZ', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYY', 'T', 'TAP', 'TDG',\n",
    "          'TDY', 'TECH', 'TEL', 'TER', 'TFC', 'TFX', 'TGT', 'TJX', 'TMO',\n",
    "          'TMUS', 'TPR', 'TRMB', 'TROW', 'TRV', 'TSCO', 'TSLA', 'TSN', 'TT',\n",
    "          'TTWO', 'TWTR', 'TXN', 'TXT', 'TYL', 'UAL', 'UDR', 'UHS', 'ULTA',\n",
    "          'UNH', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VFC', 'VICI', 'VLO',\n",
    "          'VMC', 'VNO', 'VRSK', 'VRSN', 'VRTX', 'VTR', 'VTRS', 'VZ', 'WAB',\n",
    "          'WAT', 'WBA', 'WBD', 'WDC', 'WEC', 'WELL', 'WFC', 'WHR', 'WM',\n",
    "          'WMB', 'WMT', 'WRB', 'WRK', 'WST', 'WTW', 'WY', 'WYNN', 'XEL',\n",
    "          'XOM', 'XRAY', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if downloadData:\n",
    "\n",
    "    start_date = \"2017-10-21\"\n",
    "    end_date = \"2022-10-21\"\n",
    "\n",
    "    data = web.DataReader(name = '^GSPC', data_source = 'yahoo', start = start_date, end = end_date)\n",
    "    SP500_index = pd.DataFrame(data['Close']).reset_index().rename(columns={'Close': 'SP500', 'Date': 'Dia'})\n",
    "\n",
    "    SP500_close = pd.DataFrame()\n",
    "\n",
    "    for ativo in ativos:\n",
    "  \n",
    "        if ativo == 'BF.B':\n",
    "            ativo = 'BF-B'\n",
    "\n",
    "        if ativo == 'BRK.B':\n",
    "            ativo = 'BRK-B'\n",
    "\n",
    "        data = web.DataReader(name = ativo, data_source = 'yahoo', start = start_date, end = end_date)\n",
    "        temp_close = pd.DataFrame(data['Close'])\n",
    "        SP500_close = pd.concat([SP500_close, temp_close], axis = 1)\n",
    "\n",
    "        \n",
    "    SP500_close.columns = ativos\n",
    "    SP500_close.reset_index(inplace = True)\n",
    "    SP500_close.rename(columns={'Date': 'Dia'}, inplace = True)\n",
    "\n",
    "    assert SP500_close.isna().sum().mean() == 0,  \"Valores Faltantes\"\n",
    "    assert SP500_index.isna().sum().mean() == 0,  \"Valores Faltantes\"\n",
    "\n",
    "    SP500_close.to_csv(path_or_buf = 'data/SP500_close', index = False)\n",
    "    SP500_index.to_csv(path_or_buf = 'data/SP500_index', index = False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    SP500_close = pd.read_csv('data/SP500_close')\n",
    "    SP500_index = pd.read_csv('data/SP500_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "setores = {'Industrials': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'HealthCare': df.loc[df['GICS Sector'] == 'Health Care']['Symbol'].tolist(),\n",
    "           'InformationTechnology': df.loc[df['GICS Sector'] == 'Information Technology']['Symbol'].tolist(),\n",
    "           'CommunicationServices': df.loc[df['GICS Sector'] == 'Communication Services']['Symbol'].tolist(),\n",
    "           'ConsumerStaples': df.loc[df['GICS Sector'] == 'Consumer Staples']['Symbol'].tolist(),\n",
    "           'ConsumerDiscretionary': df.loc[df['GICS Sector'] == 'Consumer Discretionary']['Symbol'].tolist(),\n",
    "           'Utilities': df.loc[df['GICS Sector'] == 'Utilities']['Symbol'].tolist(),\n",
    "           'Financials': df.loc[df['GICS Sector'] == 'Financials']['Symbol'].tolist(),\n",
    "           'Materials': df.loc[df['GICS Sector'] == 'Materials']['Symbol'].tolist(),\n",
    "           'RealEstate': df.loc[df['GICS Sector'] == 'Real Estate']['Symbol'].tolist(),\n",
    "           'Energy': df.loc[df['GICS Sector'] == 'Energy']['Symbol'].tolist()\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorTimeframeTable(table, ativo):\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "    \n",
    "    TimeframeTable = pd.DataFrame(np.zeros((len(table[ativo])-steps, steps+1), dtype='float64'), columns = nameColumns)\n",
    "\n",
    "    for index, close in enumerate(table[ativo]):\n",
    "        tempA = index\n",
    "        tempB = 0\n",
    "        for i in range(steps+1):\n",
    "            if tempA < len(table[ativo])-steps and tempA >=0:\n",
    "                TimeframeTable.iloc[tempA, tempB] = close\n",
    "\n",
    "            tempA -= 1\n",
    "            tempB += 1\n",
    "\n",
    "    timeIndex = table.iloc[steps:,0]\n",
    "    TimeframeTable[\"Dia\"] = timeIndex.to_numpy()\n",
    "    TimeframeTable.set_index(\"Dia\", inplace = True)\n",
    "    \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainScaler(df):\n",
    "    \n",
    "    trainScaler = pd.DataFrame()\n",
    " \n",
    "    for _ in range(steps+1):\n",
    "        temp_close = pd.DataFrame(df.iloc[:,-1])\n",
    "        trainScaler = pd.concat([trainScaler, temp_close], axis = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(trainScaler)\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingdata(steps, df, ativos):\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "    \n",
    "\n",
    "    aux = []\n",
    "    \n",
    "    for ativo in ativos:\n",
    "        trainDataAtivo = generatorTimeframeTable(df, ativo)\n",
    "        trainDataAtivo.dropna(axis = 0, inplace = True)\n",
    "        \n",
    "        #----Score-Z--------------------------------------\n",
    "        scaler = createTrainScaler(trainDataAtivo)\n",
    "        trainDataAtivo = scaler.transform(trainDataAtivo)\n",
    "        #-------------------------------------------------\n",
    "        aux.append(trainDataAtivo)\n",
    "    \n",
    "    trainData = np.concatenate(tuple(aux), axis=0)\n",
    "    \n",
    "    X = trainData[:, :-1]\n",
    "    y = trainData[:, -1]\n",
    "    \n",
    "\n",
    "    #------Divisão de dados entre Treino e Validação------------------------------------------------\n",
    "    \n",
    "    X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "\n",
    "    X_treino = X_treino.reshape((-1, steps, 1))\n",
    "    X_teste = X_teste.reshape((-1, steps, 1))\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return [X_treino, X_teste, y_treino, y_teste]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção, Treinamento e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime('%d-%B-%Ih%Mmin')))\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=20,\n",
    "                          verbose = verbose,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              mode=\"min\",\n",
    "                              verbose = verbose,\n",
    "                              min_delta=0.0001,\n",
    "                              min_lr=0)\n",
    "\n",
    "callbacks = [tensorboard, earlystop, reduce_lr, TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = True,\n",
    "                   input_shape = (steps, 1)))  \n",
    "\n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = True))  \n",
    "    \n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = False)) \n",
    "    \n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    Lmse = MeanSquaredError()\n",
    "    \n",
    "    opt_Adadelta = Adadelta(learning_rate = 0.01, rho = 0.95, epsilon = 1e-07)\n",
    "\n",
    "    model.compile(loss= Lmse, optimizer = opt_Adadelta)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTableFrame(ativo, tablePrevision, model, table = SP500_close):\n",
    "\n",
    "    TimeframeTable = generatorTimeframeTable(table, ativo)\n",
    "    \n",
    "    index_data = TimeframeTable.index\n",
    "    \n",
    "    scaler = createTrainScaler(TimeframeTable)\n",
    "\n",
    "    TimeframeTable = scaler.transform(TimeframeTable)\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "\n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    \n",
    "    for day in forecast:\n",
    "        \n",
    "        current_info = TimeframeTable.iloc[-1, 1:].to_numpy()\n",
    "        \n",
    "        standardCurrentInfo = current_info.reshape(1, steps, 1).astype('float32')\n",
    "        \n",
    "        current_forecast = model.predict(standardCurrentInfo, verbose=False).reshape(1,)\n",
    "        \n",
    "        new_line = np.concatenate((current_info, current_forecast), axis = 0)\n",
    "        \n",
    "        TimeframeTable = pd.concat([TimeframeTable,\n",
    "                                    pd.DataFrame(new_line.reshape(1, -1),\n",
    "                                                 columns = nameColumns,\n",
    "                                                 index = [day])], axis = 0)\n",
    "        \n",
    "        \n",
    "    index_data = TimeframeTable.index  \n",
    "    \n",
    "    TimeframeTable = scaler.inverse_transform(TimeframeTable)\n",
    "    \n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    TimeframeTable.index = pd.to_datetime(TimeframeTable.index)\n",
    "    \n",
    "    \n",
    "    #--------Popula tabela de previsão---------------------------------------------------\n",
    "    if ativo in ativos:\n",
    "            for day in forecast:\n",
    "                tablePrevision.loc[day, ativo] = TimeframeTable.loc[day, 'Close-0']\n",
    "    #------------------------------------------------------------------------------------\n",
    "   \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popula tabela de log-Retorno e gera gráficos\n",
    "\n",
    "def fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model):\n",
    "    \n",
    "    lengthTable = len(tableLogRet)\n",
    "    \n",
    "    time = datetime.now().strftime('%d-%B-%Ih%Mmin')\n",
    "\n",
    "    for ativo in setor:\n",
    "\n",
    "        TimeframeSPAux = fillTableFrame(ativo, tablePrevision, model)\n",
    "\n",
    "        #-----------Graphic------------------------------------------------------------------------------------------\n",
    "        if graphic:\n",
    "            \n",
    "            outdir = './graphics/{}-{}'.format(nameSetor, time)\n",
    "\n",
    "            if not os.path.exists(outdir):\n",
    "                os.mkdir(outdir)\n",
    "            \n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(TimeframeSPAux.index[:-len(forecast)], TimeframeSPAux.iloc[:-len(forecast), -1], linewidth=2.0, c = 'b')\n",
    "            ax.plot(TimeframeSPAux.index[-len(forecast):], TimeframeSPAux.iloc[-len(forecast):, -1], linewidth=2.0, c = 'r', ls = '-')\n",
    "            ax.legend(['Atual', 'Previsão'])\n",
    "            ax.set_title('Preço de Fechamento - {}'.format(ativo))\n",
    "            ax.set(xlabel='Tempo (ano)', ylabel='Preço ($)')\n",
    "            nameGraphic = '{}.jpg'.format(ativo)\n",
    "            fullname = os.path.join(outdir, nameGraphic)\n",
    "            plt.savefig(fullname)\n",
    "            plt.close(fig)\n",
    "        #------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        #--------Popula tabela de Log Retorno------------------------------------------------------------------------\n",
    "        for n in range(len(forecast)):\n",
    "            tableLogRet.loc[lengthTable-n-1, ativo] = \\\n",
    "            np.log(TimeframeSPAux.iloc[lengthTable-steps-n-1, -1] / TimeframeSPAux.iloc[lengthTable-steps-n-1-logRetPeriod, -1])\n",
    "        #------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela de Previsão\n",
    "\n",
    "update = pd.DataFrame(index = pd.to_datetime(forecast), columns = ativos) \\\n",
    "    .reset_index().rename(columns={'index': 'Dia'})\n",
    "\n",
    "tablePrevision = pd.concat([SP500_close, update], axis = 0, ignore_index = True).set_index('Dia')\n",
    "\n",
    "tablePrevision.index = pd.to_datetime(tablePrevision.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela de Log-Retorno vazia \n",
    "\n",
    "index_data = pd.to_datetime(SP500_close['Dia'].append(pd.Series(forecast)))\n",
    "\n",
    "tableLogRet = pd.DataFrame(index = index_data,\n",
    "                           columns = ativos).reset_index().rename(columns={'index': 'Dia'})\n",
    "\n",
    "tableLogRet['Dia'] = tableLogRet['Dia'].apply(lambda date: date.strftime('%d/%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "2143/2143 - 46s - loss: 0.0881 - val_loss: 0.0458 - lr: 0.0100 - 46s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 36s - loss: 0.0434 - val_loss: 0.0376 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 36s - loss: 0.0375 - val_loss: 0.0327 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 36s - loss: 0.0332 - val_loss: 0.0292 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 36s - loss: 0.0301 - val_loss: 0.0267 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 36s - loss: 0.0280 - val_loss: 0.0251 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 36s - loss: 0.0265 - val_loss: 0.0239 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 36s - loss: 0.0253 - val_loss: 0.0228 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 38s - loss: 0.0243 - val_loss: 0.0219 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 38s - loss: 0.0233 - val_loss: 0.0211 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 36s - loss: 0.0225 - val_loss: 0.0203 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 36s - loss: 0.0218 - val_loss: 0.0198 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 36s - loss: 0.0211 - val_loss: 0.0192 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 36s - loss: 0.0205 - val_loss: 0.0185 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 36s - loss: 0.0199 - val_loss: 0.0180 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 37s - loss: 0.0194 - val_loss: 0.0176 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 36s - loss: 0.0189 - val_loss: 0.0174 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 37s - loss: 0.0184 - val_loss: 0.0168 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 36s - loss: 0.0180 - val_loss: 0.0164 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 37s - loss: 0.0176 - val_loss: 0.0160 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 36s - loss: 0.0172 - val_loss: 0.0157 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 36s - loss: 0.0169 - val_loss: 0.0154 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 37s - loss: 0.0165 - val_loss: 0.0151 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0162 - val_loss: 0.0151 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 36s - loss: 0.0160 - val_loss: 0.0146 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 36s - loss: 0.0157 - val_loss: 0.0144 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 36s - loss: 0.0154 - val_loss: 0.0141 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 36s - loss: 0.0152 - val_loss: 0.0139 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 37s - loss: 0.0149 - val_loss: 0.0137 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0147 - val_loss: 0.0135 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 36s - loss: 0.0145 - val_loss: 0.0133 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 36s - loss: 0.0143 - val_loss: 0.0131 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 36s - loss: 0.0141 - val_loss: 0.0130 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 36s - loss: 0.0139 - val_loss: 0.0127 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 36s - loss: 0.0137 - val_loss: 0.0126 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 36s - loss: 0.0136 - val_loss: 0.0124 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 36s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 36s - loss: 0.0132 - val_loss: 0.0122 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 36s - loss: 0.0131 - val_loss: 0.0119 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 38s - loss: 0.0129 - val_loss: 0.0118 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 38s - loss: 0.0128 - val_loss: 0.0116 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0126 - val_loss: 0.0116 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 36s - loss: 0.0125 - val_loss: 0.0115 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 36s - loss: 0.0123 - val_loss: 0.0113 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 36s - loss: 0.0122 - val_loss: 0.0111 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 37s - loss: 0.0121 - val_loss: 0.0110 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 36s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 36s - loss: 0.0118 - val_loss: 0.0108 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 36s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 36s - loss: 0.0116 - val_loss: 0.0105 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 36s - loss: 0.0115 - val_loss: 0.0105 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 36s - loss: 0.0114 - val_loss: 0.0103 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 36s - loss: 0.0113 - val_loss: 0.0102 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 36s - loss: 0.0111 - val_loss: 0.0102 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 36s - loss: 0.0111 - val_loss: 0.0101 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 36s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 38s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 36s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 35s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 36s - loss: 0.0106 - val_loss: 0.0096 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 36s - loss: 0.0105 - val_loss: 0.0097 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 36s - loss: 0.0104 - val_loss: 0.0096 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 36s - loss: 0.0103 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 36s - loss: 0.0103 - val_loss: 0.0093 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 36s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 36s - loss: 0.0101 - val_loss: 0.0093 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 36s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 38s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 35s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 80/2000\n",
      "2143/2143 - 36s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 36s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 36s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 35s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 35s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 35s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 35s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 38s/epoch - 18ms/step\n",
      "Epoch 92/2000\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 38s/epoch - 18ms/step\n",
      "Epoch 98/2000\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 38s/epoch - 18ms/step\n",
      "Epoch 100/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 38s/epoch - 18ms/step\n",
      "Epoch 115/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 38s/epoch - 18ms/step\n",
      "Epoch 120/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.1920e-12 - 37s/epoch - 17ms/step\n",
      "Epoch 127/2000\n",
      "Restoring model weights from the end of the best epoch: 107.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.1920e-12 - 37s/epoch - 17ms/step\n",
      "Epoch 127: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Industrials/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Industrials/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0090\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0081\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.00898\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.00814\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "1929/1929 - 42s - loss: 0.1068 - val_loss: 0.0487 - lr: 0.0100 - 42s/epoch - 22ms/step\n",
      "Epoch 2/2000\n",
      "1929/1929 - 34s - loss: 0.0573 - val_loss: 0.0402 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 3/2000\n",
      "1929/1929 - 34s - loss: 0.0501 - val_loss: 0.0356 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 4/2000\n",
      "1929/1929 - 34s - loss: 0.0450 - val_loss: 0.0320 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 5/2000\n",
      "1929/1929 - 34s - loss: 0.0410 - val_loss: 0.0292 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 6/2000\n",
      "1929/1929 - 35s - loss: 0.0381 - val_loss: 0.0273 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 7/2000\n",
      "1929/1929 - 34s - loss: 0.0360 - val_loss: 0.0257 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 8/2000\n",
      "1929/1929 - 35s - loss: 0.0343 - val_loss: 0.0244 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 9/2000\n",
      "1929/1929 - 34s - loss: 0.0328 - val_loss: 0.0234 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 10/2000\n",
      "1929/1929 - 33s - loss: 0.0315 - val_loss: 0.0225 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "1929/1929 - 33s - loss: 0.0303 - val_loss: 0.0217 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "1929/1929 - 33s - loss: 0.0292 - val_loss: 0.0208 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "1929/1929 - 33s - loss: 0.0282 - val_loss: 0.0201 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "1929/1929 - 33s - loss: 0.0273 - val_loss: 0.0194 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "1929/1929 - 33s - loss: 0.0265 - val_loss: 0.0188 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "1929/1929 - 33s - loss: 0.0257 - val_loss: 0.0183 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "1929/1929 - 33s - loss: 0.0249 - val_loss: 0.0179 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "1929/1929 - 33s - loss: 0.0242 - val_loss: 0.0173 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "1929/1929 - 33s - loss: 0.0236 - val_loss: 0.0169 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "1929/1929 - 33s - loss: 0.0230 - val_loss: 0.0164 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "1929/1929 - 33s - loss: 0.0225 - val_loss: 0.0160 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "1929/1929 - 33s - loss: 0.0219 - val_loss: 0.0156 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "1929/1929 - 33s - loss: 0.0215 - val_loss: 0.0153 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "1929/1929 - 33s - loss: 0.0210 - val_loss: 0.0150 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "1929/1929 - 33s - loss: 0.0206 - val_loss: 0.0147 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "1929/1929 - 33s - loss: 0.0202 - val_loss: 0.0145 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "1929/1929 - 33s - loss: 0.0198 - val_loss: 0.0141 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "1929/1929 - 33s - loss: 0.0194 - val_loss: 0.0138 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "1929/1929 - 33s - loss: 0.0191 - val_loss: 0.0136 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "1929/1929 - 33s - loss: 0.0188 - val_loss: 0.0134 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "1929/1929 - 33s - loss: 0.0185 - val_loss: 0.0132 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "1929/1929 - 33s - loss: 0.0182 - val_loss: 0.0130 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "1929/1929 - 33s - loss: 0.0179 - val_loss: 0.0128 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "1929/1929 - 33s - loss: 0.0176 - val_loss: 0.0126 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "1929/1929 - 34s - loss: 0.0174 - val_loss: 0.0124 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "1929/1929 - 33s - loss: 0.0171 - val_loss: 0.0122 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "1929/1929 - 36s - loss: 0.0169 - val_loss: 0.0121 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 38/2000\n",
      "1929/1929 - 36s - loss: 0.0167 - val_loss: 0.0119 - lr: 0.0100 - 36s/epoch - 19ms/step\n",
      "Epoch 39/2000\n",
      "1929/1929 - 35s - loss: 0.0165 - val_loss: 0.0117 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 40/2000\n",
      "1929/1929 - 34s - loss: 0.0162 - val_loss: 0.0116 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 41/2000\n",
      "1929/1929 - 35s - loss: 0.0160 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 42/2000\n",
      "1929/1929 - 34s - loss: 0.0159 - val_loss: 0.0113 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 43/2000\n",
      "1929/1929 - 35s - loss: 0.0157 - val_loss: 0.0113 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 44/2000\n",
      "1929/1929 - 34s - loss: 0.0155 - val_loss: 0.0111 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 45/2000\n",
      "1929/1929 - 38s - loss: 0.0153 - val_loss: 0.0110 - lr: 0.0100 - 38s/epoch - 20ms/step\n",
      "Epoch 46/2000\n",
      "1929/1929 - 34s - loss: 0.0152 - val_loss: 0.0109 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "1929/1929 - 35s - loss: 0.0150 - val_loss: 0.0107 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 48/2000\n",
      "1929/1929 - 35s - loss: 0.0149 - val_loss: 0.0107 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 49/2000\n",
      "1929/1929 - 38s - loss: 0.0147 - val_loss: 0.0106 - lr: 0.0100 - 38s/epoch - 20ms/step\n",
      "Epoch 50/2000\n",
      "1929/1929 - 34s - loss: 0.0146 - val_loss: 0.0105 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 51/2000\n",
      "1929/1929 - 36s - loss: 0.0145 - val_loss: 0.0104 - lr: 0.0100 - 36s/epoch - 19ms/step\n",
      "Epoch 52/2000\n",
      "1929/1929 - 37s - loss: 0.0144 - val_loss: 0.0103 - lr: 0.0100 - 37s/epoch - 19ms/step\n",
      "Epoch 53/2000\n",
      "1929/1929 - 37s - loss: 0.0142 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 19ms/step\n",
      "Epoch 54/2000\n",
      "1929/1929 - 34s - loss: 0.0141 - val_loss: 0.0101 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 55/2000\n",
      "1929/1929 - 35s - loss: 0.0140 - val_loss: 0.0100 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 56/2000\n",
      "1929/1929 - 36s - loss: 0.0139 - val_loss: 0.0100 - lr: 0.0100 - 36s/epoch - 19ms/step\n",
      "Epoch 57/2000\n",
      "1929/1929 - 36s - loss: 0.0138 - val_loss: 0.0099 - lr: 0.0100 - 36s/epoch - 19ms/step\n",
      "Epoch 58/2000\n",
      "1929/1929 - 37s - loss: 0.0137 - val_loss: 0.0098 - lr: 0.0100 - 37s/epoch - 19ms/step\n",
      "Epoch 59/2000\n",
      "1929/1929 - 34s - loss: 0.0136 - val_loss: 0.0098 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 60/2000\n",
      "1929/1929 - 34s - loss: 0.0135 - val_loss: 0.0098 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 61/2000\n",
      "1929/1929 - 33s - loss: 0.0134 - val_loss: 0.0097 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "1929/1929 - 33s - loss: 0.0134 - val_loss: 0.0096 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "1929/1929 - 33s - loss: 0.0133 - val_loss: 0.0096 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "1929/1929 - 40s - loss: 0.0132 - val_loss: 0.0096 - lr: 0.0100 - 40s/epoch - 21ms/step\n",
      "Epoch 65/2000\n",
      "1929/1929 - 34s - loss: 0.0132 - val_loss: 0.0095 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 66/2000\n",
      "1929/1929 - 33s - loss: 0.0131 - val_loss: 0.0095 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "1929/1929 - 33s - loss: 0.0130 - val_loss: 0.0094 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "1929/1929 - 33s - loss: 0.0130 - val_loss: 0.0093 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "1929/1929 - 33s - loss: 0.0129 - val_loss: 0.0093 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "1929/1929 - 33s - loss: 0.0129 - val_loss: 0.0093 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "1929/1929 - 33s - loss: 0.0128 - val_loss: 0.0093 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "1929/1929 - 33s - loss: 0.0128 - val_loss: 0.0092 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "1929/1929 - 33s - loss: 0.0127 - val_loss: 0.0092 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "1929/1929 - 34s - loss: 0.0127 - val_loss: 0.0092 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 75/2000\n",
      "1929/1929 - 35s - loss: 0.0126 - val_loss: 0.0091 - lr: 0.0100 - 35s/epoch - 18ms/step\n",
      "Epoch 76/2000\n",
      "1929/1929 - 34s - loss: 0.0126 - val_loss: 0.0091 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 77/2000\n",
      "1929/1929 - 34s - loss: 0.0125 - val_loss: 0.0091 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "1929/1929 - 34s - loss: 0.0125 - val_loss: 0.0091 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "1929/1929 - 35s - loss: 0.0125 - val_loss: 0.0090 - lr: 0.0100 - 35s/epoch - 18ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "1929/1929 - 33s - loss: 0.0124 - val_loss: 0.0090 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "1929/1929 - 34s - loss: 0.0124 - val_loss: 0.0091 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "1929/1929 - 34s - loss: 0.0124 - val_loss: 0.0090 - lr: 0.0100 - 34s/epoch - 18ms/step\n",
      "Epoch 83/2000\n",
      "1929/1929 - 34s - loss: 0.0123 - val_loss: 0.0090 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "1929/1929 - 34s - loss: 0.0123 - val_loss: 0.0089 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "1929/1929 - 33s - loss: 0.0123 - val_loss: 0.0089 - lr: 0.0020 - 33s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "1929/1929 - 33s - loss: 0.0123 - val_loss: 0.0089 - lr: 0.0020 - 33s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "1929/1929 - 35s - loss: 0.0123 - val_loss: 0.0089 - lr: 0.0020 - 35s/epoch - 18ms/step\n",
      "Epoch 88/2000\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "1929/1929 - 33s - loss: 0.0123 - val_loss: 0.0089 - lr: 0.0020 - 33s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 4.0000e-04 - 33s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 4.0000e-04 - 34s/epoch - 18ms/step\n",
      "Epoch 91/2000\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 4.0000e-04 - 34s/epoch - 18ms/step\n",
      "Epoch 92/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 8.0000e-05 - 33s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 8.0000e-05 - 33s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 8.0000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.6000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.6000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.6000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "1929/1929 - 35s - loss: 0.0122 - val_loss: 0.0089 - lr: 3.2000e-06 - 35s/epoch - 18ms/step\n",
      "Epoch 99/2000\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 3.2000e-06 - 34s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 3.2000e-06 - 33s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 6.4000e-07 - 33s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 6.4000e-07 - 33s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 6.4000e-07 - 34s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.2800e-07 - 33s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.2800e-07 - 33s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.2800e-07 - 33s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "1929/1929 - 35s - loss: 0.0122 - val_loss: 0.0089 - lr: 2.5600e-08 - 35s/epoch - 18ms/step\n",
      "Epoch 108/2000\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 2.5600e-08 - 34s/epoch - 18ms/step\n",
      "Epoch 109/2000\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 2.5600e-08 - 33s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "1929/1929 - 32s - loss: 0.0122 - val_loss: 0.0089 - lr: 5.1200e-09 - 32s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "1929/1929 - 32s - loss: 0.0122 - val_loss: 0.0089 - lr: 5.1200e-09 - 32s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 5.1200e-09 - 33s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.0240e-09 - 33s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.0240e-09 - 33s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 1.0240e-09 - 33s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 2.0480e-10 - 33s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 2.0480e-10 - 33s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 2.0480e-10 - 34s/epoch - 18ms/step\n",
      "Epoch 119/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 4.0960e-11 - 33s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "1929/1929 - 33s - loss: 0.0122 - val_loss: 0.0089 - lr: 4.0960e-11 - 33s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 4.0960e-11 - 34s/epoch - 18ms/step\n",
      "Epoch 122/2000\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 8.1920e-12 - 34s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "Restoring model weights from the end of the best epoch: 103.\n",
      "1929/1929 - 34s - loss: 0.0122 - val_loss: 0.0089 - lr: 8.1920e-12 - 34s/epoch - 17ms/step\n",
      "Epoch 123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/HealthCare/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/HealthCare/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929/1929 [==============================] - 13s 7ms/step - loss: 0.0122\n",
      "483/483 [==============================] - 3s 7ms/step - loss: 0.0089\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01224\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.00889\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2292/2292 - 48s - loss: 0.0856 - val_loss: 0.0343 - lr: 0.0100 - 48s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2292/2292 - 41s - loss: 0.0432 - val_loss: 0.0279 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 3/2000\n",
      "2292/2292 - 41s - loss: 0.0372 - val_loss: 0.0244 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 4/2000\n",
      "2292/2292 - 42s - loss: 0.0330 - val_loss: 0.0219 - lr: 0.0100 - 42s/epoch - 18ms/step\n",
      "Epoch 5/2000\n",
      "2292/2292 - 41s - loss: 0.0300 - val_loss: 0.0202 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 6/2000\n",
      "2292/2292 - 39s - loss: 0.0280 - val_loss: 0.0192 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2292/2292 - 41s - loss: 0.0265 - val_loss: 0.0180 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 8/2000\n",
      "2292/2292 - 42s - loss: 0.0253 - val_loss: 0.0172 - lr: 0.0100 - 42s/epoch - 18ms/step\n",
      "Epoch 9/2000\n",
      "2292/2292 - 40s - loss: 0.0243 - val_loss: 0.0166 - lr: 0.0100 - 40s/epoch - 18ms/step\n",
      "Epoch 10/2000\n",
      "2292/2292 - 42s - loss: 0.0234 - val_loss: 0.0160 - lr: 0.0100 - 42s/epoch - 18ms/step\n",
      "Epoch 11/2000\n",
      "2292/2292 - 40s - loss: 0.0226 - val_loss: 0.0154 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2292/2292 - 44s - loss: 0.0218 - val_loss: 0.0150 - lr: 0.0100 - 44s/epoch - 19ms/step\n",
      "Epoch 13/2000\n",
      "2292/2292 - 42s - loss: 0.0212 - val_loss: 0.0144 - lr: 0.0100 - 42s/epoch - 18ms/step\n",
      "Epoch 14/2000\n",
      "2292/2292 - 41s - loss: 0.0205 - val_loss: 0.0140 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 15/2000\n",
      "2292/2292 - 42s - loss: 0.0200 - val_loss: 0.0136 - lr: 0.0100 - 42s/epoch - 18ms/step\n",
      "Epoch 16/2000\n",
      "2292/2292 - 38s - loss: 0.0195 - val_loss: 0.0132 - lr: 0.0100 - 38s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2292/2292 - 38s - loss: 0.0190 - val_loss: 0.0129 - lr: 0.0100 - 38s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2292/2292 - 39s - loss: 0.0185 - val_loss: 0.0127 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2292/2292 - 41s - loss: 0.0181 - val_loss: 0.0124 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 20/2000\n",
      "2292/2292 - 38s - loss: 0.0177 - val_loss: 0.0120 - lr: 0.0100 - 38s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2292/2292 - 38s - loss: 0.0173 - val_loss: 0.0118 - lr: 0.0100 - 38s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2292/2292 - 42s - loss: 0.0170 - val_loss: 0.0116 - lr: 0.0100 - 42s/epoch - 18ms/step\n",
      "Epoch 23/2000\n",
      "2292/2292 - 40s - loss: 0.0167 - val_loss: 0.0115 - lr: 0.0100 - 40s/epoch - 18ms/step\n",
      "Epoch 24/2000\n",
      "2292/2292 - 41s - loss: 0.0163 - val_loss: 0.0112 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 25/2000\n",
      "2292/2292 - 39s - loss: 0.0161 - val_loss: 0.0110 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2292/2292 - 40s - loss: 0.0158 - val_loss: 0.0107 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2292/2292 - 39s - loss: 0.0155 - val_loss: 0.0106 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2292/2292 - 39s - loss: 0.0153 - val_loss: 0.0104 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2292/2292 - 39s - loss: 0.0150 - val_loss: 0.0103 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2292/2292 - 40s - loss: 0.0148 - val_loss: 0.0101 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2292/2292 - 40s - loss: 0.0146 - val_loss: 0.0100 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2292/2292 - 42s - loss: 0.0144 - val_loss: 0.0098 - lr: 0.0100 - 42s/epoch - 18ms/step\n",
      "Epoch 33/2000\n",
      "2292/2292 - 48s - loss: 0.0142 - val_loss: 0.0097 - lr: 0.0100 - 48s/epoch - 21ms/step\n",
      "Epoch 34/2000\n",
      "2292/2292 - 40s - loss: 0.0140 - val_loss: 0.0095 - lr: 0.0100 - 40s/epoch - 18ms/step\n",
      "Epoch 35/2000\n",
      "2292/2292 - 40s - loss: 0.0138 - val_loss: 0.0094 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2292/2292 - 39s - loss: 0.0136 - val_loss: 0.0093 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2292/2292 - 39s - loss: 0.0135 - val_loss: 0.0092 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2292/2292 - 39s - loss: 0.0133 - val_loss: 0.0091 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2292/2292 - 39s - loss: 0.0131 - val_loss: 0.0089 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2292/2292 - 39s - loss: 0.0130 - val_loss: 0.0088 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2292/2292 - 40s - loss: 0.0128 - val_loss: 0.0087 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2292/2292 - 39s - loss: 0.0127 - val_loss: 0.0087 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2292/2292 - 39s - loss: 0.0126 - val_loss: 0.0086 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2292/2292 - 39s - loss: 0.0124 - val_loss: 0.0085 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2292/2292 - 39s - loss: 0.0123 - val_loss: 0.0083 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2292/2292 - 39s - loss: 0.0122 - val_loss: 0.0083 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2292/2292 - 39s - loss: 0.0121 - val_loss: 0.0082 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2292/2292 - 41s - loss: 0.0119 - val_loss: 0.0081 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 49/2000\n",
      "2292/2292 - 39s - loss: 0.0118 - val_loss: 0.0080 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2292/2292 - 39s - loss: 0.0117 - val_loss: 0.0080 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2292/2292 - 39s - loss: 0.0116 - val_loss: 0.0079 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2292/2292 - 40s - loss: 0.0115 - val_loss: 0.0078 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2292/2292 - 40s - loss: 0.0114 - val_loss: 0.0077 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2292/2292 - 43s - loss: 0.0113 - val_loss: 0.0077 - lr: 0.0100 - 43s/epoch - 19ms/step\n",
      "Epoch 55/2000\n",
      "2292/2292 - 43s - loss: 0.0112 - val_loss: 0.0076 - lr: 0.0100 - 43s/epoch - 19ms/step\n",
      "Epoch 56/2000\n",
      "2292/2292 - 41s - loss: 0.0112 - val_loss: 0.0076 - lr: 0.0100 - 41s/epoch - 18ms/step\n",
      "Epoch 57/2000\n",
      "2292/2292 - 40s - loss: 0.0111 - val_loss: 0.0075 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2292/2292 - 39s - loss: 0.0110 - val_loss: 0.0074 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2292/2292 - 39s - loss: 0.0109 - val_loss: 0.0074 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2292/2292 - 40s - loss: 0.0108 - val_loss: 0.0073 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2292/2292 - 40s - loss: 0.0108 - val_loss: 0.0073 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2292/2292 - 40s - loss: 0.0107 - val_loss: 0.0072 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2292/2292 - 40s - loss: 0.0106 - val_loss: 0.0072 - lr: 0.0100 - 40s/epoch - 18ms/step\n",
      "Epoch 64/2000\n",
      "2292/2292 - 39s - loss: 0.0106 - val_loss: 0.0072 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2292/2292 - 39s - loss: 0.0105 - val_loss: 0.0071 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2292/2292 - 40s - loss: 0.0104 - val_loss: 0.0071 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2292/2292 - 39s - loss: 0.0104 - val_loss: 0.0071 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2292/2292 - 39s - loss: 0.0103 - val_loss: 0.0070 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2292/2292 - 40s - loss: 0.0103 - val_loss: 0.0071 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2292/2292 - 40s - loss: 0.0102 - val_loss: 0.0069 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2292/2292 - 39s - loss: 0.0102 - val_loss: 0.0069 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2292/2292 - 39s - loss: 0.0101 - val_loss: 0.0069 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2292/2292 - 39s - loss: 0.0101 - val_loss: 0.0069 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2292/2292 - 40s - loss: 0.0101 - val_loss: 0.0068 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2292/2292 - 39s - loss: 0.0100 - val_loss: 0.0068 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2292/2292 - 39s - loss: 0.0100 - val_loss: 0.0069 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2292/2292 - 40s - loss: 0.0099 - val_loss: 0.0067 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2292/2292 - 40s - loss: 0.0099 - val_loss: 0.0067 - lr: 0.0100 - 40s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2292/2292 - 40s - loss: 0.0099 - val_loss: 0.0067 - lr: 0.0100 - 40s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2292/2292 - 39s - loss: 0.0098 - val_loss: 0.0067 - lr: 0.0100 - 39s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2292/2292 - 40s - loss: 0.0098 - val_loss: 0.0066 - lr: 0.0100 - 40s/epoch - 18ms/step\n",
      "Epoch 82/2000\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2292/2292 - 40s - loss: 0.0098 - val_loss: 0.0067 - lr: 0.0100 - 40s/epoch - 18ms/step\n",
      "Epoch 83/2000\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 0.0020 - 41s/epoch - 18ms/step\n",
      "Epoch 84/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 0.0020 - 40s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 0.0020 - 40s/epoch - 18ms/step\n",
      "Epoch 86/2000\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 0.0020 - 40s/epoch - 18ms/step\n",
      "Epoch 87/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 4.0000e-04 - 40s/epoch - 18ms/step\n",
      "Epoch 88/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 4.0000e-04 - 40s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 4.0000e-04 - 40s/epoch - 18ms/step\n",
      "Epoch 90/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 8.0000e-05 - 40s/epoch - 18ms/step\n",
      "Epoch 91/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 8.0000e-05 - 40s/epoch - 18ms/step\n",
      "Epoch 92/2000\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 8.0000e-05 - 41s/epoch - 18ms/step\n",
      "Epoch 93/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.6000e-05 - 40s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2292/2292 - 42s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.6000e-05 - 42s/epoch - 18ms/step\n",
      "Epoch 95/2000\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2292/2292 - 42s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.6000e-05 - 42s/epoch - 19ms/step\n",
      "Epoch 96/2000\n",
      "2292/2292 - 42s - loss: 0.0097 - val_loss: 0.0066 - lr: 3.2000e-06 - 42s/epoch - 18ms/step\n",
      "Epoch 97/2000\n",
      "2292/2292 - 42s - loss: 0.0097 - val_loss: 0.0066 - lr: 3.2000e-06 - 42s/epoch - 18ms/step\n",
      "Epoch 98/2000\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 3.2000e-06 - 41s/epoch - 18ms/step\n",
      "Epoch 99/2000\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 6.4000e-07 - 41s/epoch - 18ms/step\n",
      "Epoch 100/2000\n",
      "2292/2292 - 44s - loss: 0.0097 - val_loss: 0.0066 - lr: 6.4000e-07 - 44s/epoch - 19ms/step\n",
      "Epoch 101/2000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 6.4000e-07 - 41s/epoch - 18ms/step\n",
      "Epoch 102/2000\n",
      "2292/2292 - 45s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.2800e-07 - 45s/epoch - 19ms/step\n",
      "Epoch 103/2000\n",
      "2292/2292 - 42s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.2800e-07 - 42s/epoch - 18ms/step\n",
      "Epoch 104/2000\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.2800e-07 - 40s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2292/2292 - 39s - loss: 0.0097 - val_loss: 0.0066 - lr: 2.5600e-08 - 39s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2292/2292 - 39s - loss: 0.0097 - val_loss: 0.0066 - lr: 2.5600e-08 - 39s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2292/2292 - 39s - loss: 0.0097 - val_loss: 0.0066 - lr: 2.5600e-08 - 39s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 5.1200e-09 - 41s/epoch - 18ms/step\n",
      "Epoch 109/2000\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 5.1200e-09 - 41s/epoch - 18ms/step\n",
      "Epoch 110/2000\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 5.1200e-09 - 40s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2292/2292 - 39s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.0240e-09 - 39s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2292/2292 - 39s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.0240e-09 - 39s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2292/2292 - 43s - loss: 0.0097 - val_loss: 0.0066 - lr: 1.0240e-09 - 43s/epoch - 19ms/step\n",
      "Epoch 114/2000\n",
      "2292/2292 - 41s - loss: 0.0097 - val_loss: 0.0066 - lr: 2.0480e-10 - 41s/epoch - 18ms/step\n",
      "Epoch 115/2000\n",
      "2292/2292 - 51s - loss: 0.0097 - val_loss: 0.0066 - lr: 2.0480e-10 - 51s/epoch - 22ms/step\n",
      "Epoch 116/2000\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 2.0480e-10 - 40s/epoch - 18ms/step\n",
      "Epoch 117/2000\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 4.0960e-11 - 40s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2292/2292 - 39s - loss: 0.0097 - val_loss: 0.0066 - lr: 4.0960e-11 - 39s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "2292/2292 - 40s - loss: 0.0097 - val_loss: 0.0066 - lr: 4.0960e-11 - 40s/epoch - 18ms/step\n",
      "Epoch 120/2000\n",
      "2292/2292 - 47s - loss: 0.0097 - val_loss: 0.0066 - lr: 8.1920e-12 - 47s/epoch - 20ms/step\n",
      "Epoch 121/2000\n",
      "Restoring model weights from the end of the best epoch: 101.\n",
      "2292/2292 - 43s - loss: 0.0097 - val_loss: 0.0066 - lr: 8.1920e-12 - 43s/epoch - 19ms/step\n",
      "Epoch 121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/InformationTechnology/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/InformationTechnology/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2292/2292 [==============================] - 20s 9ms/step - loss: 0.0097\n",
      "573/573 [==============================] - 5s 8ms/step - loss: 0.0066\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.00970\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.00658\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "779/779 - 24s - loss: 0.1539 - val_loss: 0.1163 - lr: 0.0100 - 24s/epoch - 31ms/step\n",
      "Epoch 2/2000\n",
      "779/779 - 14s - loss: 0.0717 - val_loss: 0.0908 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 3/2000\n",
      "779/779 - 14s - loss: 0.0572 - val_loss: 0.0786 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 4/2000\n",
      "779/779 - 15s - loss: 0.0516 - val_loss: 0.0730 - lr: 0.0100 - 15s/epoch - 20ms/step\n",
      "Epoch 5/2000\n",
      "779/779 - 15s - loss: 0.0487 - val_loss: 0.0692 - lr: 0.0100 - 15s/epoch - 20ms/step\n",
      "Epoch 6/2000\n",
      "779/779 - 15s - loss: 0.0463 - val_loss: 0.0657 - lr: 0.0100 - 15s/epoch - 19ms/step\n",
      "Epoch 7/2000\n",
      "779/779 - 14s - loss: 0.0442 - val_loss: 0.0629 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 8/2000\n",
      "779/779 - 14s - loss: 0.0422 - val_loss: 0.0598 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 9/2000\n",
      "779/779 - 14s - loss: 0.0406 - val_loss: 0.0573 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 10/2000\n",
      "779/779 - 13s - loss: 0.0390 - val_loss: 0.0551 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "779/779 - 14s - loss: 0.0377 - val_loss: 0.0532 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 12/2000\n",
      "779/779 - 15s - loss: 0.0364 - val_loss: 0.0519 - lr: 0.0100 - 15s/epoch - 19ms/step\n",
      "Epoch 13/2000\n",
      "779/779 - 14s - loss: 0.0354 - val_loss: 0.0501 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 14/2000\n",
      "779/779 - 14s - loss: 0.0344 - val_loss: 0.0485 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 15/2000\n",
      "779/779 - 14s - loss: 0.0336 - val_loss: 0.0474 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 16/2000\n",
      "779/779 - 14s - loss: 0.0328 - val_loss: 0.0464 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 17/2000\n",
      "779/779 - 13s - loss: 0.0321 - val_loss: 0.0450 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "779/779 - 13s - loss: 0.0315 - val_loss: 0.0443 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "779/779 - 14s - loss: 0.0309 - val_loss: 0.0432 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 20/2000\n",
      "779/779 - 14s - loss: 0.0304 - val_loss: 0.0427 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 21/2000\n",
      "779/779 - 14s - loss: 0.0299 - val_loss: 0.0419 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 22/2000\n",
      "779/779 - 13s - loss: 0.0294 - val_loss: 0.0411 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "779/779 - 13s - loss: 0.0290 - val_loss: 0.0408 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "779/779 - 13s - loss: 0.0286 - val_loss: 0.0405 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "779/779 - 13s - loss: 0.0282 - val_loss: 0.0392 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "779/779 - 13s - loss: 0.0278 - val_loss: 0.0387 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "779/779 - 13s - loss: 0.0274 - val_loss: 0.0385 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "779/779 - 13s - loss: 0.0270 - val_loss: 0.0377 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "779/779 - 13s - loss: 0.0267 - val_loss: 0.0372 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "779/779 - 13s - loss: 0.0263 - val_loss: 0.0367 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "779/779 - 13s - loss: 0.0260 - val_loss: 0.0363 - lr: 0.0100 - 13s/epoch - 16ms/step\n",
      "Epoch 32/2000\n",
      "779/779 - 13s - loss: 0.0257 - val_loss: 0.0359 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "779/779 - 13s - loss: 0.0254 - val_loss: 0.0355 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "779/779 - 13s - loss: 0.0251 - val_loss: 0.0351 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "779/779 - 13s - loss: 0.0248 - val_loss: 0.0348 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "779/779 - 13s - loss: 0.0246 - val_loss: 0.0343 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "779/779 - 13s - loss: 0.0243 - val_loss: 0.0340 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "779/779 - 13s - loss: 0.0240 - val_loss: 0.0336 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "779/779 - 13s - loss: 0.0238 - val_loss: 0.0335 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "779/779 - 13s - loss: 0.0235 - val_loss: 0.0331 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "779/779 - 13s - loss: 0.0233 - val_loss: 0.0325 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "779/779 - 14s - loss: 0.0231 - val_loss: 0.0321 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 43/2000\n",
      "779/779 - 13s - loss: 0.0228 - val_loss: 0.0321 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "779/779 - 13s - loss: 0.0226 - val_loss: 0.0317 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "779/779 - 13s - loss: 0.0224 - val_loss: 0.0314 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "779/779 - 13s - loss: 0.0222 - val_loss: 0.0310 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "779/779 - 13s - loss: 0.0220 - val_loss: 0.0307 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "779/779 - 13s - loss: 0.0218 - val_loss: 0.0304 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "779/779 - 13s - loss: 0.0216 - val_loss: 0.0301 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "779/779 - 13s - loss: 0.0214 - val_loss: 0.0298 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "779/779 - 13s - loss: 0.0212 - val_loss: 0.0297 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "779/779 - 13s - loss: 0.0210 - val_loss: 0.0295 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "779/779 - 13s - loss: 0.0208 - val_loss: 0.0294 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "779/779 - 13s - loss: 0.0207 - val_loss: 0.0291 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "779/779 - 13s - loss: 0.0205 - val_loss: 0.0286 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "779/779 - 13s - loss: 0.0203 - val_loss: 0.0284 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "779/779 - 13s - loss: 0.0202 - val_loss: 0.0283 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "779/779 - 13s - loss: 0.0200 - val_loss: 0.0279 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "779/779 - 13s - loss: 0.0199 - val_loss: 0.0277 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "779/779 - 13s - loss: 0.0197 - val_loss: 0.0275 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "779/779 - 13s - loss: 0.0196 - val_loss: 0.0273 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "779/779 - 13s - loss: 0.0194 - val_loss: 0.0271 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "779/779 - 13s - loss: 0.0193 - val_loss: 0.0269 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "779/779 - 13s - loss: 0.0191 - val_loss: 0.0267 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "779/779 - 13s - loss: 0.0190 - val_loss: 0.0265 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "779/779 - 13s - loss: 0.0189 - val_loss: 0.0264 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "779/779 - 13s - loss: 0.0187 - val_loss: 0.0261 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "779/779 - 13s - loss: 0.0186 - val_loss: 0.0260 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "779/779 - 13s - loss: 0.0185 - val_loss: 0.0258 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "779/779 - 13s - loss: 0.0183 - val_loss: 0.0258 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "779/779 - 13s - loss: 0.0182 - val_loss: 0.0257 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "779/779 - 13s - loss: 0.0181 - val_loss: 0.0252 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "779/779 - 13s - loss: 0.0180 - val_loss: 0.0250 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "779/779 - 13s - loss: 0.0179 - val_loss: 0.0251 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "779/779 - 13s - loss: 0.0178 - val_loss: 0.0248 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "779/779 - 13s - loss: 0.0177 - val_loss: 0.0245 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "779/779 - 13s - loss: 0.0175 - val_loss: 0.0244 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "779/779 - 13s - loss: 0.0174 - val_loss: 0.0242 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "779/779 - 13s - loss: 0.0173 - val_loss: 0.0240 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 80/2000\n",
      "779/779 - 14s - loss: 0.0172 - val_loss: 0.0241 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "779/779 - 13s - loss: 0.0171 - val_loss: 0.0238 - lr: 0.0100 - 13s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/2000\n",
      "779/779 - 13s - loss: 0.0170 - val_loss: 0.0237 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "779/779 - 14s - loss: 0.0169 - val_loss: 0.0239 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "779/779 - 14s - loss: 0.0168 - val_loss: 0.0233 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "779/779 - 13s - loss: 0.0167 - val_loss: 0.0231 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "779/779 - 13s - loss: 0.0166 - val_loss: 0.0232 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "779/779 - 13s - loss: 0.0166 - val_loss: 0.0230 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "779/779 - 13s - loss: 0.0165 - val_loss: 0.0227 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "779/779 - 13s - loss: 0.0164 - val_loss: 0.0226 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "779/779 - 13s - loss: 0.0163 - val_loss: 0.0224 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "779/779 - 13s - loss: 0.0162 - val_loss: 0.0225 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "779/779 - 13s - loss: 0.0161 - val_loss: 0.0222 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "779/779 - 13s - loss: 0.0160 - val_loss: 0.0220 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "779/779 - 13s - loss: 0.0159 - val_loss: 0.0219 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "779/779 - 13s - loss: 0.0158 - val_loss: 0.0219 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "779/779 - 13s - loss: 0.0158 - val_loss: 0.0217 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "779/779 - 13s - loss: 0.0157 - val_loss: 0.0215 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "779/779 - 13s - loss: 0.0156 - val_loss: 0.0215 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "779/779 - 13s - loss: 0.0155 - val_loss: 0.0213 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "779/779 - 13s - loss: 0.0154 - val_loss: 0.0212 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "779/779 - 13s - loss: 0.0154 - val_loss: 0.0210 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "779/779 - 13s - loss: 0.0153 - val_loss: 0.0210 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "779/779 - 13s - loss: 0.0152 - val_loss: 0.0208 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "779/779 - 13s - loss: 0.0151 - val_loss: 0.0211 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "779/779 - 13s - loss: 0.0151 - val_loss: 0.0206 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "779/779 - 13s - loss: 0.0150 - val_loss: 0.0205 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "779/779 - 13s - loss: 0.0149 - val_loss: 0.0206 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "779/779 - 13s - loss: 0.0149 - val_loss: 0.0202 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "779/779 - 14s - loss: 0.0148 - val_loss: 0.0202 - lr: 0.0100 - 14s/epoch - 18ms/step\n",
      "Epoch 110/2000\n",
      "779/779 - 13s - loss: 0.0147 - val_loss: 0.0200 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "779/779 - 13s - loss: 0.0146 - val_loss: 0.0202 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "779/779 - 13s - loss: 0.0146 - val_loss: 0.0199 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "779/779 - 13s - loss: 0.0145 - val_loss: 0.0199 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "779/779 - 13s - loss: 0.0144 - val_loss: 0.0197 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "779/779 - 13s - loss: 0.0144 - val_loss: 0.0196 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "779/779 - 13s - loss: 0.0143 - val_loss: 0.0194 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "779/779 - 13s - loss: 0.0143 - val_loss: 0.0195 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "779/779 - 13s - loss: 0.0142 - val_loss: 0.0192 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "779/779 - 13s - loss: 0.0141 - val_loss: 0.0193 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "779/779 - 13s - loss: 0.0141 - val_loss: 0.0192 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "779/779 - 13s - loss: 0.0140 - val_loss: 0.0190 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "779/779 - 13s - loss: 0.0140 - val_loss: 0.0189 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "779/779 - 13s - loss: 0.0139 - val_loss: 0.0188 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "779/779 - 13s - loss: 0.0138 - val_loss: 0.0187 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "779/779 - 13s - loss: 0.0138 - val_loss: 0.0186 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "779/779 - 13s - loss: 0.0137 - val_loss: 0.0186 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 127/2000\n",
      "779/779 - 13s - loss: 0.0137 - val_loss: 0.0185 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "779/779 - 13s - loss: 0.0136 - val_loss: 0.0184 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 129/2000\n",
      "779/779 - 13s - loss: 0.0136 - val_loss: 0.0183 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 130/2000\n",
      "779/779 - 13s - loss: 0.0135 - val_loss: 0.0182 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 131/2000\n",
      "779/779 - 13s - loss: 0.0135 - val_loss: 0.0181 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 132/2000\n",
      "779/779 - 13s - loss: 0.0134 - val_loss: 0.0180 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 133/2000\n",
      "779/779 - 13s - loss: 0.0134 - val_loss: 0.0181 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 134/2000\n",
      "779/779 - 13s - loss: 0.0133 - val_loss: 0.0179 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 135/2000\n",
      "779/779 - 13s - loss: 0.0133 - val_loss: 0.0178 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 136/2000\n",
      "779/779 - 13s - loss: 0.0132 - val_loss: 0.0178 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 137/2000\n",
      "779/779 - 13s - loss: 0.0132 - val_loss: 0.0178 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 138/2000\n",
      "779/779 - 13s - loss: 0.0131 - val_loss: 0.0180 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 139/2000\n",
      "779/779 - 13s - loss: 0.0131 - val_loss: 0.0175 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 140/2000\n",
      "779/779 - 13s - loss: 0.0130 - val_loss: 0.0174 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 141/2000\n",
      "779/779 - 13s - loss: 0.0130 - val_loss: 0.0174 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 142/2000\n",
      "779/779 - 13s - loss: 0.0129 - val_loss: 0.0174 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 143/2000\n",
      "779/779 - 13s - loss: 0.0129 - val_loss: 0.0173 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 144/2000\n",
      "779/779 - 13s - loss: 0.0128 - val_loss: 0.0173 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 145/2000\n",
      "779/779 - 13s - loss: 0.0128 - val_loss: 0.0172 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 146/2000\n",
      "779/779 - 13s - loss: 0.0128 - val_loss: 0.0170 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 147/2000\n",
      "779/779 - 13s - loss: 0.0127 - val_loss: 0.0175 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 148/2000\n",
      "779/779 - 13s - loss: 0.0127 - val_loss: 0.0169 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 149/2000\n",
      "779/779 - 13s - loss: 0.0126 - val_loss: 0.0168 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 150/2000\n",
      "779/779 - 13s - loss: 0.0126 - val_loss: 0.0168 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 151/2000\n",
      "779/779 - 13s - loss: 0.0125 - val_loss: 0.0169 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 152/2000\n",
      "779/779 - 13s - loss: 0.0125 - val_loss: 0.0169 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 153/2000\n",
      "779/779 - 13s - loss: 0.0125 - val_loss: 0.0166 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 154/2000\n",
      "779/779 - 13s - loss: 0.0124 - val_loss: 0.0166 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 155/2000\n",
      "779/779 - 13s - loss: 0.0124 - val_loss: 0.0167 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 156/2000\n",
      "779/779 - 13s - loss: 0.0123 - val_loss: 0.0165 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 157/2000\n",
      "779/779 - 13s - loss: 0.0123 - val_loss: 0.0164 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 158/2000\n",
      "779/779 - 13s - loss: 0.0123 - val_loss: 0.0163 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 159/2000\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "779/779 - 13s - loss: 0.0122 - val_loss: 0.0163 - lr: 0.0100 - 13s/epoch - 17ms/step\n",
      "Epoch 160/2000\n",
      "779/779 - 13s - loss: 0.0122 - val_loss: 0.0163 - lr: 0.0020 - 13s/epoch - 17ms/step\n",
      "Epoch 161/2000\n",
      "779/779 - 13s - loss: 0.0122 - val_loss: 0.0162 - lr: 0.0020 - 13s/epoch - 17ms/step\n",
      "Epoch 162/2000\n",
      "779/779 - 13s - loss: 0.0122 - val_loss: 0.0163 - lr: 0.0020 - 13s/epoch - 17ms/step\n",
      "Epoch 163/2000\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 - 13s - loss: 0.0122 - val_loss: 0.0163 - lr: 0.0020 - 13s/epoch - 17ms/step\n",
      "Epoch 164/2000\n",
      "779/779 - 13s - loss: 0.0122 - val_loss: 0.0162 - lr: 4.0000e-04 - 13s/epoch - 17ms/step\n",
      "Epoch 165/2000\n",
      "779/779 - 13s - loss: 0.0122 - val_loss: 0.0162 - lr: 4.0000e-04 - 13s/epoch - 17ms/step\n",
      "Epoch 166/2000\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 4.0000e-04 - 13s/epoch - 17ms/step\n",
      "Epoch 167/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 8.0000e-05 - 13s/epoch - 17ms/step\n",
      "Epoch 168/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 8.0000e-05 - 13s/epoch - 17ms/step\n",
      "Epoch 169/2000\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 8.0000e-05 - 13s/epoch - 17ms/step\n",
      "Epoch 170/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 1.6000e-05 - 13s/epoch - 17ms/step\n",
      "Epoch 171/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 1.6000e-05 - 13s/epoch - 17ms/step\n",
      "Epoch 172/2000\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 1.6000e-05 - 13s/epoch - 17ms/step\n",
      "Epoch 173/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 3.2000e-06 - 13s/epoch - 17ms/step\n",
      "Epoch 174/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 3.2000e-06 - 13s/epoch - 17ms/step\n",
      "Epoch 175/2000\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 3.2000e-06 - 13s/epoch - 17ms/step\n",
      "Epoch 176/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 6.4000e-07 - 13s/epoch - 17ms/step\n",
      "Epoch 177/2000\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 6.4000e-07 - 14s/epoch - 18ms/step\n",
      "Epoch 178/2000\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 6.4000e-07 - 14s/epoch - 17ms/step\n",
      "Epoch 179/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 1.2800e-07 - 13s/epoch - 17ms/step\n",
      "Epoch 180/2000\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 1.2800e-07 - 14s/epoch - 18ms/step\n",
      "Epoch 181/2000\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 1.2800e-07 - 14s/epoch - 18ms/step\n",
      "Epoch 182/2000\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 2.5600e-08 - 14s/epoch - 18ms/step\n",
      "Epoch 183/2000\n",
      "779/779 - 13s - loss: 0.0121 - val_loss: 0.0162 - lr: 2.5600e-08 - 13s/epoch - 17ms/step\n",
      "Epoch 184/2000\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 2.5600e-08 - 14s/epoch - 18ms/step\n",
      "Epoch 185/2000\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 5.1200e-09 - 14s/epoch - 18ms/step\n",
      "Epoch 186/2000\n",
      "Restoring model weights from the end of the best epoch: 166.\n",
      "779/779 - 14s - loss: 0.0121 - val_loss: 0.0162 - lr: 5.1200e-09 - 14s/epoch - 18ms/step\n",
      "Epoch 186: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/CommunicationServices/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/CommunicationServices/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 6s 7ms/step - loss: 0.0121\n",
      "195/195 [==============================] - 1s 7ms/step - loss: 0.0162\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01215\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.01621\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "1010/1010 - 26s - loss: 0.1493 - val_loss: 0.0866 - lr: 0.0100 - 26s/epoch - 26ms/step\n",
      "Epoch 2/2000\n",
      "1010/1010 - 17s - loss: 0.0723 - val_loss: 0.0689 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "1010/1010 - 18s - loss: 0.0619 - val_loss: 0.0630 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 4/2000\n",
      "1010/1010 - 18s - loss: 0.0573 - val_loss: 0.0595 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 5/2000\n",
      "1010/1010 - 18s - loss: 0.0539 - val_loss: 0.0563 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "1010/1010 - 18s - loss: 0.0508 - val_loss: 0.0536 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 7/2000\n",
      "1010/1010 - 18s - loss: 0.0481 - val_loss: 0.0514 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "1010/1010 - 18s - loss: 0.0458 - val_loss: 0.0491 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 9/2000\n",
      "1010/1010 - 18s - loss: 0.0438 - val_loss: 0.0481 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 10/2000\n",
      "1010/1010 - 18s - loss: 0.0420 - val_loss: 0.0459 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 11/2000\n",
      "1010/1010 - 18s - loss: 0.0406 - val_loss: 0.0446 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 12/2000\n",
      "1010/1010 - 18s - loss: 0.0393 - val_loss: 0.0434 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 13/2000\n",
      "1010/1010 - 18s - loss: 0.0383 - val_loss: 0.0425 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 14/2000\n",
      "1010/1010 - 18s - loss: 0.0373 - val_loss: 0.0415 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 15/2000\n",
      "1010/1010 - 18s - loss: 0.0365 - val_loss: 0.0407 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "1010/1010 - 17s - loss: 0.0358 - val_loss: 0.0399 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "1010/1010 - 17s - loss: 0.0350 - val_loss: 0.0391 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "1010/1010 - 18s - loss: 0.0343 - val_loss: 0.0384 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 19/2000\n",
      "1010/1010 - 18s - loss: 0.0337 - val_loss: 0.0376 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 20/2000\n",
      "1010/1010 - 17s - loss: 0.0331 - val_loss: 0.0370 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "1010/1010 - 18s - loss: 0.0325 - val_loss: 0.0363 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 22/2000\n",
      "1010/1010 - 18s - loss: 0.0320 - val_loss: 0.0356 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "1010/1010 - 18s - loss: 0.0314 - val_loss: 0.0350 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "1010/1010 - 18s - loss: 0.0309 - val_loss: 0.0345 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 25/2000\n",
      "1010/1010 - 22s - loss: 0.0305 - val_loss: 0.0339 - lr: 0.0100 - 22s/epoch - 22ms/step\n",
      "Epoch 26/2000\n",
      "1010/1010 - 18s - loss: 0.0300 - val_loss: 0.0334 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "1010/1010 - 17s - loss: 0.0296 - val_loss: 0.0330 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "1010/1010 - 18s - loss: 0.0291 - val_loss: 0.0323 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 29/2000\n",
      "1010/1010 - 20s - loss: 0.0287 - val_loss: 0.0319 - lr: 0.0100 - 20s/epoch - 20ms/step\n",
      "Epoch 30/2000\n",
      "1010/1010 - 21s - loss: 0.0284 - val_loss: 0.0314 - lr: 0.0100 - 21s/epoch - 20ms/step\n",
      "Epoch 31/2000\n",
      "1010/1010 - 18s - loss: 0.0280 - val_loss: 0.0310 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 32/2000\n",
      "1010/1010 - 18s - loss: 0.0276 - val_loss: 0.0306 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 33/2000\n",
      "1010/1010 - 18s - loss: 0.0273 - val_loss: 0.0302 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 34/2000\n",
      "1010/1010 - 18s - loss: 0.0269 - val_loss: 0.0298 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 35/2000\n",
      "1010/1010 - 18s - loss: 0.0266 - val_loss: 0.0294 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 36/2000\n",
      "1010/1010 - 18s - loss: 0.0263 - val_loss: 0.0291 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 37/2000\n",
      "1010/1010 - 18s - loss: 0.0260 - val_loss: 0.0287 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 38/2000\n",
      "1010/1010 - 18s - loss: 0.0257 - val_loss: 0.0283 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 39/2000\n",
      "1010/1010 - 18s - loss: 0.0254 - val_loss: 0.0281 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 40/2000\n",
      "1010/1010 - 17s - loss: 0.0252 - val_loss: 0.0276 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "1010/1010 - 17s - loss: 0.0249 - val_loss: 0.0274 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "1010/1010 - 17s - loss: 0.0246 - val_loss: 0.0275 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "1010/1010 - 17s - loss: 0.0244 - val_loss: 0.0268 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "1010/1010 - 17s - loss: 0.0242 - val_loss: 0.0265 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "1010/1010 - 17s - loss: 0.0239 - val_loss: 0.0262 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "1010/1010 - 17s - loss: 0.0237 - val_loss: 0.0261 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "1010/1010 - 17s - loss: 0.0235 - val_loss: 0.0258 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "1010/1010 - 17s - loss: 0.0232 - val_loss: 0.0255 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "1010/1010 - 17s - loss: 0.0230 - val_loss: 0.0252 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "1010/1010 - 17s - loss: 0.0228 - val_loss: 0.0249 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "1010/1010 - 17s - loss: 0.0226 - val_loss: 0.0247 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "1010/1010 - 17s - loss: 0.0224 - val_loss: 0.0244 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "1010/1010 - 18s - loss: 0.0222 - val_loss: 0.0242 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 54/2000\n",
      "1010/1010 - 17s - loss: 0.0220 - val_loss: 0.0241 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "1010/1010 - 17s - loss: 0.0218 - val_loss: 0.0238 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "1010/1010 - 17s - loss: 0.0216 - val_loss: 0.0237 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "1010/1010 - 17s - loss: 0.0215 - val_loss: 0.0236 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "1010/1010 - 17s - loss: 0.0213 - val_loss: 0.0233 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "1010/1010 - 17s - loss: 0.0211 - val_loss: 0.0231 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "1010/1010 - 17s - loss: 0.0209 - val_loss: 0.0228 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "1010/1010 - 17s - loss: 0.0208 - val_loss: 0.0226 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "1010/1010 - 17s - loss: 0.0206 - val_loss: 0.0225 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "1010/1010 - 17s - loss: 0.0204 - val_loss: 0.0223 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "1010/1010 - 17s - loss: 0.0203 - val_loss: 0.0221 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "1010/1010 - 17s - loss: 0.0201 - val_loss: 0.0219 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "1010/1010 - 18s - loss: 0.0199 - val_loss: 0.0219 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "1010/1010 - 17s - loss: 0.0198 - val_loss: 0.0217 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "1010/1010 - 17s - loss: 0.0196 - val_loss: 0.0214 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "1010/1010 - 17s - loss: 0.0195 - val_loss: 0.0212 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "1010/1010 - 17s - loss: 0.0194 - val_loss: 0.0211 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "1010/1010 - 17s - loss: 0.0192 - val_loss: 0.0210 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "1010/1010 - 17s - loss: 0.0191 - val_loss: 0.0208 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "1010/1010 - 17s - loss: 0.0189 - val_loss: 0.0206 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "1010/1010 - 17s - loss: 0.0188 - val_loss: 0.0205 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "1010/1010 - 17s - loss: 0.0187 - val_loss: 0.0204 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "1010/1010 - 17s - loss: 0.0185 - val_loss: 0.0202 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "1010/1010 - 17s - loss: 0.0184 - val_loss: 0.0201 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "1010/1010 - 17s - loss: 0.0183 - val_loss: 0.0200 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "1010/1010 - 17s - loss: 0.0182 - val_loss: 0.0198 - lr: 0.0100 - 17s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "1010/1010 - 17s - loss: 0.0180 - val_loss: 0.0198 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "1010/1010 - 17s - loss: 0.0179 - val_loss: 0.0196 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "1010/1010 - 18s - loss: 0.0178 - val_loss: 0.0195 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "1010/1010 - 17s - loss: 0.0177 - val_loss: 0.0193 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "1010/1010 - 17s - loss: 0.0176 - val_loss: 0.0192 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "1010/1010 - 18s - loss: 0.0174 - val_loss: 0.0191 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "1010/1010 - 18s - loss: 0.0173 - val_loss: 0.0190 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "1010/1010 - 17s - loss: 0.0172 - val_loss: 0.0189 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "1010/1010 - 17s - loss: 0.0171 - val_loss: 0.0187 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "1010/1010 - 17s - loss: 0.0170 - val_loss: 0.0187 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "1010/1010 - 17s - loss: 0.0169 - val_loss: 0.0185 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "1010/1010 - 17s - loss: 0.0168 - val_loss: 0.0184 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "1010/1010 - 17s - loss: 0.0167 - val_loss: 0.0183 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "1010/1010 - 17s - loss: 0.0166 - val_loss: 0.0182 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "1010/1010 - 17s - loss: 0.0165 - val_loss: 0.0181 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "1010/1010 - 17s - loss: 0.0164 - val_loss: 0.0180 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "1010/1010 - 17s - loss: 0.0163 - val_loss: 0.0179 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "1010/1010 - 17s - loss: 0.0162 - val_loss: 0.0178 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "1010/1010 - 17s - loss: 0.0161 - val_loss: 0.0177 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "1010/1010 - 17s - loss: 0.0161 - val_loss: 0.0176 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "1010/1010 - 17s - loss: 0.0160 - val_loss: 0.0175 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "1010/1010 - 17s - loss: 0.0159 - val_loss: 0.0175 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "1010/1010 - 17s - loss: 0.0158 - val_loss: 0.0173 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "1010/1010 - 17s - loss: 0.0157 - val_loss: 0.0173 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "1010/1010 - 17s - loss: 0.0156 - val_loss: 0.0172 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "1010/1010 - 17s - loss: 0.0156 - val_loss: 0.0171 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "1010/1010 - 17s - loss: 0.0155 - val_loss: 0.0171 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "1010/1010 - 17s - loss: 0.0154 - val_loss: 0.0169 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "1010/1010 - 17s - loss: 0.0153 - val_loss: 0.0169 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "1010/1010 - 17s - loss: 0.0153 - val_loss: 0.0168 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "1010/1010 - 17s - loss: 0.0152 - val_loss: 0.0168 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "1010/1010 - 17s - loss: 0.0151 - val_loss: 0.0167 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "1010/1010 - 17s - loss: 0.0151 - val_loss: 0.0166 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "1010/1010 - 17s - loss: 0.0150 - val_loss: 0.0165 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "1010/1010 - 17s - loss: 0.0149 - val_loss: 0.0165 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "1010/1010 - 17s - loss: 0.0149 - val_loss: 0.0164 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "1010/1010 - 17s - loss: 0.0148 - val_loss: 0.0163 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "1010/1010 - 17s - loss: 0.0147 - val_loss: 0.0162 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "1010/1010 - 17s - loss: 0.0147 - val_loss: 0.0162 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "1010/1010 - 17s - loss: 0.0146 - val_loss: 0.0161 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "1010/1010 - 17s - loss: 0.0146 - val_loss: 0.0161 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "1010/1010 - 17s - loss: 0.0145 - val_loss: 0.0160 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "1010/1010 - 17s - loss: 0.0145 - val_loss: 0.0160 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "1010/1010 - 17s - loss: 0.0144 - val_loss: 0.0159 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "1010/1010 - 17s - loss: 0.0144 - val_loss: 0.0158 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "1010/1010 - 17s - loss: 0.0143 - val_loss: 0.0158 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "1010/1010 - 17s - loss: 0.0142 - val_loss: 0.0157 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 127/2000\n",
      "1010/1010 - 17s - loss: 0.0142 - val_loss: 0.0157 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "1010/1010 - 17s - loss: 0.0142 - val_loss: 0.0157 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 129/2000\n",
      "1010/1010 - 17s - loss: 0.0141 - val_loss: 0.0156 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 130/2000\n",
      "1010/1010 - 17s - loss: 0.0141 - val_loss: 0.0156 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 131/2000\n",
      "1010/1010 - 17s - loss: 0.0140 - val_loss: 0.0155 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 132/2000\n",
      "1010/1010 - 17s - loss: 0.0140 - val_loss: 0.0155 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 133/2000\n",
      "1010/1010 - 17s - loss: 0.0140 - val_loss: 0.0155 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 134/2000\n",
      "1010/1010 - 17s - loss: 0.0139 - val_loss: 0.0155 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 135/2000\n",
      "1010/1010 - 17s - loss: 0.0139 - val_loss: 0.0153 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 136/2000\n",
      "1010/1010 - 17s - loss: 0.0138 - val_loss: 0.0153 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 137/2000\n",
      "1010/1010 - 17s - loss: 0.0138 - val_loss: 0.0153 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 138/2000\n",
      "1010/1010 - 18s - loss: 0.0138 - val_loss: 0.0152 - lr: 0.0100 - 18s/epoch - 18ms/step\n",
      "Epoch 139/2000\n",
      "1010/1010 - 18s - loss: 0.0137 - val_loss: 0.0152 - lr: 0.0100 - 18s/epoch - 17ms/step\n",
      "Epoch 140/2000\n",
      "1010/1010 - 17s - loss: 0.0137 - val_loss: 0.0152 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 141/2000\n",
      "1010/1010 - 17s - loss: 0.0137 - val_loss: 0.0152 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 142/2000\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "1010/1010 - 17s - loss: 0.0136 - val_loss: 0.0151 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 143/2000\n",
      "1010/1010 - 17s - loss: 0.0136 - val_loss: 0.0151 - lr: 0.0020 - 17s/epoch - 17ms/step\n",
      "Epoch 144/2000\n",
      "1010/1010 - 18s - loss: 0.0136 - val_loss: 0.0151 - lr: 0.0020 - 18s/epoch - 17ms/step\n",
      "Epoch 145/2000\n",
      "1010/1010 - 17s - loss: 0.0136 - val_loss: 0.0151 - lr: 0.0020 - 17s/epoch - 17ms/step\n",
      "Epoch 146/2000\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "1010/1010 - 18s - loss: 0.0136 - val_loss: 0.0151 - lr: 0.0020 - 18s/epoch - 17ms/step\n",
      "Epoch 147/2000\n",
      "1010/1010 - 17s - loss: 0.0136 - val_loss: 0.0150 - lr: 4.0000e-04 - 17s/epoch - 17ms/step\n",
      "Epoch 148/2000\n",
      "1010/1010 - 17s - loss: 0.0136 - val_loss: 0.0150 - lr: 4.0000e-04 - 17s/epoch - 17ms/step\n",
      "Epoch 149/2000\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 4.0000e-04 - 17s/epoch - 17ms/step\n",
      "Epoch 150/2000\n",
      "1010/1010 - 18s - loss: 0.0135 - val_loss: 0.0150 - lr: 8.0000e-05 - 18s/epoch - 17ms/step\n",
      "Epoch 151/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 8.0000e-05 - 17s/epoch - 17ms/step\n",
      "Epoch 152/2000\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 8.0000e-05 - 17s/epoch - 17ms/step\n",
      "Epoch 153/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.6000e-05 - 17s/epoch - 17ms/step\n",
      "Epoch 154/2000\n",
      "1010/1010 - 18s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.6000e-05 - 18s/epoch - 17ms/step\n",
      "Epoch 155/2000\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.6000e-05 - 17s/epoch - 17ms/step\n",
      "Epoch 156/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 3.2000e-06 - 17s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 3.2000e-06 - 17s/epoch - 17ms/step\n",
      "Epoch 158/2000\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 3.2000e-06 - 17s/epoch - 17ms/step\n",
      "Epoch 159/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 6.4000e-07 - 17s/epoch - 17ms/step\n",
      "Epoch 160/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 6.4000e-07 - 17s/epoch - 17ms/step\n",
      "Epoch 161/2000\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 6.4000e-07 - 17s/epoch - 17ms/step\n",
      "Epoch 162/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.2800e-07 - 17s/epoch - 17ms/step\n",
      "Epoch 163/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.2800e-07 - 17s/epoch - 17ms/step\n",
      "Epoch 164/2000\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.2800e-07 - 17s/epoch - 17ms/step\n",
      "Epoch 165/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 2.5600e-08 - 17s/epoch - 17ms/step\n",
      "Epoch 166/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 2.5600e-08 - 17s/epoch - 17ms/step\n",
      "Epoch 167/2000\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 2.5600e-08 - 17s/epoch - 17ms/step\n",
      "Epoch 168/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 5.1200e-09 - 17s/epoch - 17ms/step\n",
      "Epoch 169/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 5.1200e-09 - 17s/epoch - 17ms/step\n",
      "Epoch 170/2000\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 5.1200e-09 - 17s/epoch - 17ms/step\n",
      "Epoch 171/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.0240e-09 - 17s/epoch - 17ms/step\n",
      "Epoch 172/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.0240e-09 - 17s/epoch - 17ms/step\n",
      "Epoch 173/2000\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 1.0240e-09 - 17s/epoch - 17ms/step\n",
      "Epoch 174/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 2.0480e-10 - 17s/epoch - 17ms/step\n",
      "Epoch 175/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 2.0480e-10 - 17s/epoch - 17ms/step\n",
      "Epoch 176/2000\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 2.0480e-10 - 17s/epoch - 17ms/step\n",
      "Epoch 177/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 4.0960e-11 - 17s/epoch - 17ms/step\n",
      "Epoch 178/2000\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 4.0960e-11 - 17s/epoch - 17ms/step\n",
      "Epoch 179/2000\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 4.0960e-11 - 17s/epoch - 17ms/step\n",
      "Epoch 180/2000\n",
      "Restoring model weights from the end of the best epoch: 160.\n",
      "1010/1010 - 17s - loss: 0.0135 - val_loss: 0.0150 - lr: 8.1920e-12 - 17s/epoch - 17ms/step\n",
      "Epoch 180: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerStaples/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerStaples/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010/1010 [==============================] - 7s 7ms/step - loss: 0.0135\n",
      "253/253 [==============================] - 2s 7ms/step - loss: 0.0150\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01355\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.01504\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "1714/1714 - 38s - loss: 0.1001 - val_loss: 0.0509 - lr: 0.0100 - 38s/epoch - 22ms/step\n",
      "Epoch 2/2000\n",
      "1714/1714 - 29s - loss: 0.0509 - val_loss: 0.0403 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "1714/1714 - 29s - loss: 0.0443 - val_loss: 0.0363 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "1714/1714 - 29s - loss: 0.0402 - val_loss: 0.0330 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "1714/1714 - 29s - loss: 0.0369 - val_loss: 0.0307 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "1714/1714 - 30s - loss: 0.0343 - val_loss: 0.0288 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "1714/1714 - 29s - loss: 0.0321 - val_loss: 0.0274 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "1714/1714 - 29s - loss: 0.0305 - val_loss: 0.0260 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "1714/1714 - 29s - loss: 0.0293 - val_loss: 0.0249 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "1714/1714 - 29s - loss: 0.0282 - val_loss: 0.0240 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "1714/1714 - 29s - loss: 0.0272 - val_loss: 0.0233 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "1714/1714 - 29s - loss: 0.0264 - val_loss: 0.0226 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "1714/1714 - 29s - loss: 0.0256 - val_loss: 0.0220 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "1714/1714 - 30s - loss: 0.0249 - val_loss: 0.0214 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "1714/1714 - 30s - loss: 0.0242 - val_loss: 0.0209 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "1714/1714 - 30s - loss: 0.0236 - val_loss: 0.0205 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "1714/1714 - 30s - loss: 0.0230 - val_loss: 0.0200 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "1714/1714 - 29s - loss: 0.0225 - val_loss: 0.0197 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "1714/1714 - 29s - loss: 0.0219 - val_loss: 0.0193 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "1714/1714 - 29s - loss: 0.0215 - val_loss: 0.0187 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "1714/1714 - 29s - loss: 0.0210 - val_loss: 0.0184 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "1714/1714 - 29s - loss: 0.0206 - val_loss: 0.0180 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "1714/1714 - 29s - loss: 0.0201 - val_loss: 0.0176 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "1714/1714 - 29s - loss: 0.0198 - val_loss: 0.0174 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "1714/1714 - 29s - loss: 0.0194 - val_loss: 0.0171 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "1714/1714 - 29s - loss: 0.0190 - val_loss: 0.0168 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "1714/1714 - 29s - loss: 0.0187 - val_loss: 0.0165 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "1714/1714 - 29s - loss: 0.0184 - val_loss: 0.0162 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "1714/1714 - 30s - loss: 0.0181 - val_loss: 0.0160 - lr: 0.0100 - 30s/epoch - 18ms/step\n",
      "Epoch 30/2000\n",
      "1714/1714 - 30s - loss: 0.0178 - val_loss: 0.0157 - lr: 0.0100 - 30s/epoch - 18ms/step\n",
      "Epoch 31/2000\n",
      "1714/1714 - 30s - loss: 0.0175 - val_loss: 0.0155 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "1714/1714 - 29s - loss: 0.0172 - val_loss: 0.0153 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "1714/1714 - 30s - loss: 0.0170 - val_loss: 0.0150 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "1714/1714 - 29s - loss: 0.0167 - val_loss: 0.0150 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "1714/1714 - 29s - loss: 0.0165 - val_loss: 0.0147 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "1714/1714 - 30s - loss: 0.0163 - val_loss: 0.0145 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "1714/1714 - 29s - loss: 0.0160 - val_loss: 0.0143 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "1714/1714 - 29s - loss: 0.0158 - val_loss: 0.0141 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "1714/1714 - 29s - loss: 0.0156 - val_loss: 0.0139 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "1714/1714 - 29s - loss: 0.0154 - val_loss: 0.0138 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "1714/1714 - 29s - loss: 0.0152 - val_loss: 0.0136 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "1714/1714 - 29s - loss: 0.0150 - val_loss: 0.0134 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "1714/1714 - 29s - loss: 0.0149 - val_loss: 0.0133 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "1714/1714 - 29s - loss: 0.0147 - val_loss: 0.0132 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "1714/1714 - 29s - loss: 0.0145 - val_loss: 0.0130 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "1714/1714 - 29s - loss: 0.0143 - val_loss: 0.0128 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "1714/1714 - 29s - loss: 0.0142 - val_loss: 0.0127 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "1714/1714 - 29s - loss: 0.0140 - val_loss: 0.0127 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "1714/1714 - 29s - loss: 0.0139 - val_loss: 0.0124 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "1714/1714 - 29s - loss: 0.0137 - val_loss: 0.0123 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "1714/1714 - 29s - loss: 0.0136 - val_loss: 0.0122 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "1714/1714 - 29s - loss: 0.0135 - val_loss: 0.0121 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "1714/1714 - 29s - loss: 0.0133 - val_loss: 0.0119 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "1714/1714 - 29s - loss: 0.0132 - val_loss: 0.0118 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "1714/1714 - 29s - loss: 0.0131 - val_loss: 0.0117 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "1714/1714 - 29s - loss: 0.0130 - val_loss: 0.0116 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "1714/1714 - 28s - loss: 0.0128 - val_loss: 0.0115 - lr: 0.0100 - 28s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "1714/1714 - 29s - loss: 0.0127 - val_loss: 0.0114 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "1714/1714 - 29s - loss: 0.0126 - val_loss: 0.0113 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "1714/1714 - 30s - loss: 0.0125 - val_loss: 0.0112 - lr: 0.0100 - 30s/epoch - 18ms/step\n",
      "Epoch 61/2000\n",
      "1714/1714 - 29s - loss: 0.0124 - val_loss: 0.0112 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "1714/1714 - 29s - loss: 0.0123 - val_loss: 0.0111 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "1714/1714 - 29s - loss: 0.0122 - val_loss: 0.0109 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "1714/1714 - 30s - loss: 0.0121 - val_loss: 0.0109 - lr: 0.0100 - 30s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "1714/1714 - 29s - loss: 0.0120 - val_loss: 0.0108 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "1714/1714 - 29s - loss: 0.0119 - val_loss: 0.0107 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "1714/1714 - 29s - loss: 0.0118 - val_loss: 0.0106 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "1714/1714 - 29s - loss: 0.0118 - val_loss: 0.0106 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "1714/1714 - 29s - loss: 0.0117 - val_loss: 0.0105 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "1714/1714 - 29s - loss: 0.0116 - val_loss: 0.0104 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "1714/1714 - 29s - loss: 0.0115 - val_loss: 0.0105 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "1714/1714 - 29s - loss: 0.0114 - val_loss: 0.0102 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "1714/1714 - 29s - loss: 0.0114 - val_loss: 0.0102 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "1714/1714 - 29s - loss: 0.0113 - val_loss: 0.0102 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "1714/1714 - 29s - loss: 0.0112 - val_loss: 0.0101 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "1714/1714 - 29s - loss: 0.0112 - val_loss: 0.0100 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "1714/1714 - 29s - loss: 0.0111 - val_loss: 0.0100 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "1714/1714 - 29s - loss: 0.0110 - val_loss: 0.0099 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "1714/1714 - 29s - loss: 0.0110 - val_loss: 0.0098 - lr: 0.0100 - 29s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "1714/1714 - 29s - loss: 0.0109 - val_loss: 0.0098 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "1714/1714 - 29s - loss: 0.0109 - val_loss: 0.0097 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "1714/1714 - 29s - loss: 0.0108 - val_loss: 0.0099 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "1714/1714 - 29s - loss: 0.0107 - val_loss: 0.0096 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "1714/1714 - 29s - loss: 0.0107 - val_loss: 0.0096 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "1714/1714 - 29s - loss: 0.0106 - val_loss: 0.0095 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "1714/1714 - 29s - loss: 0.0106 - val_loss: 0.0095 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "1714/1714 - 29s - loss: 0.0105 - val_loss: 0.0095 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "1714/1714 - 29s - loss: 0.0105 - val_loss: 0.0094 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "1714/1714 - 29s - loss: 0.0105 - val_loss: 0.0094 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "1714/1714 - 29s - loss: 0.0104 - val_loss: 0.0093 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "1714/1714 - 30s - loss: 0.0104 - val_loss: 0.0093 - lr: 0.0100 - 30s/epoch - 18ms/step\n",
      "Epoch 92/2000\n",
      "1714/1714 - 29s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "1714/1714 - 29s - loss: 0.0103 - val_loss: 0.0093 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "1714/1714 - 30s - loss: 0.0102 - val_loss: 0.0092 - lr: 0.0100 - 30s/epoch - 18ms/step\n",
      "Epoch 95/2000\n",
      "1714/1714 - 31s - loss: 0.0102 - val_loss: 0.0092 - lr: 0.0100 - 31s/epoch - 18ms/step\n",
      "Epoch 96/2000\n",
      "1714/1714 - 29s - loss: 0.0102 - val_loss: 0.0092 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "1714/1714 - 29s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "1714/1714 - 29s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "1714/1714 - 29s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "1714/1714 - 29s - loss: 0.0100 - val_loss: 0.0090 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "1714/1714 - 29s - loss: 0.0100 - val_loss: 0.0090 - lr: 0.0100 - 29s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "1714/1714 - 29s - loss: 0.0100 - val_loss: 0.0089 - lr: 0.0020 - 29s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "1714/1714 - 29s - loss: 0.0100 - val_loss: 0.0089 - lr: 0.0020 - 29s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 0.0020 - 29s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 0.0020 - 29s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 4.0000e-04 - 29s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 4.0000e-04 - 29s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 4.0000e-04 - 29s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 8.0000e-05 - 29s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "1714/1714 - 30s - loss: 0.0099 - val_loss: 0.0089 - lr: 8.0000e-05 - 30s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 8.0000e-05 - 29s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.6000e-05 - 29s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.6000e-05 - 29s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.6000e-05 - 29s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 3.2000e-06 - 29s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 3.2000e-06 - 29s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "1714/1714 - 30s - loss: 0.0099 - val_loss: 0.0089 - lr: 3.2000e-06 - 30s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 6.4000e-07 - 29s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 6.4000e-07 - 29s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 6.4000e-07 - 29s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.2800e-07 - 29s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "1714/1714 - 30s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.2800e-07 - 30s/epoch - 18ms/step\n",
      "Epoch 123/2000\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.2800e-07 - 29s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 2.5600e-08 - 29s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "1714/1714 - 31s - loss: 0.0099 - val_loss: 0.0089 - lr: 2.5600e-08 - 31s/epoch - 18ms/step\n",
      "Epoch 126/2000\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "1714/1714 - 30s - loss: 0.0099 - val_loss: 0.0089 - lr: 2.5600e-08 - 30s/epoch - 18ms/step\n",
      "Epoch 127/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 5.1200e-09 - 29s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "1714/1714 - 30s - loss: 0.0099 - val_loss: 0.0089 - lr: 5.1200e-09 - 30s/epoch - 17ms/step\n",
      "Epoch 129/2000\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "1714/1714 - 30s - loss: 0.0099 - val_loss: 0.0089 - lr: 5.1200e-09 - 30s/epoch - 17ms/step\n",
      "Epoch 130/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.0240e-09 - 29s/epoch - 17ms/step\n",
      "Epoch 131/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.0240e-09 - 29s/epoch - 17ms/step\n",
      "Epoch 132/2000\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.0240e-09 - 29s/epoch - 17ms/step\n",
      "Epoch 133/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 2.0480e-10 - 29s/epoch - 17ms/step\n",
      "Epoch 134/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 2.0480e-10 - 29s/epoch - 17ms/step\n",
      "Epoch 135/2000\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 2.0480e-10 - 29s/epoch - 17ms/step\n",
      "Epoch 136/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 4.0960e-11 - 29s/epoch - 17ms/step\n",
      "Epoch 137/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 4.0960e-11 - 29s/epoch - 17ms/step\n",
      "Epoch 138/2000\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 4.0960e-11 - 29s/epoch - 17ms/step\n",
      "Epoch 139/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 8.1920e-12 - 29s/epoch - 17ms/step\n",
      "Epoch 140/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 8.1920e-12 - 29s/epoch - 17ms/step\n",
      "Epoch 141/2000\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 8.1920e-12 - 29s/epoch - 17ms/step\n",
      "Epoch 142/2000\n",
      "1714/1714 - 28s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.6384e-12 - 28s/epoch - 17ms/step\n",
      "Epoch 143/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.6384e-12 - 29s/epoch - 17ms/step\n",
      "Epoch 144/2000\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 1.6384e-12 - 29s/epoch - 17ms/step\n",
      "Epoch 145/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 3.2768e-13 - 29s/epoch - 17ms/step\n",
      "Epoch 146/2000\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 3.2768e-13 - 29s/epoch - 17ms/step\n",
      "Epoch 147/2000\n",
      "Restoring model weights from the end of the best epoch: 127.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 6.553599351567796e-14.\n",
      "1714/1714 - 29s - loss: 0.0099 - val_loss: 0.0089 - lr: 3.2768e-13 - 29s/epoch - 17ms/step\n",
      "Epoch 147: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_15_layer_call_fn, lstm_cell_15_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn, lstm_cell_16_layer_call_and_return_conditional_losses, lstm_cell_17_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerDiscretionary/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerDiscretionary/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1714/1714 [==============================] - 11s 7ms/step - loss: 0.0099\n",
      "429/429 [==============================] - 3s 7ms/step - loss: 0.0089\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.00992\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.00890\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "892/892 - 24s - loss: 0.2090 - val_loss: 0.1273 - lr: 0.0100 - 24s/epoch - 27ms/step\n",
      "Epoch 2/2000\n",
      "892/892 - 15s - loss: 0.1073 - val_loss: 0.0946 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "892/892 - 15s - loss: 0.0880 - val_loss: 0.0832 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "892/892 - 15s - loss: 0.0790 - val_loss: 0.0744 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "892/892 - 15s - loss: 0.0720 - val_loss: 0.0679 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "892/892 - 16s - loss: 0.0664 - val_loss: 0.0628 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "892/892 - 15s - loss: 0.0619 - val_loss: 0.0590 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "892/892 - 15s - loss: 0.0585 - val_loss: 0.0563 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "892/892 - 15s - loss: 0.0560 - val_loss: 0.0546 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "892/892 - 15s - loss: 0.0541 - val_loss: 0.0522 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "892/892 - 15s - loss: 0.0524 - val_loss: 0.0507 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "892/892 - 15s - loss: 0.0510 - val_loss: 0.0494 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "892/892 - 15s - loss: 0.0496 - val_loss: 0.0481 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "892/892 - 15s - loss: 0.0483 - val_loss: 0.0470 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "892/892 - 15s - loss: 0.0472 - val_loss: 0.0459 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "892/892 - 15s - loss: 0.0461 - val_loss: 0.0449 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "892/892 - 15s - loss: 0.0451 - val_loss: 0.0440 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "892/892 - 16s - loss: 0.0441 - val_loss: 0.0430 - lr: 0.0100 - 16s/epoch - 18ms/step\n",
      "Epoch 19/2000\n",
      "892/892 - 15s - loss: 0.0432 - val_loss: 0.0422 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "892/892 - 15s - loss: 0.0424 - val_loss: 0.0414 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "892/892 - 15s - loss: 0.0417 - val_loss: 0.0407 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "892/892 - 15s - loss: 0.0409 - val_loss: 0.0400 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "892/892 - 15s - loss: 0.0402 - val_loss: 0.0394 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "892/892 - 15s - loss: 0.0395 - val_loss: 0.0389 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "892/892 - 15s - loss: 0.0389 - val_loss: 0.0381 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "892/892 - 15s - loss: 0.0382 - val_loss: 0.0376 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "892/892 - 15s - loss: 0.0377 - val_loss: 0.0371 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "892/892 - 15s - loss: 0.0371 - val_loss: 0.0365 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "892/892 - 16s - loss: 0.0366 - val_loss: 0.0359 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "892/892 - 15s - loss: 0.0361 - val_loss: 0.0355 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "892/892 - 15s - loss: 0.0356 - val_loss: 0.0350 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "892/892 - 15s - loss: 0.0352 - val_loss: 0.0344 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "892/892 - 15s - loss: 0.0347 - val_loss: 0.0341 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "892/892 - 15s - loss: 0.0343 - val_loss: 0.0336 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "892/892 - 15s - loss: 0.0339 - val_loss: 0.0332 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "892/892 - 15s - loss: 0.0335 - val_loss: 0.0330 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "892/892 - 15s - loss: 0.0331 - val_loss: 0.0325 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "892/892 - 15s - loss: 0.0327 - val_loss: 0.0321 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "892/892 - 15s - loss: 0.0324 - val_loss: 0.0321 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "892/892 - 15s - loss: 0.0320 - val_loss: 0.0313 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "892/892 - 15s - loss: 0.0317 - val_loss: 0.0310 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "892/892 - 15s - loss: 0.0313 - val_loss: 0.0307 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "892/892 - 15s - loss: 0.0310 - val_loss: 0.0303 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "892/892 - 15s - loss: 0.0307 - val_loss: 0.0300 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "892/892 - 15s - loss: 0.0304 - val_loss: 0.0297 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "892/892 - 15s - loss: 0.0301 - val_loss: 0.0294 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "892/892 - 15s - loss: 0.0297 - val_loss: 0.0291 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "892/892 - 15s - loss: 0.0295 - val_loss: 0.0289 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "892/892 - 15s - loss: 0.0292 - val_loss: 0.0286 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "892/892 - 15s - loss: 0.0289 - val_loss: 0.0282 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "892/892 - 15s - loss: 0.0287 - val_loss: 0.0279 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "892/892 - 16s - loss: 0.0284 - val_loss: 0.0277 - lr: 0.0100 - 16s/epoch - 18ms/step\n",
      "Epoch 53/2000\n",
      "892/892 - 15s - loss: 0.0281 - val_loss: 0.0277 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "892/892 - 15s - loss: 0.0279 - val_loss: 0.0272 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "892/892 - 15s - loss: 0.0276 - val_loss: 0.0269 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "892/892 - 15s - loss: 0.0274 - val_loss: 0.0266 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "892/892 - 15s - loss: 0.0271 - val_loss: 0.0264 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "892/892 - 15s - loss: 0.0269 - val_loss: 0.0261 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "892/892 - 15s - loss: 0.0267 - val_loss: 0.0259 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "892/892 - 15s - loss: 0.0265 - val_loss: 0.0257 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "892/892 - 15s - loss: 0.0262 - val_loss: 0.0255 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "892/892 - 15s - loss: 0.0260 - val_loss: 0.0253 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "892/892 - 15s - loss: 0.0258 - val_loss: 0.0252 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "892/892 - 15s - loss: 0.0256 - val_loss: 0.0249 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "892/892 - 15s - loss: 0.0254 - val_loss: 0.0247 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "892/892 - 15s - loss: 0.0252 - val_loss: 0.0245 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "892/892 - 15s - loss: 0.0250 - val_loss: 0.0242 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "892/892 - 15s - loss: 0.0248 - val_loss: 0.0241 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "892/892 - 15s - loss: 0.0246 - val_loss: 0.0238 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "892/892 - 15s - loss: 0.0244 - val_loss: 0.0237 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "892/892 - 15s - loss: 0.0243 - val_loss: 0.0236 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "892/892 - 15s - loss: 0.0241 - val_loss: 0.0233 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "892/892 - 15s - loss: 0.0240 - val_loss: 0.0231 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "892/892 - 15s - loss: 0.0238 - val_loss: 0.0230 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "892/892 - 15s - loss: 0.0236 - val_loss: 0.0228 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "892/892 - 15s - loss: 0.0235 - val_loss: 0.0226 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "892/892 - 15s - loss: 0.0233 - val_loss: 0.0225 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "892/892 - 15s - loss: 0.0232 - val_loss: 0.0223 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "892/892 - 15s - loss: 0.0230 - val_loss: 0.0222 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 80/2000\n",
      "892/892 - 15s - loss: 0.0229 - val_loss: 0.0220 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "892/892 - 15s - loss: 0.0227 - val_loss: 0.0219 - lr: 0.0100 - 15s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/2000\n",
      "892/892 - 15s - loss: 0.0226 - val_loss: 0.0218 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "892/892 - 15s - loss: 0.0225 - val_loss: 0.0216 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "892/892 - 15s - loss: 0.0223 - val_loss: 0.0216 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "892/892 - 16s - loss: 0.0222 - val_loss: 0.0213 - lr: 0.0100 - 16s/epoch - 18ms/step\n",
      "Epoch 86/2000\n",
      "892/892 - 15s - loss: 0.0221 - val_loss: 0.0214 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "892/892 - 15s - loss: 0.0220 - val_loss: 0.0211 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "892/892 - 15s - loss: 0.0218 - val_loss: 0.0209 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "892/892 - 16s - loss: 0.0217 - val_loss: 0.0208 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "892/892 - 15s - loss: 0.0216 - val_loss: 0.0207 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "892/892 - 15s - loss: 0.0215 - val_loss: 0.0207 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "892/892 - 15s - loss: 0.0214 - val_loss: 0.0205 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "892/892 - 15s - loss: 0.0213 - val_loss: 0.0204 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "892/892 - 15s - loss: 0.0212 - val_loss: 0.0203 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "892/892 - 15s - loss: 0.0211 - val_loss: 0.0202 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "892/892 - 15s - loss: 0.0210 - val_loss: 0.0201 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "892/892 - 15s - loss: 0.0209 - val_loss: 0.0201 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "892/892 - 15s - loss: 0.0208 - val_loss: 0.0199 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "892/892 - 15s - loss: 0.0208 - val_loss: 0.0199 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "892/892 - 15s - loss: 0.0207 - val_loss: 0.0197 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "892/892 - 15s - loss: 0.0206 - val_loss: 0.0196 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "892/892 - 15s - loss: 0.0205 - val_loss: 0.0197 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "892/892 - 15s - loss: 0.0204 - val_loss: 0.0195 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "892/892 - 15s - loss: 0.0204 - val_loss: 0.0194 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "892/892 - 15s - loss: 0.0203 - val_loss: 0.0194 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "892/892 - 15s - loss: 0.0202 - val_loss: 0.0193 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "892/892 - 15s - loss: 0.0202 - val_loss: 0.0192 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "892/892 - 15s - loss: 0.0201 - val_loss: 0.0191 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "892/892 - 15s - loss: 0.0200 - val_loss: 0.0191 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "892/892 - 16s - loss: 0.0200 - val_loss: 0.0191 - lr: 0.0100 - 16s/epoch - 18ms/step\n",
      "Epoch 111/2000\n",
      "892/892 - 16s - loss: 0.0199 - val_loss: 0.0189 - lr: 0.0100 - 16s/epoch - 18ms/step\n",
      "Epoch 112/2000\n",
      "892/892 - 15s - loss: 0.0199 - val_loss: 0.0190 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "892/892 - 15s - loss: 0.0198 - val_loss: 0.0188 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "892/892 - 15s - loss: 0.0198 - val_loss: 0.0188 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "892/892 - 15s - loss: 0.0197 - val_loss: 0.0191 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "892/892 - 15s - loss: 0.0197 - val_loss: 0.0187 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "892/892 - 15s - loss: 0.0196 - val_loss: 0.0186 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "892/892 - 15s - loss: 0.0196 - val_loss: 0.0185 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "892/892 - 15s - loss: 0.0196 - val_loss: 0.0185 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "892/892 - 15s - loss: 0.0195 - val_loss: 0.0185 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "892/892 - 15s - loss: 0.0195 - val_loss: 0.0184 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "892/892 - 15s - loss: 0.0194 - val_loss: 0.0184 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "892/892 - 15s - loss: 0.0194 - val_loss: 0.0184 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "892/892 - 15s - loss: 0.0194 - val_loss: 0.0185 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "892/892 - 15s - loss: 0.0193 - val_loss: 0.0183 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "892/892 - 15s - loss: 0.0193 - val_loss: 0.0183 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 127/2000\n",
      "892/892 - 15s - loss: 0.0193 - val_loss: 0.0186 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "892/892 - 15s - loss: 0.0193 - val_loss: 0.0183 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 129/2000\n",
      "892/892 - 15s - loss: 0.0192 - val_loss: 0.0182 - lr: 0.0020 - 15s/epoch - 17ms/step\n",
      "Epoch 130/2000\n",
      "892/892 - 15s - loss: 0.0192 - val_loss: 0.0182 - lr: 0.0020 - 15s/epoch - 17ms/step\n",
      "Epoch 131/2000\n",
      "892/892 - 15s - loss: 0.0192 - val_loss: 0.0182 - lr: 0.0020 - 15s/epoch - 17ms/step\n",
      "Epoch 132/2000\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "892/892 - 15s - loss: 0.0192 - val_loss: 0.0181 - lr: 0.0020 - 15s/epoch - 17ms/step\n",
      "Epoch 133/2000\n",
      "892/892 - 15s - loss: 0.0192 - val_loss: 0.0181 - lr: 4.0000e-04 - 15s/epoch - 17ms/step\n",
      "Epoch 134/2000\n",
      "892/892 - 15s - loss: 0.0192 - val_loss: 0.0181 - lr: 4.0000e-04 - 15s/epoch - 17ms/step\n",
      "Epoch 135/2000\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "892/892 - 15s - loss: 0.0192 - val_loss: 0.0181 - lr: 4.0000e-04 - 15s/epoch - 17ms/step\n",
      "Epoch 136/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 8.0000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 137/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 8.0000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 138/2000\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 8.0000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 139/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.6000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 140/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.6000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 141/2000\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.6000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 142/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 3.2000e-06 - 15s/epoch - 17ms/step\n",
      "Epoch 143/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 3.2000e-06 - 15s/epoch - 17ms/step\n",
      "Epoch 144/2000\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 3.2000e-06 - 15s/epoch - 17ms/step\n",
      "Epoch 145/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 6.4000e-07 - 15s/epoch - 17ms/step\n",
      "Epoch 146/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 6.4000e-07 - 15s/epoch - 17ms/step\n",
      "Epoch 147/2000\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 6.4000e-07 - 15s/epoch - 17ms/step\n",
      "Epoch 148/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.2800e-07 - 15s/epoch - 17ms/step\n",
      "Epoch 149/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.2800e-07 - 15s/epoch - 17ms/step\n",
      "Epoch 150/2000\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.2800e-07 - 15s/epoch - 17ms/step\n",
      "Epoch 151/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 2.5600e-08 - 15s/epoch - 17ms/step\n",
      "Epoch 152/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 2.5600e-08 - 15s/epoch - 17ms/step\n",
      "Epoch 153/2000\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 2.5600e-08 - 15s/epoch - 17ms/step\n",
      "Epoch 154/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 5.1200e-09 - 15s/epoch - 17ms/step\n",
      "Epoch 155/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 5.1200e-09 - 15s/epoch - 17ms/step\n",
      "Epoch 156/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 5.1200e-09 - 15s/epoch - 17ms/step\n",
      "Epoch 157/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.0240e-09 - 15s/epoch - 17ms/step\n",
      "Epoch 158/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.0240e-09 - 15s/epoch - 17ms/step\n",
      "Epoch 159/2000\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 1.0240e-09 - 15s/epoch - 17ms/step\n",
      "Epoch 160/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 2.0480e-10 - 15s/epoch - 17ms/step\n",
      "Epoch 161/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 2.0480e-10 - 15s/epoch - 17ms/step\n",
      "Epoch 162/2000\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 2.0480e-10 - 15s/epoch - 17ms/step\n",
      "Epoch 163/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 4.0960e-11 - 15s/epoch - 17ms/step\n",
      "Epoch 164/2000\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 4.0960e-11 - 15s/epoch - 17ms/step\n",
      "Epoch 165/2000\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 4.0960e-11 - 15s/epoch - 17ms/step\n",
      "Epoch 166/2000\n",
      "Restoring model weights from the end of the best epoch: 146.\n",
      "892/892 - 15s - loss: 0.0191 - val_loss: 0.0181 - lr: 8.1920e-12 - 15s/epoch - 17ms/step\n",
      "Epoch 166: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_18_layer_call_fn, lstm_cell_18_layer_call_and_return_conditional_losses, lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Utilities/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Utilities/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892/892 [==============================] - 6s 7ms/step - loss: 0.0191\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.0181\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01915\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.01814\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2020/2020 - 44s - loss: 0.1029 - val_loss: 0.0589 - lr: 0.0100 - 44s/epoch - 22ms/step\n",
      "Epoch 2/2000\n",
      "2020/2020 - 34s - loss: 0.0504 - val_loss: 0.0483 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2020/2020 - 35s - loss: 0.0434 - val_loss: 0.0425 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2020/2020 - 35s - loss: 0.0383 - val_loss: 0.0381 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2020/2020 - 35s - loss: 0.0343 - val_loss: 0.0347 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2020/2020 - 35s - loss: 0.0314 - val_loss: 0.0322 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2020/2020 - 35s - loss: 0.0294 - val_loss: 0.0303 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2020/2020 - 35s - loss: 0.0279 - val_loss: 0.0290 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2020/2020 - 35s - loss: 0.0266 - val_loss: 0.0276 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2020/2020 - 34s - loss: 0.0255 - val_loss: 0.0266 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2020/2020 - 34s - loss: 0.0245 - val_loss: 0.0258 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2020/2020 - 35s - loss: 0.0237 - val_loss: 0.0247 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2020/2020 - 35s - loss: 0.0229 - val_loss: 0.0240 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2020/2020 - 34s - loss: 0.0222 - val_loss: 0.0233 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2020/2020 - 34s - loss: 0.0216 - val_loss: 0.0225 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2020/2020 - 34s - loss: 0.0210 - val_loss: 0.0220 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2020/2020 - 35s - loss: 0.0205 - val_loss: 0.0214 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2020/2020 - 34s - loss: 0.0200 - val_loss: 0.0212 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2020/2020 - 34s - loss: 0.0196 - val_loss: 0.0205 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2020/2020 - 35s - loss: 0.0192 - val_loss: 0.0201 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2020/2020 - 34s - loss: 0.0188 - val_loss: 0.0197 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2020/2020 - 36s - loss: 0.0184 - val_loss: 0.0193 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 23/2000\n",
      "2020/2020 - 35s - loss: 0.0181 - val_loss: 0.0191 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2020/2020 - 34s - loss: 0.0178 - val_loss: 0.0187 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2020/2020 - 33s - loss: 0.0175 - val_loss: 0.0184 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2020/2020 - 34s - loss: 0.0172 - val_loss: 0.0181 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2020/2020 - 35s - loss: 0.0169 - val_loss: 0.0178 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2020/2020 - 34s - loss: 0.0167 - val_loss: 0.0175 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2020/2020 - 33s - loss: 0.0164 - val_loss: 0.0173 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2020/2020 - 34s - loss: 0.0162 - val_loss: 0.0171 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2020/2020 - 34s - loss: 0.0160 - val_loss: 0.0169 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2020/2020 - 33s - loss: 0.0158 - val_loss: 0.0166 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2020/2020 - 34s - loss: 0.0156 - val_loss: 0.0164 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2020/2020 - 34s - loss: 0.0154 - val_loss: 0.0163 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2020/2020 - 34s - loss: 0.0152 - val_loss: 0.0161 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2020/2020 - 35s - loss: 0.0150 - val_loss: 0.0158 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2020/2020 - 35s - loss: 0.0148 - val_loss: 0.0156 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2020/2020 - 35s - loss: 0.0146 - val_loss: 0.0155 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2020/2020 - 35s - loss: 0.0145 - val_loss: 0.0153 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2020/2020 - 34s - loss: 0.0143 - val_loss: 0.0152 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2020/2020 - 35s - loss: 0.0141 - val_loss: 0.0150 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2020/2020 - 34s - loss: 0.0140 - val_loss: 0.0148 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2020/2020 - 34s - loss: 0.0138 - val_loss: 0.0147 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2020/2020 - 35s - loss: 0.0137 - val_loss: 0.0146 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2020/2020 - 35s - loss: 0.0135 - val_loss: 0.0145 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2020/2020 - 35s - loss: 0.0134 - val_loss: 0.0143 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2020/2020 - 34s - loss: 0.0133 - val_loss: 0.0141 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2020/2020 - 36s - loss: 0.0131 - val_loss: 0.0141 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 49/2000\n",
      "2020/2020 - 35s - loss: 0.0130 - val_loss: 0.0138 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2020/2020 - 35s - loss: 0.0129 - val_loss: 0.0137 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2020/2020 - 34s - loss: 0.0128 - val_loss: 0.0136 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2020/2020 - 34s - loss: 0.0127 - val_loss: 0.0135 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2020/2020 - 34s - loss: 0.0125 - val_loss: 0.0134 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2020/2020 - 34s - loss: 0.0124 - val_loss: 0.0132 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2020/2020 - 34s - loss: 0.0123 - val_loss: 0.0132 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2020/2020 - 34s - loss: 0.0122 - val_loss: 0.0130 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2020/2020 - 34s - loss: 0.0121 - val_loss: 0.0129 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2020/2020 - 34s - loss: 0.0120 - val_loss: 0.0128 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2020/2020 - 35s - loss: 0.0119 - val_loss: 0.0127 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2020/2020 - 34s - loss: 0.0118 - val_loss: 0.0126 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2020/2020 - 34s - loss: 0.0118 - val_loss: 0.0125 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2020/2020 - 34s - loss: 0.0117 - val_loss: 0.0125 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2020/2020 - 35s - loss: 0.0116 - val_loss: 0.0124 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2020/2020 - 35s - loss: 0.0115 - val_loss: 0.0123 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2020/2020 - 34s - loss: 0.0114 - val_loss: 0.0122 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2020/2020 - 34s - loss: 0.0114 - val_loss: 0.0121 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2020/2020 - 34s - loss: 0.0113 - val_loss: 0.0121 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2020/2020 - 33s - loss: 0.0112 - val_loss: 0.0120 - lr: 0.0100 - 33s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2020/2020 - 34s - loss: 0.0111 - val_loss: 0.0119 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2020/2020 - 34s - loss: 0.0111 - val_loss: 0.0119 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2020/2020 - 35s - loss: 0.0110 - val_loss: 0.0120 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2020/2020 - 34s - loss: 0.0110 - val_loss: 0.0117 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2020/2020 - 34s - loss: 0.0109 - val_loss: 0.0117 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2020/2020 - 34s - loss: 0.0108 - val_loss: 0.0116 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2020/2020 - 35s - loss: 0.0108 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2020/2020 - 35s - loss: 0.0107 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2020/2020 - 35s - loss: 0.0107 - val_loss: 0.0114 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2020/2020 - 35s - loss: 0.0106 - val_loss: 0.0114 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2020/2020 - 35s - loss: 0.0106 - val_loss: 0.0114 - lr: 0.0100 - 35s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2020/2020 - 35s - loss: 0.0105 - val_loss: 0.0113 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2020/2020 - 35s - loss: 0.0105 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2020/2020 - 34s - loss: 0.0105 - val_loss: 0.0112 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2020/2020 - 34s - loss: 0.0104 - val_loss: 0.0113 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2020/2020 - 34s - loss: 0.0104 - val_loss: 0.0112 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2020/2020 - 35s - loss: 0.0103 - val_loss: 0.0111 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2020/2020 - 34s - loss: 0.0103 - val_loss: 0.0111 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2020/2020 - 34s - loss: 0.0103 - val_loss: 0.0110 - lr: 0.0100 - 34s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0110 - lr: 0.0020 - 34s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0110 - lr: 0.0020 - 34s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0110 - lr: 0.0020 - 35s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0110 - lr: 0.0020 - 34s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 4.0000e-04 - 34s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 4.0000e-04 - 34s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 4.0000e-04 - 35s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 8.0000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 8.0000e-05 - 35s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 8.0000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.6000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.6000e-05 - 34s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.6000e-05 - 35s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 3.2000e-06 - 35s/epoch - 18ms/step\n",
      "Epoch 102/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 3.2000e-06 - 34s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 3.2000e-06 - 35s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 6.4000e-07 - 35s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 6.4000e-07 - 34s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 6.4000e-07 - 35s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.2800e-07 - 34s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2020/2020 - 33s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.2800e-07 - 33s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.2800e-07 - 34s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 2.5600e-08 - 34s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 2.5600e-08 - 34s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 2.5600e-08 - 34s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 5.1200e-09 - 34s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2020/2020 - 34s - loss: 0.0102 - val_loss: 0.0109 - lr: 5.1200e-09 - 34s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 5.1200e-09 - 35s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.0240e-09 - 35s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "2020/2020 - 35s - loss: 0.0102 - val_loss: 0.0109 - lr: 1.0240e-09 - 35s/epoch - 17ms/step\n",
      "Epoch 117: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn, lstm_cell_22_layer_call_and_return_conditional_losses, lstm_cell_23_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Financials/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Financials/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/2020 [==============================] - 14s 7ms/step - loss: 0.0102\n",
      "505/505 [==============================] - 3s 7ms/step - loss: 0.0109\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01019\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.01094\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "839/839 - 25s - loss: 0.1570 - val_loss: 0.0666 - lr: 0.0100 - 25s/epoch - 29ms/step\n",
      "Epoch 2/2000\n",
      "839/839 - 14s - loss: 0.0721 - val_loss: 0.0503 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "839/839 - 14s - loss: 0.0585 - val_loss: 0.0429 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "839/839 - 15s - loss: 0.0525 - val_loss: 0.0399 - lr: 0.0100 - 15s/epoch - 18ms/step\n",
      "Epoch 5/2000\n",
      "839/839 - 14s - loss: 0.0492 - val_loss: 0.0379 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "839/839 - 14s - loss: 0.0464 - val_loss: 0.0362 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "839/839 - 14s - loss: 0.0440 - val_loss: 0.0342 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "839/839 - 14s - loss: 0.0418 - val_loss: 0.0328 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "839/839 - 14s - loss: 0.0400 - val_loss: 0.0315 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "839/839 - 14s - loss: 0.0383 - val_loss: 0.0302 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "839/839 - 14s - loss: 0.0368 - val_loss: 0.0295 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "839/839 - 14s - loss: 0.0356 - val_loss: 0.0286 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "839/839 - 14s - loss: 0.0345 - val_loss: 0.0280 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "839/839 - 14s - loss: 0.0336 - val_loss: 0.0270 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "839/839 - 15s - loss: 0.0328 - val_loss: 0.0263 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "839/839 - 14s - loss: 0.0322 - val_loss: 0.0267 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "839/839 - 14s - loss: 0.0315 - val_loss: 0.0262 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "839/839 - 14s - loss: 0.0310 - val_loss: 0.0249 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "839/839 - 14s - loss: 0.0304 - val_loss: 0.0245 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "839/839 - 14s - loss: 0.0299 - val_loss: 0.0250 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "839/839 - 14s - loss: 0.0294 - val_loss: 0.0242 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "839/839 - 14s - loss: 0.0290 - val_loss: 0.0231 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "839/839 - 14s - loss: 0.0285 - val_loss: 0.0227 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "839/839 - 14s - loss: 0.0281 - val_loss: 0.0224 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "839/839 - 14s - loss: 0.0277 - val_loss: 0.0221 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "839/839 - 14s - loss: 0.0273 - val_loss: 0.0217 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "839/839 - 14s - loss: 0.0269 - val_loss: 0.0213 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "839/839 - 14s - loss: 0.0266 - val_loss: 0.0211 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "839/839 - 14s - loss: 0.0262 - val_loss: 0.0207 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "839/839 - 14s - loss: 0.0259 - val_loss: 0.0204 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "839/839 - 14s - loss: 0.0256 - val_loss: 0.0202 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "839/839 - 14s - loss: 0.0253 - val_loss: 0.0199 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "839/839 - 14s - loss: 0.0250 - val_loss: 0.0196 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "839/839 - 14s - loss: 0.0247 - val_loss: 0.0194 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "839/839 - 14s - loss: 0.0244 - val_loss: 0.0191 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "839/839 - 14s - loss: 0.0241 - val_loss: 0.0189 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "839/839 - 14s - loss: 0.0239 - val_loss: 0.0187 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "839/839 - 14s - loss: 0.0236 - val_loss: 0.0188 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "839/839 - 14s - loss: 0.0234 - val_loss: 0.0183 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "839/839 - 15s - loss: 0.0231 - val_loss: 0.0180 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "839/839 - 14s - loss: 0.0229 - val_loss: 0.0178 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "839/839 - 14s - loss: 0.0227 - val_loss: 0.0176 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "839/839 - 14s - loss: 0.0224 - val_loss: 0.0174 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "839/839 - 14s - loss: 0.0222 - val_loss: 0.0172 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "839/839 - 14s - loss: 0.0220 - val_loss: 0.0174 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "839/839 - 14s - loss: 0.0218 - val_loss: 0.0169 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "839/839 - 14s - loss: 0.0216 - val_loss: 0.0168 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "839/839 - 14s - loss: 0.0214 - val_loss: 0.0166 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "839/839 - 14s - loss: 0.0212 - val_loss: 0.0163 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "839/839 - 14s - loss: 0.0210 - val_loss: 0.0162 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "839/839 - 14s - loss: 0.0208 - val_loss: 0.0164 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "839/839 - 14s - loss: 0.0206 - val_loss: 0.0160 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "839/839 - 14s - loss: 0.0204 - val_loss: 0.0157 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "839/839 - 14s - loss: 0.0203 - val_loss: 0.0157 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "839/839 - 14s - loss: 0.0201 - val_loss: 0.0155 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "839/839 - 14s - loss: 0.0199 - val_loss: 0.0153 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "839/839 - 14s - loss: 0.0198 - val_loss: 0.0153 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "839/839 - 14s - loss: 0.0196 - val_loss: 0.0151 - lr: 0.0100 - 14s/epoch - 16ms/step\n",
      "Epoch 59/2000\n",
      "839/839 - 14s - loss: 0.0194 - val_loss: 0.0149 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "839/839 - 14s - loss: 0.0193 - val_loss: 0.0148 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "839/839 - 14s - loss: 0.0191 - val_loss: 0.0153 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "839/839 - 14s - loss: 0.0190 - val_loss: 0.0146 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "839/839 - 15s - loss: 0.0189 - val_loss: 0.0145 - lr: 0.0100 - 15s/epoch - 18ms/step\n",
      "Epoch 64/2000\n",
      "839/839 - 14s - loss: 0.0187 - val_loss: 0.0144 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "839/839 - 14s - loss: 0.0186 - val_loss: 0.0143 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "839/839 - 14s - loss: 0.0184 - val_loss: 0.0141 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "839/839 - 14s - loss: 0.0183 - val_loss: 0.0141 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "839/839 - 14s - loss: 0.0182 - val_loss: 0.0139 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "839/839 - 14s - loss: 0.0180 - val_loss: 0.0138 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "839/839 - 14s - loss: 0.0179 - val_loss: 0.0137 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "839/839 - 14s - loss: 0.0178 - val_loss: 0.0137 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "839/839 - 14s - loss: 0.0177 - val_loss: 0.0137 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "839/839 - 14s - loss: 0.0175 - val_loss: 0.0135 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "839/839 - 14s - loss: 0.0174 - val_loss: 0.0134 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "839/839 - 14s - loss: 0.0173 - val_loss: 0.0134 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "839/839 - 14s - loss: 0.0172 - val_loss: 0.0133 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "839/839 - 14s - loss: 0.0171 - val_loss: 0.0131 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "839/839 - 15s - loss: 0.0169 - val_loss: 0.0130 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "839/839 - 14s - loss: 0.0168 - val_loss: 0.0129 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 80/2000\n",
      "839/839 - 14s - loss: 0.0167 - val_loss: 0.0129 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "839/839 - 14s - loss: 0.0166 - val_loss: 0.0128 - lr: 0.0100 - 14s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/2000\n",
      "839/839 - 14s - loss: 0.0165 - val_loss: 0.0127 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "839/839 - 14s - loss: 0.0164 - val_loss: 0.0127 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "839/839 - 14s - loss: 0.0163 - val_loss: 0.0126 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "839/839 - 14s - loss: 0.0162 - val_loss: 0.0125 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "839/839 - 14s - loss: 0.0161 - val_loss: 0.0124 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "839/839 - 15s - loss: 0.0160 - val_loss: 0.0124 - lr: 0.0100 - 15s/epoch - 18ms/step\n",
      "Epoch 88/2000\n",
      "839/839 - 14s - loss: 0.0159 - val_loss: 0.0126 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "839/839 - 14s - loss: 0.0158 - val_loss: 0.0122 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "839/839 - 14s - loss: 0.0157 - val_loss: 0.0122 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "839/839 - 14s - loss: 0.0156 - val_loss: 0.0121 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "839/839 - 14s - loss: 0.0156 - val_loss: 0.0120 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "839/839 - 14s - loss: 0.0155 - val_loss: 0.0120 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "839/839 - 14s - loss: 0.0154 - val_loss: 0.0119 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "839/839 - 14s - loss: 0.0153 - val_loss: 0.0118 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "839/839 - 14s - loss: 0.0152 - val_loss: 0.0120 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "839/839 - 14s - loss: 0.0151 - val_loss: 0.0117 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "839/839 - 14s - loss: 0.0151 - val_loss: 0.0116 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "839/839 - 15s - loss: 0.0150 - val_loss: 0.0118 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "839/839 - 14s - loss: 0.0149 - val_loss: 0.0116 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "839/839 - 15s - loss: 0.0148 - val_loss: 0.0115 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "839/839 - 14s - loss: 0.0147 - val_loss: 0.0114 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "839/839 - 15s - loss: 0.0147 - val_loss: 0.0114 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "839/839 - 14s - loss: 0.0146 - val_loss: 0.0114 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "839/839 - 14s - loss: 0.0145 - val_loss: 0.0113 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "839/839 - 14s - loss: 0.0144 - val_loss: 0.0113 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "839/839 - 15s - loss: 0.0144 - val_loss: 0.0112 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "839/839 - 14s - loss: 0.0143 - val_loss: 0.0111 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "839/839 - 14s - loss: 0.0142 - val_loss: 0.0111 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "839/839 - 14s - loss: 0.0142 - val_loss: 0.0110 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "839/839 - 14s - loss: 0.0141 - val_loss: 0.0110 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "839/839 - 14s - loss: 0.0140 - val_loss: 0.0110 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "839/839 - 14s - loss: 0.0140 - val_loss: 0.0110 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "839/839 - 14s - loss: 0.0139 - val_loss: 0.0108 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "839/839 - 14s - loss: 0.0138 - val_loss: 0.0109 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "839/839 - 14s - loss: 0.0138 - val_loss: 0.0107 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "839/839 - 14s - loss: 0.0137 - val_loss: 0.0107 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "839/839 - 14s - loss: 0.0136 - val_loss: 0.0106 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "839/839 - 14s - loss: 0.0136 - val_loss: 0.0106 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "839/839 - 14s - loss: 0.0135 - val_loss: 0.0106 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "839/839 - 14s - loss: 0.0135 - val_loss: 0.0105 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "839/839 - 14s - loss: 0.0134 - val_loss: 0.0105 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "839/839 - 14s - loss: 0.0134 - val_loss: 0.0105 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "839/839 - 14s - loss: 0.0133 - val_loss: 0.0104 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "839/839 - 15s - loss: 0.0132 - val_loss: 0.0104 - lr: 0.0100 - 15s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "839/839 - 15s - loss: 0.0132 - val_loss: 0.0103 - lr: 0.0100 - 15s/epoch - 18ms/step\n",
      "Epoch 127/2000\n",
      "839/839 - 14s - loss: 0.0131 - val_loss: 0.0103 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "839/839 - 14s - loss: 0.0131 - val_loss: 0.0103 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 129/2000\n",
      "839/839 - 14s - loss: 0.0130 - val_loss: 0.0102 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 130/2000\n",
      "839/839 - 14s - loss: 0.0130 - val_loss: 0.0102 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 131/2000\n",
      "839/839 - 14s - loss: 0.0129 - val_loss: 0.0102 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 132/2000\n",
      "839/839 - 14s - loss: 0.0129 - val_loss: 0.0103 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 133/2000\n",
      "839/839 - 14s - loss: 0.0129 - val_loss: 0.0102 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 134/2000\n",
      "839/839 - 14s - loss: 0.0128 - val_loss: 0.0101 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 135/2000\n",
      "839/839 - 14s - loss: 0.0127 - val_loss: 0.0100 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 136/2000\n",
      "839/839 - 14s - loss: 0.0127 - val_loss: 0.0100 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 137/2000\n",
      "839/839 - 14s - loss: 0.0127 - val_loss: 0.0100 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 138/2000\n",
      "839/839 - 14s - loss: 0.0126 - val_loss: 0.0100 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 139/2000\n",
      "839/839 - 14s - loss: 0.0126 - val_loss: 0.0099 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 140/2000\n",
      "839/839 - 14s - loss: 0.0125 - val_loss: 0.0099 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 141/2000\n",
      "839/839 - 14s - loss: 0.0125 - val_loss: 0.0098 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 142/2000\n",
      "839/839 - 14s - loss: 0.0125 - val_loss: 0.0098 - lr: 0.0100 - 14s/epoch - 16ms/step\n",
      "Epoch 143/2000\n",
      "839/839 - 14s - loss: 0.0124 - val_loss: 0.0099 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 144/2000\n",
      "839/839 - 14s - loss: 0.0124 - val_loss: 0.0098 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 145/2000\n",
      "839/839 - 14s - loss: 0.0123 - val_loss: 0.0097 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 146/2000\n",
      "839/839 - 14s - loss: 0.0123 - val_loss: 0.0097 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 147/2000\n",
      "839/839 - 14s - loss: 0.0123 - val_loss: 0.0097 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 148/2000\n",
      "839/839 - 14s - loss: 0.0122 - val_loss: 0.0097 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 149/2000\n",
      "839/839 - 14s - loss: 0.0122 - val_loss: 0.0096 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 150/2000\n",
      "839/839 - 14s - loss: 0.0122 - val_loss: 0.0097 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 151/2000\n",
      "839/839 - 14s - loss: 0.0121 - val_loss: 0.0100 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 152/2000\n",
      "839/839 - 14s - loss: 0.0121 - val_loss: 0.0096 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 153/2000\n",
      "839/839 - 14s - loss: 0.0120 - val_loss: 0.0097 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 154/2000\n",
      "839/839 - 14s - loss: 0.0120 - val_loss: 0.0096 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 155/2000\n",
      "839/839 - 14s - loss: 0.0120 - val_loss: 0.0095 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 156/2000\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0096 - lr: 0.0100 - 14s/epoch - 17ms/step\n",
      "Epoch 157/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0095 - lr: 0.0020 - 14s/epoch - 17ms/step\n",
      "Epoch 158/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 0.0020 - 14s/epoch - 17ms/step\n",
      "Epoch 159/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 0.0020 - 14s/epoch - 17ms/step\n",
      "Epoch 160/2000\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 0.0020 - 14s/epoch - 17ms/step\n",
      "Epoch 161/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 4.0000e-04 - 14s/epoch - 17ms/step\n",
      "Epoch 162/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 4.0000e-04 - 14s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/2000\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 4.0000e-04 - 14s/epoch - 17ms/step\n",
      "Epoch 164/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 8.0000e-05 - 14s/epoch - 17ms/step\n",
      "Epoch 165/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 8.0000e-05 - 14s/epoch - 17ms/step\n",
      "Epoch 166/2000\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 8.0000e-05 - 15s/epoch - 18ms/step\n",
      "Epoch 167/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.6000e-05 - 14s/epoch - 17ms/step\n",
      "Epoch 168/2000\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.6000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 169/2000\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.6000e-05 - 15s/epoch - 17ms/step\n",
      "Epoch 170/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 3.2000e-06 - 14s/epoch - 17ms/step\n",
      "Epoch 171/2000\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 3.2000e-06 - 15s/epoch - 17ms/step\n",
      "Epoch 172/2000\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 3.2000e-06 - 14s/epoch - 17ms/step\n",
      "Epoch 173/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 6.4000e-07 - 14s/epoch - 17ms/step\n",
      "Epoch 174/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 6.4000e-07 - 14s/epoch - 17ms/step\n",
      "Epoch 175/2000\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 6.4000e-07 - 14s/epoch - 17ms/step\n",
      "Epoch 176/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.2800e-07 - 14s/epoch - 17ms/step\n",
      "Epoch 177/2000\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.2800e-07 - 15s/epoch - 17ms/step\n",
      "Epoch 178/2000\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.2800e-07 - 14s/epoch - 17ms/step\n",
      "Epoch 179/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 2.5600e-08 - 14s/epoch - 17ms/step\n",
      "Epoch 180/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 2.5600e-08 - 14s/epoch - 17ms/step\n",
      "Epoch 181/2000\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 2.5600e-08 - 14s/epoch - 17ms/step\n",
      "Epoch 182/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 5.1200e-09 - 14s/epoch - 17ms/step\n",
      "Epoch 183/2000\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 5.1200e-09 - 15s/epoch - 17ms/step\n",
      "Epoch 184/2000\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 5.1200e-09 - 14s/epoch - 17ms/step\n",
      "Epoch 185/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.0240e-09 - 14s/epoch - 17ms/step\n",
      "Epoch 186/2000\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.0240e-09 - 14s/epoch - 17ms/step\n",
      "Epoch 187/2000\n",
      "\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 1.0240e-09 - 14s/epoch - 17ms/step\n",
      "Epoch 188/2000\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 2.0480e-10 - 15s/epoch - 17ms/step\n",
      "Epoch 189/2000\n",
      "839/839 - 15s - loss: 0.0119 - val_loss: 0.0094 - lr: 2.0480e-10 - 15s/epoch - 18ms/step\n",
      "Epoch 190/2000\n",
      "\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 2.0480e-10 - 14s/epoch - 17ms/step\n",
      "Epoch 191/2000\n",
      "Restoring model weights from the end of the best epoch: 171.\n",
      "839/839 - 14s - loss: 0.0119 - val_loss: 0.0094 - lr: 4.0960e-11 - 14s/epoch - 17ms/step\n",
      "Epoch 191: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_24_layer_call_fn, lstm_cell_24_layer_call_and_return_conditional_losses, lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Materials/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Materials/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839/839 [==============================] - 6s 7ms/step - loss: 0.0119\n",
      "210/210 [==============================] - 1s 7ms/step - loss: 0.0094\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01186\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.00942\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "948/948 - 24s - loss: 0.1515 - val_loss: 0.0929 - lr: 0.0100 - 24s/epoch - 25ms/step\n",
      "Epoch 2/2000\n",
      "948/948 - 17s - loss: 0.0656 - val_loss: 0.0692 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "948/948 - 16s - loss: 0.0535 - val_loss: 0.0589 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "948/948 - 16s - loss: 0.0490 - val_loss: 0.0543 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "948/948 - 16s - loss: 0.0460 - val_loss: 0.0509 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "948/948 - 16s - loss: 0.0434 - val_loss: 0.0481 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "948/948 - 16s - loss: 0.0411 - val_loss: 0.0455 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "948/948 - 16s - loss: 0.0391 - val_loss: 0.0431 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "948/948 - 16s - loss: 0.0373 - val_loss: 0.0414 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "948/948 - 16s - loss: 0.0357 - val_loss: 0.0394 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "948/948 - 16s - loss: 0.0343 - val_loss: 0.0380 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "948/948 - 16s - loss: 0.0332 - val_loss: 0.0368 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "948/948 - 16s - loss: 0.0322 - val_loss: 0.0356 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "948/948 - 16s - loss: 0.0313 - val_loss: 0.0348 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "948/948 - 16s - loss: 0.0306 - val_loss: 0.0342 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "948/948 - 16s - loss: 0.0299 - val_loss: 0.0332 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "948/948 - 16s - loss: 0.0293 - val_loss: 0.0327 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "948/948 - 16s - loss: 0.0287 - val_loss: 0.0320 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "948/948 - 16s - loss: 0.0282 - val_loss: 0.0313 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "948/948 - 16s - loss: 0.0277 - val_loss: 0.0308 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "948/948 - 16s - loss: 0.0272 - val_loss: 0.0303 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "948/948 - 16s - loss: 0.0267 - val_loss: 0.0301 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "948/948 - 16s - loss: 0.0263 - val_loss: 0.0294 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "948/948 - 16s - loss: 0.0259 - val_loss: 0.0289 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "948/948 - 16s - loss: 0.0255 - val_loss: 0.0285 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "948/948 - 16s - loss: 0.0251 - val_loss: 0.0281 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "948/948 - 16s - loss: 0.0247 - val_loss: 0.0278 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "948/948 - 16s - loss: 0.0243 - val_loss: 0.0273 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "948/948 - 16s - loss: 0.0240 - val_loss: 0.0268 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "948/948 - 16s - loss: 0.0237 - val_loss: 0.0265 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "948/948 - 16s - loss: 0.0233 - val_loss: 0.0262 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "948/948 - 16s - loss: 0.0230 - val_loss: 0.0258 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "948/948 - 16s - loss: 0.0227 - val_loss: 0.0255 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "948/948 - 16s - loss: 0.0224 - val_loss: 0.0253 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "948/948 - 16s - loss: 0.0221 - val_loss: 0.0248 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "948/948 - 16s - loss: 0.0219 - val_loss: 0.0246 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "948/948 - 16s - loss: 0.0216 - val_loss: 0.0243 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "948/948 - 16s - loss: 0.0214 - val_loss: 0.0240 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "948/948 - 16s - loss: 0.0211 - val_loss: 0.0237 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "948/948 - 16s - loss: 0.0209 - val_loss: 0.0234 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "948/948 - 16s - loss: 0.0206 - val_loss: 0.0233 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "948/948 - 16s - loss: 0.0204 - val_loss: 0.0233 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "948/948 - 17s - loss: 0.0202 - val_loss: 0.0227 - lr: 0.0100 - 17s/epoch - 18ms/step\n",
      "Epoch 44/2000\n",
      "948/948 - 16s - loss: 0.0200 - val_loss: 0.0224 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "948/948 - 16s - loss: 0.0198 - val_loss: 0.0223 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "948/948 - 16s - loss: 0.0196 - val_loss: 0.0220 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "948/948 - 17s - loss: 0.0194 - val_loss: 0.0217 - lr: 0.0100 - 17s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "948/948 - 16s - loss: 0.0192 - val_loss: 0.0215 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "948/948 - 16s - loss: 0.0190 - val_loss: 0.0213 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "948/948 - 16s - loss: 0.0188 - val_loss: 0.0212 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "948/948 - 16s - loss: 0.0186 - val_loss: 0.0209 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "948/948 - 16s - loss: 0.0185 - val_loss: 0.0206 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "948/948 - 16s - loss: 0.0183 - val_loss: 0.0205 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "948/948 - 16s - loss: 0.0181 - val_loss: 0.0203 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "948/948 - 16s - loss: 0.0180 - val_loss: 0.0201 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "948/948 - 16s - loss: 0.0178 - val_loss: 0.0199 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "948/948 - 16s - loss: 0.0177 - val_loss: 0.0197 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "948/948 - 16s - loss: 0.0175 - val_loss: 0.0196 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "948/948 - 16s - loss: 0.0174 - val_loss: 0.0194 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "948/948 - 16s - loss: 0.0172 - val_loss: 0.0192 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "948/948 - 16s - loss: 0.0171 - val_loss: 0.0191 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "948/948 - 16s - loss: 0.0170 - val_loss: 0.0189 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "948/948 - 16s - loss: 0.0168 - val_loss: 0.0188 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "948/948 - 16s - loss: 0.0167 - val_loss: 0.0185 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "948/948 - 16s - loss: 0.0166 - val_loss: 0.0184 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "948/948 - 16s - loss: 0.0164 - val_loss: 0.0183 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "948/948 - 16s - loss: 0.0163 - val_loss: 0.0181 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "948/948 - 16s - loss: 0.0162 - val_loss: 0.0179 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "948/948 - 16s - loss: 0.0161 - val_loss: 0.0178 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "948/948 - 16s - loss: 0.0160 - val_loss: 0.0177 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "948/948 - 16s - loss: 0.0158 - val_loss: 0.0175 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "948/948 - 16s - loss: 0.0157 - val_loss: 0.0174 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "948/948 - 16s - loss: 0.0156 - val_loss: 0.0173 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "948/948 - 16s - loss: 0.0155 - val_loss: 0.0171 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "948/948 - 16s - loss: 0.0154 - val_loss: 0.0170 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "948/948 - 16s - loss: 0.0153 - val_loss: 0.0169 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "948/948 - 16s - loss: 0.0152 - val_loss: 0.0168 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "948/948 - 16s - loss: 0.0151 - val_loss: 0.0166 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "948/948 - 16s - loss: 0.0150 - val_loss: 0.0165 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 80/2000\n",
      "948/948 - 16s - loss: 0.0149 - val_loss: 0.0164 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "948/948 - 16s - loss: 0.0148 - val_loss: 0.0164 - lr: 0.0100 - 16s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/2000\n",
      "948/948 - 16s - loss: 0.0147 - val_loss: 0.0162 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "948/948 - 16s - loss: 0.0146 - val_loss: 0.0161 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "948/948 - 16s - loss: 0.0145 - val_loss: 0.0160 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "948/948 - 16s - loss: 0.0144 - val_loss: 0.0159 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "948/948 - 16s - loss: 0.0143 - val_loss: 0.0157 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "948/948 - 16s - loss: 0.0143 - val_loss: 0.0157 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "948/948 - 16s - loss: 0.0142 - val_loss: 0.0155 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "948/948 - 16s - loss: 0.0141 - val_loss: 0.0154 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "948/948 - 16s - loss: 0.0140 - val_loss: 0.0153 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "948/948 - 16s - loss: 0.0139 - val_loss: 0.0152 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "948/948 - 16s - loss: 0.0138 - val_loss: 0.0151 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "948/948 - 16s - loss: 0.0138 - val_loss: 0.0151 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "948/948 - 16s - loss: 0.0137 - val_loss: 0.0149 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "948/948 - 16s - loss: 0.0136 - val_loss: 0.0148 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "948/948 - 16s - loss: 0.0135 - val_loss: 0.0147 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "948/948 - 16s - loss: 0.0135 - val_loss: 0.0147 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "948/948 - 16s - loss: 0.0134 - val_loss: 0.0146 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "948/948 - 17s - loss: 0.0133 - val_loss: 0.0145 - lr: 0.0100 - 17s/epoch - 18ms/step\n",
      "Epoch 100/2000\n",
      "948/948 - 16s - loss: 0.0132 - val_loss: 0.0144 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "948/948 - 16s - loss: 0.0132 - val_loss: 0.0143 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "948/948 - 16s - loss: 0.0131 - val_loss: 0.0143 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "948/948 - 16s - loss: 0.0130 - val_loss: 0.0142 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "948/948 - 16s - loss: 0.0130 - val_loss: 0.0141 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "948/948 - 16s - loss: 0.0129 - val_loss: 0.0140 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "948/948 - 16s - loss: 0.0128 - val_loss: 0.0139 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "948/948 - 16s - loss: 0.0128 - val_loss: 0.0138 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "948/948 - 16s - loss: 0.0127 - val_loss: 0.0138 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "948/948 - 16s - loss: 0.0126 - val_loss: 0.0137 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "948/948 - 15s - loss: 0.0126 - val_loss: 0.0136 - lr: 0.0100 - 15s/epoch - 16ms/step\n",
      "Epoch 111/2000\n",
      "948/948 - 16s - loss: 0.0125 - val_loss: 0.0135 - lr: 0.0100 - 16s/epoch - 16ms/step\n",
      "Epoch 112/2000\n",
      "948/948 - 16s - loss: 0.0125 - val_loss: 0.0136 - lr: 0.0100 - 16s/epoch - 16ms/step\n",
      "Epoch 113/2000\n",
      "948/948 - 16s - loss: 0.0124 - val_loss: 0.0134 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "948/948 - 16s - loss: 0.0124 - val_loss: 0.0134 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "948/948 - 16s - loss: 0.0123 - val_loss: 0.0133 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "948/948 - 16s - loss: 0.0122 - val_loss: 0.0132 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "948/948 - 16s - loss: 0.0122 - val_loss: 0.0132 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "948/948 - 16s - loss: 0.0121 - val_loss: 0.0131 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "948/948 - 16s - loss: 0.0121 - val_loss: 0.0130 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "948/948 - 16s - loss: 0.0120 - val_loss: 0.0130 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "948/948 - 16s - loss: 0.0120 - val_loss: 0.0130 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "948/948 - 16s - loss: 0.0119 - val_loss: 0.0129 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "948/948 - 16s - loss: 0.0119 - val_loss: 0.0129 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "948/948 - 16s - loss: 0.0118 - val_loss: 0.0128 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "948/948 - 16s - loss: 0.0118 - val_loss: 0.0127 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "948/948 - 16s - loss: 0.0118 - val_loss: 0.0127 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 127/2000\n",
      "948/948 - 16s - loss: 0.0117 - val_loss: 0.0126 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "948/948 - 16s - loss: 0.0117 - val_loss: 0.0126 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 129/2000\n",
      "948/948 - 16s - loss: 0.0116 - val_loss: 0.0125 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 130/2000\n",
      "948/948 - 16s - loss: 0.0116 - val_loss: 0.0125 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 131/2000\n",
      "948/948 - 16s - loss: 0.0115 - val_loss: 0.0124 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 132/2000\n",
      "948/948 - 16s - loss: 0.0115 - val_loss: 0.0123 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 133/2000\n",
      "948/948 - 16s - loss: 0.0115 - val_loss: 0.0123 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 134/2000\n",
      "948/948 - 16s - loss: 0.0114 - val_loss: 0.0123 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 135/2000\n",
      "948/948 - 16s - loss: 0.0114 - val_loss: 0.0123 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 136/2000\n",
      "948/948 - 16s - loss: 0.0113 - val_loss: 0.0122 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 137/2000\n",
      "948/948 - 16s - loss: 0.0113 - val_loss: 0.0122 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 138/2000\n",
      "948/948 - 16s - loss: 0.0113 - val_loss: 0.0121 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 139/2000\n",
      "948/948 - 16s - loss: 0.0112 - val_loss: 0.0120 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 140/2000\n",
      "948/948 - 16s - loss: 0.0112 - val_loss: 0.0120 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 141/2000\n",
      "948/948 - 16s - loss: 0.0112 - val_loss: 0.0120 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 142/2000\n",
      "948/948 - 16s - loss: 0.0111 - val_loss: 0.0119 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 143/2000\n",
      "948/948 - 16s - loss: 0.0111 - val_loss: 0.0119 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 144/2000\n",
      "948/948 - 16s - loss: 0.0111 - val_loss: 0.0119 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 145/2000\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "948/948 - 16s - loss: 0.0111 - val_loss: 0.0118 - lr: 0.0100 - 16s/epoch - 17ms/step\n",
      "Epoch 146/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 0.0020 - 16s/epoch - 17ms/step\n",
      "Epoch 147/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 0.0020 - 16s/epoch - 17ms/step\n",
      "Epoch 148/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 0.0020 - 16s/epoch - 17ms/step\n",
      "Epoch 149/2000\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 0.0020 - 16s/epoch - 17ms/step\n",
      "Epoch 150/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 4.0000e-04 - 16s/epoch - 17ms/step\n",
      "Epoch 151/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 4.0000e-04 - 16s/epoch - 17ms/step\n",
      "Epoch 152/2000\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 4.0000e-04 - 16s/epoch - 17ms/step\n",
      "Epoch 153/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 8.0000e-05 - 16s/epoch - 17ms/step\n",
      "Epoch 154/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 8.0000e-05 - 16s/epoch - 17ms/step\n",
      "Epoch 155/2000\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "948/948 - 17s - loss: 0.0110 - val_loss: 0.0118 - lr: 8.0000e-05 - 17s/epoch - 18ms/step\n",
      "Epoch 156/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.6000e-05 - 16s/epoch - 17ms/step\n",
      "Epoch 157/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.6000e-05 - 16s/epoch - 17ms/step\n",
      "Epoch 158/2000\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.6000e-05 - 16s/epoch - 17ms/step\n",
      "Epoch 159/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 3.2000e-06 - 16s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 3.2000e-06 - 16s/epoch - 17ms/step\n",
      "Epoch 161/2000\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 3.2000e-06 - 16s/epoch - 17ms/step\n",
      "Epoch 162/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 6.4000e-07 - 16s/epoch - 17ms/step\n",
      "Epoch 163/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 6.4000e-07 - 16s/epoch - 17ms/step\n",
      "Epoch 164/2000\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 6.4000e-07 - 16s/epoch - 17ms/step\n",
      "Epoch 165/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.2800e-07 - 16s/epoch - 17ms/step\n",
      "Epoch 166/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.2800e-07 - 16s/epoch - 17ms/step\n",
      "Epoch 167/2000\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.2800e-07 - 16s/epoch - 17ms/step\n",
      "Epoch 168/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 2.5600e-08 - 16s/epoch - 17ms/step\n",
      "Epoch 169/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 2.5600e-08 - 16s/epoch - 17ms/step\n",
      "Epoch 170/2000\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 2.5600e-08 - 16s/epoch - 17ms/step\n",
      "Epoch 171/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 5.1200e-09 - 16s/epoch - 17ms/step\n",
      "Epoch 172/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 5.1200e-09 - 16s/epoch - 17ms/step\n",
      "Epoch 173/2000\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 5.1200e-09 - 16s/epoch - 17ms/step\n",
      "Epoch 174/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.0240e-09 - 16s/epoch - 17ms/step\n",
      "Epoch 175/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.0240e-09 - 16s/epoch - 17ms/step\n",
      "Epoch 176/2000\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 1.0240e-09 - 16s/epoch - 17ms/step\n",
      "Epoch 177/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 2.0480e-10 - 16s/epoch - 17ms/step\n",
      "Epoch 178/2000\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 2.0480e-10 - 16s/epoch - 17ms/step\n",
      "Epoch 179/2000\n",
      "Restoring model weights from the end of the best epoch: 159.\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "948/948 - 16s - loss: 0.0110 - val_loss: 0.0118 - lr: 2.0480e-10 - 16s/epoch - 16ms/step\n",
      "Epoch 179: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_27_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn, lstm_cell_28_layer_call_and_return_conditional_losses, lstm_cell_29_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/RealEstate/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/RealEstate/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 6s 7ms/step - loss: 0.0110\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 0.0118\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01097\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.01177\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for nameSetor, setor in setores.items():\n",
    "\n",
    "    if trainModel:\n",
    "        X_treino, X_teste, y_treino, y_teste = preprocessingdata(steps, SP500_close, setor)\n",
    "        model = create_model()\n",
    "\n",
    "        model.fit(x = X_treino,\n",
    "                  y = y_treino,\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  verbose = verbose,\n",
    "                  validation_data = (X_teste, y_teste),\n",
    "                  callbacks = callbacks)\n",
    "        \n",
    "        #---------Save Model---------------------------\n",
    "        model.save('saveModel/{}/'.format(nameSetor),\n",
    "                   overwrite=True,\n",
    "                   include_optimizer=True,\n",
    "                   save_format = 'tf')\n",
    "        #----------------------------------------------\n",
    "\n",
    "        scoreTrain = model.evaluate(X_treino, y_treino)\n",
    "        scoreTest = model.evaluate(X_teste, y_teste)\n",
    "\n",
    "        print('\\n\\nErro quadrático médio em dados de treinamento: {:.5f}\\n\\nErro quadrático médio em dados de teste: {:.5f}\\n\\n'\\\n",
    "            .format(scoreTrain, scoreTest))\n",
    "\n",
    "        fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model)\n",
    "        \n",
    "    else:\n",
    "        model = load_model('saveModel/{}/'.format(nameSetor))\n",
    "        fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva tabela de previsão\n",
    "\n",
    "outdirP = './previsao/{}'.format(datetime.now().strftime('%d-%B-%Ih%Mmin'))\n",
    "\n",
    "if not os.path.exists(outdirP):\n",
    "    os.mkdir(outdirP)\n",
    "\n",
    "fullnameP = os.path.join(outdirP, 'previsao.csv')\n",
    "\n",
    "tablePrevision.to_csv(fullnameP, index = True, decimal = '.', sep=',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva tabela Entregável do Log-Retorno no padrão\n",
    "\n",
    "outdirLR = './logRetorno/{}'.format(datetime.now().strftime('%d-%B-%Ih%Mmin'))\n",
    "\n",
    "if not os.path.exists(outdirLR):\n",
    "    os.mkdir(outdirLR)\n",
    "\n",
    "fullnameLR = os.path.join(outdirLR, 'logRetorno.csv')\n",
    "\n",
    "tableLogRet.iloc[-len(forecast):, :].to_csv(fullnameLR, index = False, decimal = '.', sep=',')       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
