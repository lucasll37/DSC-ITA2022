{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Challenge @ ITA 2022</font>\n",
    "# <font color='blue'>Equipe DIOMGIS</font>\n",
    "\n",
    "## <font color='blue'>1º Fase</font>\n",
    "\n",
    "### <font color='blue'>Predição de pregões futuros de ativos que compõem o índice SP500.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data\\image\\logo.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas e Frameworks\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorboard import notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Equipe DIOMGIS\n",
      "\n",
      "seaborn          : 0.11.2\n",
      "pandas           : 1.4.2\n",
      "tensorboard      : 2.10.0\n",
      "pandas_datareader: 0.10.0\n",
      "numpy            : 1.22.3\n",
      "matplotlib       : 3.5.1\n",
      "tensorflow       : 2.10.0\n",
      "keras            : 2.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Equipe DIOMGIS\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "plt.rcParams['figure.figsize'] = (15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Confirma se o TensorFlow pode acessar a GPU\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 17 21:02:45 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.94       Driver Version: 516.94       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:65:00.0  On |                  N/A |\n",
      "|  0%   43C    P2    30W / 220W |   2540MiB /  8192MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       732    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      2592    C+G   ...txyewy\\MiniSearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3516    C+G   ...o Webcam\\GoPro Webcam.exe    N/A      |\n",
      "|    0   N/A  N/A      7452    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     12876    C+G   ...bat\\acrocef_2\\AcroCEF.exe    N/A      |\n",
      "|    0   N/A  N/A     13784    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14424    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14448    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15244    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     16788    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     17300    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     20344    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20628    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20992    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     21560    C+G   ...persky VPN 5.7\\ksdeui.exe    N/A      |\n",
      "|    0   N/A  N/A     22980    C+G   ...x64__pc75e8sa7ep4e\\XD.exe    N/A      |\n",
      "|    0   N/A  N/A     28216    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     30040    C+G   ...370.42\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     33760      C   ...ucas\\anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     34880    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     35264      C   ...ucas\\anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     40704    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     45908    C+G   ...370.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     47028    C+G   ...app-2.2238.7\\WhatsApp.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Estado da GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros fixos de treinamento\n",
    "\n",
    "verbose = 2\n",
    "seed = 25\n",
    "steps = 30\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "graphic = True\n",
    "logRetPeriod = 20\n",
    "downloadData = False\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = ['2022-10-24', '2022-10-25', '2022-10-26', '2022-10-27', '2022-10-28', \n",
    "            '2022-10-31', '2022-11-01', '2022-11-02', '2022-11-03', '2022-11-04', \n",
    "            '2022-11-07', '2022-11-08', '2022-11-09', '2022-11-10', '2022-11-11',\n",
    "            '2022-11-14', '2022-11-15', '2022-11-16', '2022-11-17', '2022-11-18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ativos = ['A', 'AAL', 'AAP', 'AAPL', 'ABBV', 'ABC', 'ABMD', 'ABT',\n",
    "          'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES',\n",
    "          'AFL', 'AIG', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL',\n",
    "          'ALLE', 'AMAT', 'AMCR', 'AMD', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN',\n",
    "          'ANET', 'ANSS', 'AON', 'AOS', 'APA', 'APD', 'APH', 'APTV', 'ARE',\n",
    "          'ATO', 'ATVI', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AZO', 'BA',\n",
    "          'BAC', 'BALL', 'BAX', 'BBWI', 'BBY', 'BDX', 'BEN', 'BF.B', 'BIIB',\n",
    "          'BIO', 'BK', 'BKNG', 'BKR', 'BLK', 'BMY', 'BR', 'BRK.B', 'BRO',\n",
    "          'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CARR', 'CAT', 'CB',\n",
    "          'CBOE', 'CBRE', 'CCI', 'CCL', 'CDAY', 'CDNS', 'CDW', 'CE', 'CEG',\n",
    "          'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX',\n",
    "          'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF',\n",
    "          'COO', 'COP', 'COST', 'CPB', 'CPRT', 'CPT', 'CRL', 'CRM', 'CSCO',\n",
    "          'CSGP', 'CSX', 'CTAS', 'CTLT', 'CTRA', 'CTSH', 'CTVA', 'CVS',\n",
    "          'CVX', 'CZR', 'D', 'DAL', 'DD', 'DE', 'DFS', 'DG', 'DGX', 'DHI',\n",
    "          'DHR', 'DIS', 'DISH', 'DLR', 'DLTR', 'DOV', 'DOW', 'DPZ', 'DRI',\n",
    "          'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'DXCM', 'EA', 'EBAY', 'ECL',\n",
    "          'ED', 'EFX', 'EIX', 'EL', 'ELV', 'EMN', 'EMR', 'ENPH', 'EOG',\n",
    "          'EPAM', 'EQIX', 'EQR', 'EQT', 'ES', 'ESS', 'ETN', 'ETR', 'ETSY',\n",
    "          'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG', 'FAST',\n",
    "          'FBHS', 'FCX', 'FDS', 'FDX', 'FE', 'FFIV', 'FIS', 'FISV', 'FITB',\n",
    "          'FLT', 'FMC', 'FOX', 'FOXA', 'FRC', 'FRT', 'FTNT', 'FTV', 'GD',\n",
    "          'GE', 'GILD', 'GIS', 'GL', 'GLW', 'GM', 'GNRC', 'GOOG', 'GOOGL',\n",
    "          'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HCA',\n",
    "          'HD', 'HES', 'HIG', 'HII', 'HLT', 'HOLX', 'HON', 'HPE', 'HPQ',\n",
    "          'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'HWM', 'IBM', 'ICE', 'IDXX',\n",
    "          'IEX', 'IFF', 'ILMN', 'INCY', 'INTC', 'INTU', 'INVH', 'IP', 'IPG',\n",
    "          'IQV', 'IR', 'IRM', 'ISRG', 'IT', 'ITW', 'IVZ', 'J', 'JBHT', 'JCI',\n",
    "          'JKHY', 'JNJ', 'JNPR', 'JPM', 'K', 'KDP', 'KEY', 'KEYS', 'KHC',\n",
    "          'KIM', 'KLAC', 'KMB', 'KMI', 'KMX', 'KO', 'KR', 'L', 'LDOS', 'LEN',\n",
    "          'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW',\n",
    "          'LRCX', 'LUMN', 'LUV', 'LVS', 'LW', 'LYB', 'LYV', 'MA', 'MAA',\n",
    "          'MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET',\n",
    "          'META', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST',\n",
    "          'MO', 'MOH', 'MOS', 'MPC', 'MPWR', 'MRK', 'MRNA', 'MRO', 'MS',\n",
    "          'MSCI', 'MSFT', 'MSI', 'MTB', 'MTCH', 'MTD', 'MU', 'NCLH', 'NDAQ',\n",
    "          'NDSN', 'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NLOK', 'NLSN', 'NOC',\n",
    "          'NOW', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWL',\n",
    "          'NWS', 'NWSA', 'NXPI', 'O', 'ODFL', 'OGN', 'OKE', 'OMC', 'ON',\n",
    "          'ORCL', 'ORLY', 'OTIS', 'OXY', 'PARA', 'PAYC', 'PAYX', 'PCAR',\n",
    "          'PCG', 'PEAK', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH',\n",
    "          'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'POOL',\n",
    "          'PPG', 'PPL', 'PRU', 'PSA', 'PSX', 'PTC', 'PWR', 'PXD', 'PYPL',\n",
    "          'QCOM', 'QRVO', 'RCL', 'RE', 'REG', 'REGN', 'RF', 'RHI', 'RJF',\n",
    "          'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX', 'SBAC',\n",
    "          'SBNY', 'SBUX', 'SCHW', 'SEDG', 'SEE', 'SHW', 'SIVB', 'SJM', 'SLB',\n",
    "          'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE', 'STE', 'STT', 'STX',\n",
    "          'STZ', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYY', 'T', 'TAP', 'TDG',\n",
    "          'TDY', 'TECH', 'TEL', 'TER', 'TFC', 'TFX', 'TGT', 'TJX', 'TMO',\n",
    "          'TMUS', 'TPR', 'TRMB', 'TROW', 'TRV', 'TSCO', 'TSLA', 'TSN', 'TT',\n",
    "          'TTWO', 'TWTR', 'TXN', 'TXT', 'TYL', 'UAL', 'UDR', 'UHS', 'ULTA',\n",
    "          'UNH', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VFC', 'VICI', 'VLO',\n",
    "          'VMC', 'VNO', 'VRSK', 'VRSN', 'VRTX', 'VTR', 'VTRS', 'VZ', 'WAB',\n",
    "          'WAT', 'WBA', 'WBD', 'WDC', 'WEC', 'WELL', 'WFC', 'WHR', 'WM',\n",
    "          'WMB', 'WMT', 'WRB', 'WRK', 'WST', 'WTW', 'WY', 'WYNN', 'XEL',\n",
    "          'XOM', 'XRAY', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if downloadData:\n",
    "\n",
    "    start_date = \"2017-10-21\"\n",
    "    end_date = \"2022-10-21\"\n",
    "\n",
    "    data = web.DataReader(name = '^GSPC', data_source = 'yahoo', start = start_date, end = end_date)\n",
    "    SP500_index = pd.DataFrame(data['Close']).reset_index().rename(columns={'Close': 'SP500', 'Date': 'Dia'})\n",
    "\n",
    "    SP500_close = pd.DataFrame()\n",
    "\n",
    "    for ativo in ativos:\n",
    "  \n",
    "        if ativo == 'BF.B':\n",
    "            ativo = 'BF-B'\n",
    "\n",
    "        if ativo == 'BRK.B':\n",
    "            ativo = 'BRK-B'\n",
    "\n",
    "        data = web.DataReader(name = ativo, data_source = 'yahoo', start = start_date, end = end_date)\n",
    "        temp_close = pd.DataFrame(data['Close'])  # .rename(columns={'Close': 'SP500', 'Date': 'Dia'})\n",
    "        SP500_close = pd.concat([SP500_close, temp_close], axis = 1)\n",
    "\n",
    "        \n",
    "    SP500_close.columns = ativos # .rename(columns={'Close': 'SP500', 'Date': 'Dia'})\n",
    "    SP500_close.reset_index(inplace = True)\n",
    "    SP500_close.rename(columns={'Date': 'Dia'}, inplace = True)\n",
    "\n",
    "    assert SP500_close.isna().sum().mean() == 0,  \"Valores Faltantes\"\n",
    "    assert SP500_index.isna().sum().mean() == 0,  \"Valores Faltantes\"\n",
    "\n",
    "    SP500_close.to_csv(path_or_buf = 'data/SP500_close', index = False)\n",
    "    SP500_index.to_csv(path_or_buf = 'data/SP500_index', index = False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    SP500_close = pd.read_csv('data/SP500_close')\n",
    "    SP500_index = pd.read_csv('data/SP500_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "setores = {'Industrials': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'HealthCare': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'InformationTechnology': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'CommunicationServices': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'ConsumerStaples': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'ConsumerDiscretionary': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'Utilities': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'Financials': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'Materials': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist(),\n",
    "           'RealEstate': df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorTimeframeTable(table, ativo):\n",
    "    TimeframeTable = pd.DataFrame(np.zeros((len(table[ativo])-steps, steps+1), dtype='float64'))\n",
    "\n",
    "    for index, close in enumerate(table[ativo]):\n",
    "        tempA = index\n",
    "        tempB = 0\n",
    "        for i in range(steps+1):\n",
    "            if tempA < len(table[ativo])-steps and tempA >=0:\n",
    "                TimeframeTable.iloc[tempA, tempB] = close\n",
    "\n",
    "            tempA -= 1\n",
    "            tempB += 1\n",
    "\n",
    "    timeIndex = table.iloc[steps:,0]\n",
    "    TimeframeTable[\"Dia\"] = timeIndex.to_numpy()\n",
    "    TimeframeTable.set_index(\"Dia\", inplace = True)\n",
    "    \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainScaler(df):\n",
    "    \n",
    "    trainScaler = pd.DataFrame()\n",
    " \n",
    "    for _ in range(steps+1):\n",
    "        temp_close = pd.DataFrame(df.iloc[:,-1])\n",
    "        trainScaler = pd.concat([trainScaler, temp_close], axis = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(trainScaler)\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessingdata(steps, df, ativos):\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "    \n",
    "\n",
    "    aux = []\n",
    "    \n",
    "    for ativo in ativos:\n",
    "        trainDataAtivo = generatorTimeframeTable(df, ativo)\n",
    "        trainDataAtivo.dropna(axis = 0, inplace = True)\n",
    "        \n",
    "        #----Score-Z--------------------------------------\n",
    "        scaler = createTrainScaler(trainDataAtivo)\n",
    "        trainDataAtivo = scaler.transform(trainDataAtivo)\n",
    "        #-------------------------------------------------\n",
    "        aux.append(trainDataAtivo)\n",
    "    \n",
    "    trainData = np.concatenate(tuple(aux), axis=0)\n",
    "    \n",
    "    X = trainData[:, :-1]\n",
    "    y = trainData[:, -1]\n",
    "    \n",
    "\n",
    "    #------Divisão de dados entre Treino e Validação------------------------------------------------\n",
    "    \n",
    "    X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "\n",
    "    X_treino = X_treino.reshape((-1, steps, 1))\n",
    "    X_teste = X_teste.reshape((-1, steps, 1))\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return [X_treino, X_teste, y_treino, y_teste]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção, Treinamento e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime('%d-%B-%Ih%Mmin')))\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=20,\n",
    "                          verbose = verbose,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              mode=\"min\",\n",
    "                              verbose = verbose,\n",
    "                              min_delta=0.0001,\n",
    "                              min_lr=0)\n",
    "\n",
    "callbacks = [tensorboard, earlystop, reduce_lr, TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = True,\n",
    "                   input_shape = (steps, 1)))  \n",
    "\n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = True))  \n",
    "    \n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = False)) \n",
    "    \n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    Lmse = MeanSquaredError()\n",
    "    \n",
    "    opt_Adadelta = Adadelta(learning_rate = 0.01, rho = 0.95, epsilon = 1e-07)\n",
    "\n",
    "    model.compile(loss= Lmse, optimizer = opt_Adadelta)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTableFrame(ativo, tablePrevision, model, table = SP500_close):\n",
    "    \n",
    "    TimeframeTable = generatorTimeframeTable(table, ativo)\n",
    "    \n",
    "    index_data = TimeframeTable.index\n",
    "    \n",
    "    scaler = createTrainScaler(TimeframeTable)\n",
    "\n",
    "    TimeframeTable = scaler.transform(TimeframeTable)\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "\n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    \n",
    "    for day in forecast:\n",
    "        \n",
    "        current_info = TimeframeTable.iloc[-1, 1:].to_numpy()\n",
    "        \n",
    "        standardCurrentInfo = current_info.reshape(1, steps, 1).astype('float32')\n",
    "        \n",
    "        current_forecast = model.predict(standardCurrentInfo, verbose=False).reshape(1,)\n",
    "        \n",
    "        new_line = np.concatenate((current_info, current_forecast), axis = 0)\n",
    "        \n",
    "        TimeframeTable = pd.concat([TimeframeTable,\n",
    "                                    pd.DataFrame(new_line.reshape(1, -1),\n",
    "                                                 columns = nameColumns,\n",
    "                                                 index = [day])], axis = 0)\n",
    "        \n",
    "        \n",
    "    index_data = TimeframeTable.index  \n",
    "    \n",
    "    TimeframeTable = scaler.inverse_transform(TimeframeTable)\n",
    "    \n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    TimeframeTable.index = pd.to_datetime(TimeframeTable.index)\n",
    "    \n",
    "    \n",
    "    #--------Popula tabela de previsão---------------------------------------------------\n",
    "    if ativo in ativos:\n",
    "            for day in forecast:\n",
    "                tablePrevision.loc[day, ativo] = TimeframeTable.loc[day, 'Close-0']\n",
    "    #------------------------------------------------------------------------------------\n",
    "   \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popula tabela de log-Retorno e gera gráficos\n",
    "\n",
    "def fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model):\n",
    "    lengthTable = len(tableLogRet)\n",
    "\n",
    "    outdir = './graphics/{}-{}'.format(nameSetor, datetime.now().strftime('%d-%B-%Ih%Mmin'))\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    for ativo in setor:\n",
    "\n",
    "        TimeframeSPAux = fillTableFrame(ativo, tablePrevision, model)\n",
    "\n",
    "        #-----------Graphic------------------------------------------------------------------------------------------\n",
    "        if graphic:\n",
    "            fig, ax = plt.subplots()\n",
    "            # substituir steps por len(forecast)\n",
    "            ax.plot(TimeframeSPAux.index[:-steps], TimeframeSPAux.iloc[:-steps, -1], linewidth=2.0, c = 'b')\n",
    "            ax.plot(TimeframeSPAux.index[-steps:], TimeframeSPAux.iloc[-steps:, -1], linewidth=2.0, c = 'r', ls = '-')\n",
    "            ax.legend(['Atual', 'Previsão'])\n",
    "            ax.set_title('Preço de Fechamento - {}'.format(ativo))\n",
    "            ax.set(xlabel='Tempo (ano)', ylabel='Preço ($)')\n",
    "            nameGraphic = '{}.jpg'.format(ativo)\n",
    "            fullname = os.path.join(outdir, nameGraphic)\n",
    "            plt.savefig(fullname)\n",
    "            plt.close(fig)\n",
    "        #------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        #--------Popula tabela de Log Retorno------------------------------------------------------------------------\n",
    "        for n in range(len(forecast)):\n",
    "            tableLogRet.loc[lengthTable-n-1, ativo] = \\\n",
    "            np.log(TimeframeSPAux.iloc[lengthTable-steps-n-1, -1] / TimeframeSPAux.iloc[lengthTable-steps-n-1-logRetPeriod, -1])\n",
    "        #------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela de Previsão\n",
    "\n",
    "update = pd.DataFrame(index = pd.to_datetime(forecast), columns = ativos) \\\n",
    "    .reset_index().rename(columns={'index': 'Dia'})\n",
    "\n",
    "# remover .copy()\n",
    "tablePrevision = pd.concat([SP500_close.copy(), update], axis = 0, ignore_index = True).set_index('Dia')\n",
    "\n",
    "tablePrevision.index = pd.to_datetime(tablePrevision.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela de Log-Retorno vazia \n",
    "\n",
    "index_data = pd.to_datetime(SP500_close['Dia'].append(pd.Series(forecast)))\n",
    "\n",
    "tableLogRet = pd.DataFrame(index = index_data,\n",
    "                           columns = ativos).reset_index().rename(columns={'index': 'Dia'})\n",
    "\n",
    "tableLogRet['Dia'] = tableLogRet['Dia'].apply(lambda date: date.strftime('%d/%m'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Modelo Não treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "2143/2143 - 45s - loss: 0.0921 - val_loss: 0.0461 - lr: 0.0100 - 45s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 38s - loss: 0.0428 - val_loss: 0.0367 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 38s - loss: 0.0370 - val_loss: 0.0323 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 37s - loss: 0.0329 - val_loss: 0.0289 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 37s - loss: 0.0299 - val_loss: 0.0267 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 37s - loss: 0.0279 - val_loss: 0.0250 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 37s - loss: 0.0264 - val_loss: 0.0237 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 37s - loss: 0.0252 - val_loss: 0.0227 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0242 - val_loss: 0.0218 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 38s - loss: 0.0233 - val_loss: 0.0211 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 37s - loss: 0.0225 - val_loss: 0.0204 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 36s - loss: 0.0218 - val_loss: 0.0199 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 36s - loss: 0.0211 - val_loss: 0.0191 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 36s - loss: 0.0205 - val_loss: 0.0186 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 36s - loss: 0.0199 - val_loss: 0.0182 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 36s - loss: 0.0194 - val_loss: 0.0177 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 37s - loss: 0.0189 - val_loss: 0.0172 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 38s - loss: 0.0184 - val_loss: 0.0169 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 37s - loss: 0.0180 - val_loss: 0.0164 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 37s - loss: 0.0176 - val_loss: 0.0160 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 38s - loss: 0.0172 - val_loss: 0.0157 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 44s - loss: 0.0169 - val_loss: 0.0154 - lr: 0.0100 - 44s/epoch - 20ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 37s - loss: 0.0166 - val_loss: 0.0151 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0162 - val_loss: 0.0150 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 37s - loss: 0.0160 - val_loss: 0.0146 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 38s - loss: 0.0157 - val_loss: 0.0143 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 40s - loss: 0.0154 - val_loss: 0.0141 - lr: 0.0100 - 40s/epoch - 19ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 38s - loss: 0.0152 - val_loss: 0.0139 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 37s - loss: 0.0149 - val_loss: 0.0138 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0147 - val_loss: 0.0136 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 36s - loss: 0.0145 - val_loss: 0.0133 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0143 - val_loss: 0.0131 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0141 - val_loss: 0.0129 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 36s - loss: 0.0139 - val_loss: 0.0128 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 36s - loss: 0.0137 - val_loss: 0.0125 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0135 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 37s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 38s - loss: 0.0132 - val_loss: 0.0121 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 38s - loss: 0.0130 - val_loss: 0.0120 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0129 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 39s - loss: 0.0127 - val_loss: 0.0116 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 39s - loss: 0.0126 - val_loss: 0.0115 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 37s - loss: 0.0124 - val_loss: 0.0114 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 37s - loss: 0.0123 - val_loss: 0.0112 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 38s - loss: 0.0122 - val_loss: 0.0112 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 38s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 38s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 38s - loss: 0.0118 - val_loss: 0.0108 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 38s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 38s - loss: 0.0116 - val_loss: 0.0105 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 38s - loss: 0.0115 - val_loss: 0.0104 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0104 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 38s - loss: 0.0112 - val_loss: 0.0103 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 40s - loss: 0.0111 - val_loss: 0.0101 - lr: 0.0100 - 40s/epoch - 19ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 38s - loss: 0.0110 - val_loss: 0.0102 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 38s - loss: 0.0109 - val_loss: 0.0100 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 38s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 40s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 40s/epoch - 19ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 39s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 38s - loss: 0.0106 - val_loss: 0.0096 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 39s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 36s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 36s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 36s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 36s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 36s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 38s - loss: 0.0095 - val_loss: 0.0087 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 38s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 39s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 38s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 38s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2143/2143 - 45s - loss: 0.0089 - val_loss: 0.0081 - lr: 6.4000e-07 - 45s/epoch - 21ms/step\n",
      "Epoch 108/2000\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "2143/2143 - 36s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "Restoring model weights from the end of the best epoch: 105.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Industrials/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Industrials/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0089\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0081\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00894\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00811\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 48s - loss: 0.0921 - val_loss: 0.0468 - lr: 0.0100 - 48s/epoch - 22ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 39s - loss: 0.0436 - val_loss: 0.0374 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 39s - loss: 0.0377 - val_loss: 0.0329 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 38s - loss: 0.0336 - val_loss: 0.0297 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 37s - loss: 0.0305 - val_loss: 0.0270 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 38s - loss: 0.0282 - val_loss: 0.0257 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 37s - loss: 0.0266 - val_loss: 0.0240 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 38s - loss: 0.0254 - val_loss: 0.0229 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0243 - val_loss: 0.0221 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 36s - loss: 0.0234 - val_loss: 0.0212 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 37s - loss: 0.0226 - val_loss: 0.0204 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 36s - loss: 0.0218 - val_loss: 0.0198 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 36s - loss: 0.0212 - val_loss: 0.0192 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 36s - loss: 0.0205 - val_loss: 0.0187 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 35s - loss: 0.0200 - val_loss: 0.0181 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 35s - loss: 0.0194 - val_loss: 0.0177 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 35s - loss: 0.0189 - val_loss: 0.0173 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 35s - loss: 0.0185 - val_loss: 0.0168 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 35s - loss: 0.0180 - val_loss: 0.0164 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 35s - loss: 0.0176 - val_loss: 0.0161 - lr: 0.0100 - 35s/epoch - 16ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 35s - loss: 0.0173 - val_loss: 0.0158 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 36s - loss: 0.0169 - val_loss: 0.0154 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 36s - loss: 0.0166 - val_loss: 0.0151 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0163 - val_loss: 0.0149 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 36s - loss: 0.0160 - val_loss: 0.0146 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 37s - loss: 0.0157 - val_loss: 0.0144 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 37s - loss: 0.0155 - val_loss: 0.0142 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 36s - loss: 0.0152 - val_loss: 0.0139 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 36s - loss: 0.0150 - val_loss: 0.0138 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0147 - val_loss: 0.0137 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0145 - val_loss: 0.0133 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0143 - val_loss: 0.0131 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0141 - val_loss: 0.0129 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 37s - loss: 0.0139 - val_loss: 0.0127 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 37s - loss: 0.0138 - val_loss: 0.0126 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0136 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 37s - loss: 0.0134 - val_loss: 0.0125 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 37s - loss: 0.0132 - val_loss: 0.0121 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 36s - loss: 0.0131 - val_loss: 0.0119 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0129 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0128 - val_loss: 0.0117 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 36s - loss: 0.0126 - val_loss: 0.0116 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 37s - loss: 0.0125 - val_loss: 0.0114 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 39s - loss: 0.0123 - val_loss: 0.0113 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 38s - loss: 0.0122 - val_loss: 0.0112 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 37s - loss: 0.0121 - val_loss: 0.0110 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 37s - loss: 0.0118 - val_loss: 0.0108 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 37s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 37s - loss: 0.0116 - val_loss: 0.0106 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 37s - loss: 0.0115 - val_loss: 0.0105 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 36s - loss: 0.0114 - val_loss: 0.0104 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0103 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 38s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 36s - loss: 0.0111 - val_loss: 0.0101 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 37s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 37s - loss: 0.0109 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 36s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 37s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 36s - loss: 0.0106 - val_loss: 0.0096 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 37s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 36s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 40s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 40s/epoch - 19ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0090 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 38s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0087 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 36s - loss: 0.0095 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 36s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 36s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 38s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 38s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 90/2000\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 35s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 35s/epoch - 16ms/step\n",
      "Epoch 94/2000\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 38s/epoch - 18ms/step\n",
      "Epoch 120/2000\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 120: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/HealthCare/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/HealthCare/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0090\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0081\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00896\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00812\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 45s - loss: 0.0917 - val_loss: 0.0454 - lr: 0.0100 - 45s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 36s - loss: 0.0435 - val_loss: 0.0373 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 37s - loss: 0.0376 - val_loss: 0.0327 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 37s - loss: 0.0333 - val_loss: 0.0295 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 36s - loss: 0.0302 - val_loss: 0.0271 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 37s - loss: 0.0281 - val_loss: 0.0251 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 37s - loss: 0.0266 - val_loss: 0.0239 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 37s - loss: 0.0254 - val_loss: 0.0229 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0244 - val_loss: 0.0222 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 36s - loss: 0.0235 - val_loss: 0.0212 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 37s - loss: 0.0227 - val_loss: 0.0205 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 37s - loss: 0.0219 - val_loss: 0.0198 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 37s - loss: 0.0212 - val_loss: 0.0192 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 37s - loss: 0.0206 - val_loss: 0.0187 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 37s - loss: 0.0200 - val_loss: 0.0182 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 37s - loss: 0.0195 - val_loss: 0.0178 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 37s - loss: 0.0190 - val_loss: 0.0173 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 37s - loss: 0.0185 - val_loss: 0.0169 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 37s - loss: 0.0181 - val_loss: 0.0165 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 37s - loss: 0.0177 - val_loss: 0.0162 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 37s - loss: 0.0173 - val_loss: 0.0158 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 37s - loss: 0.0170 - val_loss: 0.0155 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 37s - loss: 0.0166 - val_loss: 0.0152 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 37s - loss: 0.0163 - val_loss: 0.0149 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 37s - loss: 0.0160 - val_loss: 0.0146 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 37s - loss: 0.0157 - val_loss: 0.0145 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 37s - loss: 0.0155 - val_loss: 0.0142 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 37s - loss: 0.0152 - val_loss: 0.0139 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 37s - loss: 0.0150 - val_loss: 0.0137 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0148 - val_loss: 0.0135 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0145 - val_loss: 0.0133 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0143 - val_loss: 0.0131 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0141 - val_loss: 0.0129 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 37s - loss: 0.0139 - val_loss: 0.0128 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 37s - loss: 0.0138 - val_loss: 0.0126 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0136 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 36s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 37s - loss: 0.0133 - val_loss: 0.0121 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 37s - loss: 0.0131 - val_loss: 0.0120 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 36s - loss: 0.0129 - val_loss: 0.0118 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 36s - loss: 0.0128 - val_loss: 0.0117 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0126 - val_loss: 0.0116 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 37s - loss: 0.0125 - val_loss: 0.0114 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 37s - loss: 0.0124 - val_loss: 0.0113 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 37s - loss: 0.0122 - val_loss: 0.0112 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 36s - loss: 0.0121 - val_loss: 0.0111 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 37s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 37s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 37s - loss: 0.0116 - val_loss: 0.0106 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 36s - loss: 0.0115 - val_loss: 0.0105 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 37s - loss: 0.0114 - val_loss: 0.0104 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0103 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 37s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 37s - loss: 0.0111 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 37s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 37s - loss: 0.0109 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 37s - loss: 0.0108 - val_loss: 0.0099 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 37s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 39s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 39s - loss: 0.0106 - val_loss: 0.0097 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 38s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 38s - loss: 0.0104 - val_loss: 0.0096 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 38s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 38s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 36s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0085 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 38s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 38s/epoch - 18ms/step\n",
      "Epoch 97/2000\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 127/2000\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.1920e-12 - 36s/epoch - 17ms/step\n",
      "Epoch 129/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.1920e-12 - 36s/epoch - 17ms/step\n",
      "Epoch 130/2000\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "2143/2143 - 35s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.1920e-12 - 35s/epoch - 16ms/step\n",
      "Epoch 131/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6384e-12 - 36s/epoch - 17ms/step\n",
      "Epoch 132/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6384e-12 - 36s/epoch - 17ms/step\n",
      "Epoch 133/2000\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6384e-12 - 36s/epoch - 17ms/step\n",
      "Epoch 134/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2768e-13 - 36s/epoch - 17ms/step\n",
      "Epoch 135/2000\n",
      "Restoring model weights from the end of the best epoch: 115.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2768e-13 - 36s/epoch - 17ms/step\n",
      "Epoch 135: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/InformationTechnology/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/InformationTechnology/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 14s 7ms/step - loss: 0.0091\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0082\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00906\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00822\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 45s - loss: 0.0938 - val_loss: 0.0453 - lr: 0.0100 - 45s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 37s - loss: 0.0424 - val_loss: 0.0369 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 37s - loss: 0.0368 - val_loss: 0.0322 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 36s - loss: 0.0329 - val_loss: 0.0291 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 37s - loss: 0.0300 - val_loss: 0.0269 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 37s - loss: 0.0279 - val_loss: 0.0249 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 36s - loss: 0.0263 - val_loss: 0.0237 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 36s - loss: 0.0251 - val_loss: 0.0226 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0241 - val_loss: 0.0217 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 36s - loss: 0.0232 - val_loss: 0.0209 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 36s - loss: 0.0224 - val_loss: 0.0203 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 36s - loss: 0.0216 - val_loss: 0.0197 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 36s - loss: 0.0210 - val_loss: 0.0193 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 37s - loss: 0.0204 - val_loss: 0.0185 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 37s - loss: 0.0198 - val_loss: 0.0180 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 37s - loss: 0.0193 - val_loss: 0.0175 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 36s - loss: 0.0188 - val_loss: 0.0172 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 36s - loss: 0.0183 - val_loss: 0.0167 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 36s - loss: 0.0179 - val_loss: 0.0163 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 36s - loss: 0.0175 - val_loss: 0.0160 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 36s - loss: 0.0171 - val_loss: 0.0157 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 36s - loss: 0.0168 - val_loss: 0.0153 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 36s - loss: 0.0165 - val_loss: 0.0150 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0161 - val_loss: 0.0148 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 36s - loss: 0.0159 - val_loss: 0.0145 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 36s - loss: 0.0156 - val_loss: 0.0143 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 36s - loss: 0.0153 - val_loss: 0.0143 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 36s - loss: 0.0151 - val_loss: 0.0138 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 36s - loss: 0.0148 - val_loss: 0.0136 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0146 - val_loss: 0.0135 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0144 - val_loss: 0.0132 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0142 - val_loss: 0.0130 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0140 - val_loss: 0.0129 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 37s - loss: 0.0138 - val_loss: 0.0126 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 36s - loss: 0.0136 - val_loss: 0.0127 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 36s - loss: 0.0135 - val_loss: 0.0123 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 36s - loss: 0.0133 - val_loss: 0.0123 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 36s - loss: 0.0131 - val_loss: 0.0120 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 36s - loss: 0.0130 - val_loss: 0.0119 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0128 - val_loss: 0.0117 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0127 - val_loss: 0.0116 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0125 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 36s - loss: 0.0124 - val_loss: 0.0113 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 37s - loss: 0.0122 - val_loss: 0.0113 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 36s - loss: 0.0121 - val_loss: 0.0111 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 36s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 36s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 36s - loss: 0.0116 - val_loss: 0.0107 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 36s - loss: 0.0115 - val_loss: 0.0105 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 36s - loss: 0.0114 - val_loss: 0.0104 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 36s - loss: 0.0113 - val_loss: 0.0103 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 36s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 37s - loss: 0.0111 - val_loss: 0.0101 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 36s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 36s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 36s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 36s - loss: 0.0107 - val_loss: 0.0098 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 36s - loss: 0.0106 - val_loss: 0.0097 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 36s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 36s - loss: 0.0105 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 36s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 36s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 36s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 36s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 36s - loss: 0.0100 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 36s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 36s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 36s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 36s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 36s - loss: 0.0093 - val_loss: 0.0086 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 36s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.5600e-08 - 38s/epoch - 18ms/step\n",
      "Epoch 113/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.0240e-09 - 38s/epoch - 18ms/step\n",
      "Epoch 119/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "Restoring model weights from the end of the best epoch: 105.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 8.1920e-12 - 36s/epoch - 17ms/step\n",
      "Epoch 125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_15_layer_call_fn, lstm_cell_15_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn, lstm_cell_16_layer_call_and_return_conditional_losses, lstm_cell_17_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/CommunicationServices/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/CommunicationServices/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0090\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0082\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00899\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00816\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 45s - loss: 0.0905 - val_loss: 0.0452 - lr: 0.0100 - 45s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 37s - loss: 0.0425 - val_loss: 0.0363 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 36s - loss: 0.0367 - val_loss: 0.0320 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 38s - loss: 0.0327 - val_loss: 0.0289 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 36s - loss: 0.0298 - val_loss: 0.0265 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 36s - loss: 0.0278 - val_loss: 0.0248 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 36s - loss: 0.0263 - val_loss: 0.0236 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 36s - loss: 0.0251 - val_loss: 0.0226 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 36s - loss: 0.0241 - val_loss: 0.0217 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 37s - loss: 0.0232 - val_loss: 0.0210 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 37s - loss: 0.0224 - val_loss: 0.0202 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 36s - loss: 0.0217 - val_loss: 0.0196 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 37s - loss: 0.0210 - val_loss: 0.0191 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 36s - loss: 0.0204 - val_loss: 0.0185 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 36s - loss: 0.0198 - val_loss: 0.0180 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 36s - loss: 0.0193 - val_loss: 0.0175 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 36s - loss: 0.0188 - val_loss: 0.0172 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 36s - loss: 0.0184 - val_loss: 0.0167 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 37s - loss: 0.0179 - val_loss: 0.0164 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 37s - loss: 0.0175 - val_loss: 0.0160 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 37s - loss: 0.0172 - val_loss: 0.0157 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 37s - loss: 0.0168 - val_loss: 0.0154 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 37s - loss: 0.0165 - val_loss: 0.0151 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 37s - loss: 0.0162 - val_loss: 0.0148 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 37s - loss: 0.0159 - val_loss: 0.0146 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 37s - loss: 0.0157 - val_loss: 0.0146 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 37s - loss: 0.0154 - val_loss: 0.0141 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 37s - loss: 0.0152 - val_loss: 0.0138 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 37s - loss: 0.0149 - val_loss: 0.0137 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0147 - val_loss: 0.0135 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0145 - val_loss: 0.0133 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0143 - val_loss: 0.0131 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0141 - val_loss: 0.0129 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 37s - loss: 0.0139 - val_loss: 0.0127 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 37s - loss: 0.0137 - val_loss: 0.0125 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0135 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 37s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 37s - loss: 0.0132 - val_loss: 0.0121 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 37s - loss: 0.0130 - val_loss: 0.0119 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0129 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0127 - val_loss: 0.0117 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0126 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 36s - loss: 0.0124 - val_loss: 0.0114 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 36s - loss: 0.0123 - val_loss: 0.0112 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 36s - loss: 0.0122 - val_loss: 0.0111 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 37s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 37s - loss: 0.0118 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 37s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 37s - loss: 0.0116 - val_loss: 0.0105 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 37s - loss: 0.0114 - val_loss: 0.0105 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0104 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 37s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 36s - loss: 0.0111 - val_loss: 0.0102 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 36s - loss: 0.0110 - val_loss: 0.0101 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 36s - loss: 0.0109 - val_loss: 0.0100 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 36s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 37s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 37s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 36s - loss: 0.0106 - val_loss: 0.0096 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 37s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 37s - loss: 0.0099 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 38s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 42s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 42s/epoch - 20ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 42s - loss: 0.0092 - val_loss: 0.0085 - lr: 0.0100 - 42s/epoch - 19ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 40s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 40s/epoch - 18ms/step\n",
      "Epoch 86/2000\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 41s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 41s/epoch - 19ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 44s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 44s/epoch - 20ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 50s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 50s/epoch - 24ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 46s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 46s/epoch - 21ms/step\n",
      "Epoch 90/2000\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 40s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 40s/epoch - 19ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2143/2143 - 38s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 38s/epoch - 18ms/step\n",
      "Epoch 102/2000\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "Restoring model weights from the end of the best epoch: 103.\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 123: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_18_layer_call_fn, lstm_cell_18_layer_call_and_return_conditional_losses, lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerStaples/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerStaples/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0091\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0082\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00909\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00825\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 46s - loss: 0.0894 - val_loss: 0.0453 - lr: 0.0100 - 46s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 36s - loss: 0.0427 - val_loss: 0.0368 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 36s - loss: 0.0369 - val_loss: 0.0322 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 36s - loss: 0.0329 - val_loss: 0.0288 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 36s - loss: 0.0298 - val_loss: 0.0266 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 36s - loss: 0.0277 - val_loss: 0.0249 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 37s - loss: 0.0263 - val_loss: 0.0240 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 37s - loss: 0.0251 - val_loss: 0.0226 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0241 - val_loss: 0.0217 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 37s - loss: 0.0232 - val_loss: 0.0209 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 36s - loss: 0.0224 - val_loss: 0.0203 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 36s - loss: 0.0217 - val_loss: 0.0196 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 37s - loss: 0.0210 - val_loss: 0.0190 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 37s - loss: 0.0204 - val_loss: 0.0185 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 37s - loss: 0.0198 - val_loss: 0.0181 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 37s - loss: 0.0193 - val_loss: 0.0176 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 37s - loss: 0.0188 - val_loss: 0.0172 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 36s - loss: 0.0184 - val_loss: 0.0168 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 36s - loss: 0.0180 - val_loss: 0.0163 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 37s - loss: 0.0176 - val_loss: 0.0161 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 36s - loss: 0.0172 - val_loss: 0.0157 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 36s - loss: 0.0168 - val_loss: 0.0155 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 36s - loss: 0.0165 - val_loss: 0.0151 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0162 - val_loss: 0.0148 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 37s - loss: 0.0159 - val_loss: 0.0146 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 36s - loss: 0.0157 - val_loss: 0.0144 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 36s - loss: 0.0154 - val_loss: 0.0141 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 36s - loss: 0.0151 - val_loss: 0.0139 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 36s - loss: 0.0149 - val_loss: 0.0136 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 36s - loss: 0.0147 - val_loss: 0.0134 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0145 - val_loss: 0.0133 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0143 - val_loss: 0.0131 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0141 - val_loss: 0.0129 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 36s - loss: 0.0139 - val_loss: 0.0127 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 36s - loss: 0.0137 - val_loss: 0.0125 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 36s - loss: 0.0135 - val_loss: 0.0124 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 36s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 36s - loss: 0.0132 - val_loss: 0.0120 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 36s - loss: 0.0130 - val_loss: 0.0119 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0129 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0127 - val_loss: 0.0116 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0126 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 36s - loss: 0.0124 - val_loss: 0.0114 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 36s - loss: 0.0123 - val_loss: 0.0112 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 36s - loss: 0.0122 - val_loss: 0.0111 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 36s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 37s - loss: 0.0118 - val_loss: 0.0108 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 37s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 37s - loss: 0.0116 - val_loss: 0.0105 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 37s - loss: 0.0115 - val_loss: 0.0105 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0104 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 36s - loss: 0.0112 - val_loss: 0.0103 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 36s - loss: 0.0111 - val_loss: 0.0102 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 36s - loss: 0.0110 - val_loss: 0.0101 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 36s - loss: 0.0109 - val_loss: 0.0100 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 37s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 37s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 36s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 37s - loss: 0.0106 - val_loss: 0.0097 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 37s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 36s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 36s - loss: 0.0104 - val_loss: 0.0094 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0090 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 36s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 36s - loss: 0.0095 - val_loss: 0.0087 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 36s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 36s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 36s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 36s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 38s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 38s/epoch - 18ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 36s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 36s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "Restoring model weights from the end of the best epoch: 104.\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0082 - lr: 4.0960e-11 - 36s/epoch - 17ms/step\n",
      "Epoch 124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_22_layer_call_fn, lstm_cell_22_layer_call_and_return_conditional_losses, lstm_cell_23_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerDiscretionary/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/ConsumerDiscretionary/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0091\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0082\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00906\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00822\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 44s - loss: 0.0896 - val_loss: 0.0456 - lr: 0.0100 - 44s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 37s - loss: 0.0436 - val_loss: 0.0376 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 37s - loss: 0.0379 - val_loss: 0.0332 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 37s - loss: 0.0337 - val_loss: 0.0296 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 37s - loss: 0.0306 - val_loss: 0.0272 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 37s - loss: 0.0283 - val_loss: 0.0254 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 36s - loss: 0.0267 - val_loss: 0.0240 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 37s - loss: 0.0255 - val_loss: 0.0230 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0245 - val_loss: 0.0221 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 37s - loss: 0.0236 - val_loss: 0.0215 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 37s - loss: 0.0227 - val_loss: 0.0207 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 37s - loss: 0.0220 - val_loss: 0.0199 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 37s - loss: 0.0213 - val_loss: 0.0193 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 37s - loss: 0.0207 - val_loss: 0.0189 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 36s - loss: 0.0201 - val_loss: 0.0183 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 36s - loss: 0.0196 - val_loss: 0.0179 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 36s - loss: 0.0191 - val_loss: 0.0173 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 36s - loss: 0.0186 - val_loss: 0.0169 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 36s - loss: 0.0182 - val_loss: 0.0165 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 36s - loss: 0.0178 - val_loss: 0.0162 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 36s - loss: 0.0174 - val_loss: 0.0159 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 36s - loss: 0.0170 - val_loss: 0.0155 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 36s - loss: 0.0167 - val_loss: 0.0152 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0164 - val_loss: 0.0150 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 36s - loss: 0.0161 - val_loss: 0.0147 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 36s - loss: 0.0158 - val_loss: 0.0144 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 36s - loss: 0.0155 - val_loss: 0.0142 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 36s - loss: 0.0153 - val_loss: 0.0139 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 36s - loss: 0.0150 - val_loss: 0.0137 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 36s - loss: 0.0148 - val_loss: 0.0135 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0146 - val_loss: 0.0134 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 36s - loss: 0.0144 - val_loss: 0.0131 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 36s - loss: 0.0142 - val_loss: 0.0130 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 36s - loss: 0.0140 - val_loss: 0.0128 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 36s - loss: 0.0138 - val_loss: 0.0126 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0136 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 37s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 37s - loss: 0.0133 - val_loss: 0.0121 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 37s - loss: 0.0131 - val_loss: 0.0120 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0130 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0128 - val_loss: 0.0117 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0127 - val_loss: 0.0116 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 37s - loss: 0.0125 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 36s - loss: 0.0124 - val_loss: 0.0113 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 37s - loss: 0.0123 - val_loss: 0.0113 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 37s - loss: 0.0121 - val_loss: 0.0111 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 37s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 36s - loss: 0.0118 - val_loss: 0.0108 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 36s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 37s - loss: 0.0115 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 37s - loss: 0.0114 - val_loss: 0.0105 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0103 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 37s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 37s - loss: 0.0111 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 36s - loss: 0.0110 - val_loss: 0.0101 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 36s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 36s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 36s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 36s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 37s - loss: 0.0106 - val_loss: 0.0097 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 38s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 36s - loss: 0.0105 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 36s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 36s - loss: 0.0102 - val_loss: 0.0094 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 36s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 36s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 36s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 36s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0091 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 36s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 36s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 36s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 42s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.6000e-05 - 42s/epoch - 20ms/step\n",
      "Epoch 100/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 38s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.6000e-05 - 38s/epoch - 18ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 45s - loss: 0.0091 - val_loss: 0.0083 - lr: 3.2000e-06 - 45s/epoch - 21ms/step\n",
      "Epoch 103/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 38s - loss: 0.0091 - val_loss: 0.0083 - lr: 6.4000e-07 - 38s/epoch - 18ms/step\n",
      "Epoch 106/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.0240e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "Restoring model weights from the end of the best epoch: 101.\n",
      "2143/2143 - 36s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 121: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_24_layer_call_fn, lstm_cell_24_layer_call_and_return_conditional_losses, lstm_cell_25_layer_call_fn, lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Utilities/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Utilities/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0091\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0083\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00913\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00829\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 45s - loss: 0.0907 - val_loss: 0.0446 - lr: 0.0100 - 45s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 36s - loss: 0.0428 - val_loss: 0.0369 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 36s - loss: 0.0372 - val_loss: 0.0326 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 36s - loss: 0.0332 - val_loss: 0.0292 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 36s - loss: 0.0302 - val_loss: 0.0269 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 36s - loss: 0.0281 - val_loss: 0.0253 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 36s - loss: 0.0266 - val_loss: 0.0240 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 36s - loss: 0.0254 - val_loss: 0.0229 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 36s - loss: 0.0244 - val_loss: 0.0220 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 36s - loss: 0.0235 - val_loss: 0.0212 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 36s - loss: 0.0227 - val_loss: 0.0205 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 36s - loss: 0.0219 - val_loss: 0.0200 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 36s - loss: 0.0212 - val_loss: 0.0194 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 36s - loss: 0.0206 - val_loss: 0.0187 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 36s - loss: 0.0201 - val_loss: 0.0182 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 36s - loss: 0.0195 - val_loss: 0.0177 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 36s - loss: 0.0191 - val_loss: 0.0173 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 36s - loss: 0.0186 - val_loss: 0.0169 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 36s - loss: 0.0181 - val_loss: 0.0166 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 36s - loss: 0.0178 - val_loss: 0.0163 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 37s - loss: 0.0174 - val_loss: 0.0159 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 37s - loss: 0.0170 - val_loss: 0.0155 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 37s - loss: 0.0167 - val_loss: 0.0152 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0164 - val_loss: 0.0150 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 37s - loss: 0.0161 - val_loss: 0.0147 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 37s - loss: 0.0158 - val_loss: 0.0144 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 37s - loss: 0.0155 - val_loss: 0.0142 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 37s - loss: 0.0153 - val_loss: 0.0140 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 37s - loss: 0.0150 - val_loss: 0.0139 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0148 - val_loss: 0.0135 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0146 - val_loss: 0.0134 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0144 - val_loss: 0.0131 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0142 - val_loss: 0.0130 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 37s - loss: 0.0140 - val_loss: 0.0128 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 37s - loss: 0.0138 - val_loss: 0.0127 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0136 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 37s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 37s - loss: 0.0133 - val_loss: 0.0121 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 37s - loss: 0.0131 - val_loss: 0.0120 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0129 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0128 - val_loss: 0.0117 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0126 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 37s - loss: 0.0125 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 37s - loss: 0.0124 - val_loss: 0.0114 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 37s - loss: 0.0122 - val_loss: 0.0112 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 36s - loss: 0.0121 - val_loss: 0.0110 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 38s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 37s - loss: 0.0118 - val_loss: 0.0108 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 36s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 37s - loss: 0.0116 - val_loss: 0.0106 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 37s - loss: 0.0115 - val_loss: 0.0106 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 37s - loss: 0.0114 - val_loss: 0.0104 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0103 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 37s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 37s - loss: 0.0111 - val_loss: 0.0101 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 37s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 37s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 38s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 39s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 39s/epoch - 18ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 37s - loss: 0.0106 - val_loss: 0.0096 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 37s - loss: 0.0105 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0099 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 37s - loss: 0.0099 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 38s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "2143/2143 - 38s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.0480e-10 - 38s/epoch - 18ms/step\n",
      "Epoch 122/2000\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "Restoring model weights from the end of the best epoch: 104.\n",
      "2143/2143 - 37s - loss: 0.0089 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 124: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_27_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn, lstm_cell_28_layer_call_and_return_conditional_losses, lstm_cell_29_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Financials/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Financials/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0089\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0081\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00895\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00812\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 46s - loss: 0.0950 - val_loss: 0.0452 - lr: 0.0100 - 46s/epoch - 22ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 37s - loss: 0.0428 - val_loss: 0.0377 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 37s - loss: 0.0372 - val_loss: 0.0325 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 37s - loss: 0.0332 - val_loss: 0.0292 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 37s - loss: 0.0301 - val_loss: 0.0269 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 36s - loss: 0.0279 - val_loss: 0.0250 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 37s - loss: 0.0263 - val_loss: 0.0238 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 37s - loss: 0.0251 - val_loss: 0.0227 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0241 - val_loss: 0.0218 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 37s - loss: 0.0233 - val_loss: 0.0211 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 37s - loss: 0.0224 - val_loss: 0.0203 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 37s - loss: 0.0217 - val_loss: 0.0196 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 37s - loss: 0.0211 - val_loss: 0.0191 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 37s - loss: 0.0204 - val_loss: 0.0186 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 37s - loss: 0.0199 - val_loss: 0.0181 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 37s - loss: 0.0194 - val_loss: 0.0176 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 37s - loss: 0.0189 - val_loss: 0.0172 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 37s - loss: 0.0184 - val_loss: 0.0167 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 37s - loss: 0.0180 - val_loss: 0.0164 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 37s - loss: 0.0176 - val_loss: 0.0160 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 37s - loss: 0.0172 - val_loss: 0.0157 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 37s - loss: 0.0168 - val_loss: 0.0154 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 37s - loss: 0.0165 - val_loss: 0.0151 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 37s - loss: 0.0162 - val_loss: 0.0149 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 37s - loss: 0.0159 - val_loss: 0.0145 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 37s - loss: 0.0156 - val_loss: 0.0143 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 37s - loss: 0.0154 - val_loss: 0.0142 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 37s - loss: 0.0151 - val_loss: 0.0139 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 37s - loss: 0.0149 - val_loss: 0.0136 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0147 - val_loss: 0.0135 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0145 - val_loss: 0.0132 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0142 - val_loss: 0.0130 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 37s - loss: 0.0141 - val_loss: 0.0129 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 37s - loss: 0.0139 - val_loss: 0.0127 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 37s - loss: 0.0137 - val_loss: 0.0125 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0135 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 37s - loss: 0.0133 - val_loss: 0.0124 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 38s - loss: 0.0132 - val_loss: 0.0121 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 37s - loss: 0.0130 - val_loss: 0.0119 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0128 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0127 - val_loss: 0.0117 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0126 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 37s - loss: 0.0124 - val_loss: 0.0114 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 37s - loss: 0.0123 - val_loss: 0.0112 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 37s - loss: 0.0121 - val_loss: 0.0111 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 37s - loss: 0.0120 - val_loss: 0.0110 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0119 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 37s - loss: 0.0118 - val_loss: 0.0108 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 37s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 37s - loss: 0.0116 - val_loss: 0.0106 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 37s - loss: 0.0114 - val_loss: 0.0104 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0103 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 37s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 37s - loss: 0.0111 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 37s - loss: 0.0110 - val_loss: 0.0101 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 37s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 37s - loss: 0.0109 - val_loss: 0.0099 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 37s - loss: 0.0108 - val_loss: 0.0098 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 37s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 38s - loss: 0.0106 - val_loss: 0.0097 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 37s - loss: 0.0105 - val_loss: 0.0096 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0092 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 37s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 36s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 36s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 36s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 36s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 36s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.2800e-07 - 38s/epoch - 18ms/step\n",
      "Epoch 110/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 5.1200e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0082 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "Restoring model weights from the end of the best epoch: 105.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 125: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_30_layer_call_fn, lstm_cell_30_layer_call_and_return_conditional_losses, lstm_cell_31_layer_call_fn, lstm_cell_31_layer_call_and_return_conditional_losses, lstm_cell_32_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Materials/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Materials/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0090\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0082\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00903\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00819\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2143/2143 - 44s - loss: 0.0915 - val_loss: 0.0465 - lr: 0.0100 - 44s/epoch - 21ms/step\n",
      "Epoch 2/2000\n",
      "2143/2143 - 37s - loss: 0.0435 - val_loss: 0.0376 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2143/2143 - 37s - loss: 0.0376 - val_loss: 0.0328 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2143/2143 - 37s - loss: 0.0334 - val_loss: 0.0294 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2143/2143 - 37s - loss: 0.0303 - val_loss: 0.0269 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2143/2143 - 37s - loss: 0.0282 - val_loss: 0.0255 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "2143/2143 - 37s - loss: 0.0267 - val_loss: 0.0240 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 8/2000\n",
      "2143/2143 - 37s - loss: 0.0255 - val_loss: 0.0230 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2143/2143 - 37s - loss: 0.0245 - val_loss: 0.0221 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 10/2000\n",
      "2143/2143 - 37s - loss: 0.0236 - val_loss: 0.0213 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "2143/2143 - 36s - loss: 0.0228 - val_loss: 0.0206 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2143/2143 - 37s - loss: 0.0220 - val_loss: 0.0200 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2143/2143 - 37s - loss: 0.0214 - val_loss: 0.0195 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2143/2143 - 37s - loss: 0.0207 - val_loss: 0.0188 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2143/2143 - 37s - loss: 0.0201 - val_loss: 0.0183 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2143/2143 - 36s - loss: 0.0196 - val_loss: 0.0178 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2143/2143 - 36s - loss: 0.0191 - val_loss: 0.0174 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2143/2143 - 36s - loss: 0.0186 - val_loss: 0.0170 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2143/2143 - 36s - loss: 0.0182 - val_loss: 0.0165 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2143/2143 - 36s - loss: 0.0178 - val_loss: 0.0162 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2143/2143 - 36s - loss: 0.0174 - val_loss: 0.0158 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2143/2143 - 36s - loss: 0.0170 - val_loss: 0.0156 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2143/2143 - 37s - loss: 0.0167 - val_loss: 0.0152 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2143/2143 - 36s - loss: 0.0164 - val_loss: 0.0150 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2143/2143 - 36s - loss: 0.0161 - val_loss: 0.0147 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2143/2143 - 36s - loss: 0.0158 - val_loss: 0.0145 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2143/2143 - 37s - loss: 0.0155 - val_loss: 0.0142 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2143/2143 - 37s - loss: 0.0153 - val_loss: 0.0140 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2143/2143 - 37s - loss: 0.0150 - val_loss: 0.0138 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2143/2143 - 37s - loss: 0.0148 - val_loss: 0.0136 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2143/2143 - 37s - loss: 0.0146 - val_loss: 0.0133 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2143/2143 - 37s - loss: 0.0144 - val_loss: 0.0132 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2143/2143 - 36s - loss: 0.0142 - val_loss: 0.0130 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2143/2143 - 36s - loss: 0.0140 - val_loss: 0.0129 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2143/2143 - 36s - loss: 0.0138 - val_loss: 0.0127 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2143/2143 - 37s - loss: 0.0136 - val_loss: 0.0125 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2143/2143 - 37s - loss: 0.0134 - val_loss: 0.0123 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2143/2143 - 37s - loss: 0.0133 - val_loss: 0.0121 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2143/2143 - 37s - loss: 0.0131 - val_loss: 0.0120 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2143/2143 - 37s - loss: 0.0130 - val_loss: 0.0119 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2143/2143 - 37s - loss: 0.0128 - val_loss: 0.0118 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2143/2143 - 37s - loss: 0.0127 - val_loss: 0.0116 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2143/2143 - 37s - loss: 0.0125 - val_loss: 0.0115 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2143/2143 - 37s - loss: 0.0124 - val_loss: 0.0113 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2143/2143 - 37s - loss: 0.0122 - val_loss: 0.0112 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2143/2143 - 37s - loss: 0.0121 - val_loss: 0.0111 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2143/2143 - 37s - loss: 0.0120 - val_loss: 0.0109 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2143/2143 - 38s - loss: 0.0119 - val_loss: 0.0108 - lr: 0.0100 - 38s/epoch - 18ms/step\n",
      "Epoch 49/2000\n",
      "2143/2143 - 37s - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2143/2143 - 37s - loss: 0.0116 - val_loss: 0.0106 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2143/2143 - 37s - loss: 0.0115 - val_loss: 0.0105 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2143/2143 - 36s - loss: 0.0114 - val_loss: 0.0104 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2143/2143 - 37s - loss: 0.0113 - val_loss: 0.0103 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2143/2143 - 37s - loss: 0.0112 - val_loss: 0.0102 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2143/2143 - 37s - loss: 0.0111 - val_loss: 0.0101 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2143/2143 - 37s - loss: 0.0110 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2143/2143 - 37s - loss: 0.0109 - val_loss: 0.0100 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2143/2143 - 37s - loss: 0.0108 - val_loss: 0.0099 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2143/2143 - 37s - loss: 0.0107 - val_loss: 0.0098 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2143/2143 - 37s - loss: 0.0107 - val_loss: 0.0097 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "2143/2143 - 37s - loss: 0.0106 - val_loss: 0.0096 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "2143/2143 - 36s - loss: 0.0105 - val_loss: 0.0095 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "2143/2143 - 37s - loss: 0.0104 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0095 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2143/2143 - 37s - loss: 0.0103 - val_loss: 0.0094 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2143/2143 - 37s - loss: 0.0102 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0093 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2143/2143 - 37s - loss: 0.0101 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2143/2143 - 37s - loss: 0.0100 - val_loss: 0.0091 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2143/2143 - 36s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2143/2143 - 37s - loss: 0.0099 - val_loss: 0.0090 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2143/2143 - 37s - loss: 0.0098 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0089 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2143/2143 - 37s - loss: 0.0097 - val_loss: 0.0088 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 76/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0087 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2143/2143 - 37s - loss: 0.0096 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2143/2143 - 37s - loss: 0.0095 - val_loss: 0.0086 - lr: 0.0100 - 37s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2143/2143 - 37s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2143/2143 - 36s - loss: 0.0094 - val_loss: 0.0085 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0085 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2143/2143 - 37s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 86/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 87/2000\n",
      "2143/2143 - 37s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0084 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0083 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2143/2143 - 37s - loss: 0.0091 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0082 - lr: 0.0100 - 37s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 0.0020 - 37s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0000e-04 - 37s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 8.0000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 103/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 37s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 3.2000e-06 - 37s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 6.4000e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.2800e-07 - 37s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 37s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 117/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 119/2000\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2143/2143 - 36s - loss: 0.0090 - val_loss: 0.0081 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "2143/2143 - 38s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 38s/epoch - 18ms/step\n",
      "Epoch 122/2000\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 1.0240e-09 - 37s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 2.0480e-10 - 37s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "Restoring model weights from the end of the best epoch: 106.\n",
      "2143/2143 - 37s - loss: 0.0090 - val_loss: 0.0081 - lr: 4.0960e-11 - 37s/epoch - 17ms/step\n",
      "Epoch 126: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_33_layer_call_fn, lstm_cell_33_layer_call_and_return_conditional_losses, lstm_cell_34_layer_call_fn, lstm_cell_34_layer_call_and_return_conditional_losses, lstm_cell_35_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/RealEstate/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/RealEstate/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2143/2143 [==============================] - 15s 7ms/step - loss: 0.0090\n",
      "536/536 [==============================] - 4s 7ms/step - loss: 0.0081\n",
      "\n",
      "\n",
      "Erro quadrático médio em treinamento: 0.00896\n",
      "\n",
      "Erro quadrático médio em Validação: 0.00812\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for nameSetor, setor in setores.items():\n",
    "    \n",
    "    X_treino, X_teste, y_treino, y_teste = Preprocessingdata(steps, SP500_close, setor)\n",
    "    model = create_model()\n",
    "    \n",
    "    model.fit(x = X_treino,\n",
    "              y = y_treino,\n",
    "              batch_size = batch_size,\n",
    "              epochs = epochs,\n",
    "              verbose = verbose,\n",
    "              validation_data = (X_teste, y_teste),\n",
    "              callbacks = callbacks)\n",
    "    \n",
    "    model.save('saveModel/{}/'.format(nameSetor),\n",
    "               overwrite=True,\n",
    "               include_optimizer=True,\n",
    "               save_format = 'tf')\n",
    "    \n",
    "    scoreTrain = model.evaluate(X_treino, y_treino)\n",
    "    scoreTest = model.evaluate(X_teste, y_teste)\n",
    "\n",
    "    print('\\n\\nErro quadrático médio em dados de treinamento: {:.5f}\\n\\nErro quadrático médio em dados de teste: {:.5f}\\n\\n'\\\n",
    "        .format(scoreTrain, scoreTest))\n",
    "    \n",
    "    fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nameSetor, setor in setores.items():\n",
    "       \n",
    "    model = load_model('saveModel/{}/'.format(nameSetor))\n",
    "    \n",
    "    fillPrediction(tableLogRet, tablePrevision, setor, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva tabela de previsão\n",
    "\n",
    "outdirP = './previsao/{}'.format(datetime.now().strftime('%d-%B-%Ih%Mmin'))\n",
    "\n",
    "if not os.path.exists(outdirP):\n",
    "    os.mkdir(outdirP)\n",
    "\n",
    "fullnameP = os.path.join(outdirP, 'previsao.csv')\n",
    "\n",
    "tablePrevision.to_csv(fullnameP, index = True, decimal = '.', sep=',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva tabela Entregável do Log-Retorno no padrão\n",
    "\n",
    "outdirLR = './logRetorno/{}'.format(datetime.now().strftime('%d-%B-%Ih%Mmin'))\n",
    "\n",
    "if not os.path.exists(outdirLR):\n",
    "    os.mkdir(outdirLR)\n",
    "\n",
    "fullnameLR = os.path.join(outdirLR, 'logRetorno.csv')\n",
    "\n",
    "tableLogRet.iloc[-len(forecast):, :].to_csv(fullnameLR, index = False, decimal = '.', sep=',')       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
