{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0424d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 1s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.sequence' has no attribute 'pad_sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 121>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_test), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest sequences\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPad sequences (samples x time)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequences\u001b[49m(x_train, maxlen\u001b[38;5;241m=\u001b[39mmaxlen)\n\u001b[0;32m    122\u001b[0m x_test \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mpad_sequences(x_test, maxlen\u001b[38;5;241m=\u001b[39mmaxlen)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, x_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.preprocessing.sequence' has no attribute 'pad_sequences'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import TensorBoard, Callback\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#import pdb\n",
    "\n",
    "class TensorBoardAUC(TensorBoard):\n",
    "\n",
    "    def __init__(self, log_dir='./logs',\n",
    "                 histogram_freq=0,\n",
    "                 batch_size=32,\n",
    "                 write_graph=True,\n",
    "                 write_grads=False,\n",
    "                 write_images=False,\n",
    "                 embeddings_freq=0,\n",
    "                 embeddings_layer_names=None,\n",
    "                 embeddings_metadata=None):\n",
    "        TensorBoard.__init__(self,log_dir=log_dir,\n",
    "                 histogram_freq=histogram_freq,\n",
    "                 batch_size=batch_size,\n",
    "                 write_graph=write_graph,\n",
    "                 write_grads=write_grads,\n",
    "                 write_images=write_images,\n",
    "                 embeddings_freq=embeddings_freq,\n",
    "                 embeddings_layer_names=embeddings_layer_names,\n",
    "                 embeddings_metadata=embeddings_metadata)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if not self.validation_data and self.histogram_freq:\n",
    "            raise ValueError('If printing histograms, validation_data must be '\n",
    "                             'provided, and cannot be a generator.')\n",
    "        if self.validation_data and self.histogram_freq:\n",
    "            if epoch % self.histogram_freq == 0:\n",
    "\n",
    "                val_data = self.validation_data\n",
    "                tensors = (self.model.inputs +\n",
    "                           self.model.targets +\n",
    "                           self.model.sample_weights)\n",
    "\n",
    "                if self.model.uses_learning_phase:\n",
    "                    tensors += [K.learning_phase()]\n",
    "\n",
    "                assert len(val_data) == len(tensors)\n",
    "                val_size = val_data[0].shape[0]\n",
    "                i = 0\n",
    "                while i < val_size:\n",
    "                    step = min(self.batch_size, val_size - i)\n",
    "                    if self.model.uses_learning_phase:\n",
    "                        # do not slice the learning phase\n",
    "                        batch_val = [x[i:i + step] for x in val_data[:-1]]\n",
    "                        batch_val.append(val_data[-1])\n",
    "                    else:\n",
    "                        batch_val = [x[i:i + step] for x in val_data]\n",
    "                    assert len(batch_val) == len(tensors)\n",
    "                    feed_dict = dict(zip(tensors, batch_val))\n",
    "                    result = self.sess.run([self.merged], feed_dict=feed_dict)\n",
    "                    summary_str = result[0]\n",
    "                    self.writer.add_summary(summary_str, epoch)\n",
    "                    i += self.batch_size\n",
    "\n",
    "        if self.embeddings_freq and self.embeddings_ckpt_path:\n",
    "            if epoch % self.embeddings_freq == 0:\n",
    "                self.saver.save(self.sess,\n",
    "                                self.embeddings_ckpt_path,\n",
    "                                epoch)\n",
    "\n",
    "\n",
    "        #quick hack        \n",
    "        b = {'val_auc':0.0}\n",
    "        logs = {**logs,**b}\n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            if name == 'val_auc':\n",
    "                X_val, y_val = self.validation_data[0], self.validation_data[1] \n",
    "                y_pred = self.model.predict_proba(X_val, verbose=0)\n",
    "                value = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(summary, epoch)\n",
    "\n",
    "        #val_auc summary\n",
    "        #summary = tf.Summary()\n",
    "        #summary_value = summary.value.add()\n",
    "\n",
    "        #X_val, y_val = self.validation_data[0], self.validation_data[1] \n",
    "        #y_pred = self.model.predict_proba(X_val, verbose=0)\n",
    "        #score = roc_auc_score(y_val, y_pred)\n",
    "        #summary_value.simple_value = score \n",
    "        #summary_value.tag = \"val_auc\"\n",
    "        #self.writer.add_summary(summary, epoch)\n",
    "\n",
    "        self.writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 256\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "TB = TensorBoardAUC(log_dir='./logs', histogram_freq=1, batch_size=batch_size, write_graph=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test),callbacks=[TB])\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f409e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
