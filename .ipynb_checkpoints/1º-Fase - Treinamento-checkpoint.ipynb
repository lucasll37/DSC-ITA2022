{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Challenge @ ITA 2022</font>\n",
    "# <font color='blue'>Equipe DIOMGIS</font>\n",
    "\n",
    "## <font color='blue'>1º Fase</font>\n",
    "\n",
    "### <font color='blue'>Predição de pregões futuros de ativos que compõem o índice SP500.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data\\image\\logo.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas e Frameworks\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorboard import notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Equipe DIOMGIS\n",
      "\n",
      "pandas           : 1.4.2\n",
      "pandas_datareader: 0.10.0\n",
      "tensorflow       : 2.10.0\n",
      "seaborn          : 0.11.2\n",
      "keras            : 2.10.0\n",
      "tensorboard      : 2.10.0\n",
      "matplotlib       : 3.5.1\n",
      "numpy            : 1.22.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Equipe DIOMGIS\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros fixos\n",
    "\n",
    "verbose = 2\n",
    "seed = 25\n",
    "steps = 30\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "logRetPeriod = 20\n",
    "graphic = False\n",
    "downloadData = False\n",
    "trainModel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = ['2022-10-24', '2022-10-25', '2022-10-26', '2022-10-27', '2022-10-28', \n",
    "            '2022-10-31', '2022-11-01', '2022-11-02', '2022-11-03', '2022-11-04', \n",
    "            '2022-11-07', '2022-11-08', '2022-11-09', '2022-11-10', '2022-11-11',\n",
    "            '2022-11-14', '2022-11-15', '2022-11-16', '2022-11-17', '2022-11-18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Confirma se o TensorFlow pode acessar a GPU\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 19 13:39:24 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.94       Driver Version: 516.94       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:65:00.0  On |                  N/A |\n",
      "|  0%   39C    P2    27W / 220W |   1185MiB /  8192MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       732    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      2592    C+G   ...txyewy\\MiniSearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3516    C+G   ...o Webcam\\GoPro Webcam.exe    N/A      |\n",
      "|    0   N/A  N/A     10752      C   ...ucas\\anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     12876    C+G   ...bat\\acrocef_2\\AcroCEF.exe    N/A      |\n",
      "|    0   N/A  N/A     13256    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13784    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14424    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14448    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15244    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     16788    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     17300    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     20344    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20628    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20920    C+G   ...\\app-1.0.9006\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     20992    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     21560    C+G   ...persky VPN 5.7\\ksdeui.exe    N/A      |\n",
      "|    0   N/A  N/A     22768    C+G   ...370.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     22980    C+G   ...x64__pc75e8sa7ep4e\\XD.exe    N/A      |\n",
      "|    0   N/A  N/A     30316    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     30600    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     40876    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     44408    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     45908    C+G   ...370.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     47076    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Estado da GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setores = {\n",
    "    'Industrials': ['MMM', 'AOS', 'ALK', 'ALLE', 'AAL', 'AME', 'BA',\n",
    "                    'CHRW', 'CARR', 'CAT', 'CTAS', 'CPRT', 'CSGP',\n",
    "                    'CSX', 'CMI', 'DE', 'DAL', 'DOV', 'ETN', 'EMR',\n",
    "                    'EFX', 'EXPD', 'FAST', 'FDX', 'FTV', 'FBHS',\n",
    "                    'GNRC', 'GD', 'GE', 'HON', 'HWM', 'HII', 'IEX',\n",
    "                    'ITW', 'IR', 'JBHT', 'J', 'JCI', 'LHX', 'LDOS',\n",
    "                    'LMT', 'MAS', 'NDSN', 'NSC', 'NOC', 'ODFL',\n",
    "                    'OTIS', 'PCAR', 'PH', 'PNR', 'PWR', 'RTX', 'RSG',\n",
    "                    'RHI', 'ROK', 'ROL', 'SNA', 'LUV', 'SWK', 'TXT',\n",
    "                    'TT', 'TDG', 'UNP', 'UAL', 'UPS', 'URI', 'VRSK',\n",
    "                    'WAB', 'WM', 'GWW', 'XYL'],\n",
    "    \n",
    "    'HealthCare': ['ABT', 'ABBV', 'ABMD', 'A', 'ALGN', 'ABC', 'AMGN',\n",
    "                   'BAX', 'BDX', 'BIO', 'TECH', 'BIIB', 'BSX', 'BMY',\n",
    "                   'CAH', 'CTLT', 'CNC', 'CRL', 'CI', 'COO', 'CVS',\n",
    "                   'DHR', 'DVA', 'XRAY', 'DXCM', 'EW', 'ELV', 'LLY',\n",
    "                   'GILD', 'HCA', 'HSIC', 'HOLX', 'HUM', 'IDXX',\n",
    "                   'ILMN', 'INCY', 'ISRG', 'IQV', 'JNJ', 'LH', 'MCK',\n",
    "                   'MDT', 'MRK', 'MTD', 'MRNA', 'MOH', 'OGN', 'PKI',\n",
    "                   'PFE', 'DGX', 'REGN', 'RMD', 'STE', 'SYK', 'TFX',\n",
    "                   'TMO', 'UNH', 'UHS', 'VRTX', 'VTRS', 'WAT',\n",
    "                   'WST', 'ZBH', 'ZTS'],\n",
    "    \n",
    "    'InformationTechnology': ['ACN', 'ADBE', 'ADP', 'AKAM', 'AMD', 'APH',\n",
    "                              'ADI', 'ANSS', 'AAPL', 'AMAT', 'ANET', 'ADSK',\n",
    "                              'AVGO', 'BR', 'CDNS', 'CDW', 'CDAY', 'CSCO',\n",
    "                              'CTSH', 'GLW', 'DXC', 'ENPH', 'EPAM', 'FFIV',\n",
    "                              'FIS', 'FISV', 'FLT', 'FTNT', 'IT', 'GPN',\n",
    "                              'HPE', 'HPQ', 'IBM', 'INTC', 'INTU', 'JKHY',\n",
    "                              'JNPR', 'KEYS', 'KLAC', 'LRCX', 'MA', 'MCHP',\n",
    "                              'MU', 'MSFT', 'MPWR', 'MSI', 'NTAP', 'NLOK',\n",
    "                              'NVDA', 'NXPI', 'ON', 'ORCL', 'PAYX', 'PAYC',\n",
    "                              'PYPL', 'PTC', 'QRVO', 'QCOM', 'ROP', 'CRM',\n",
    "                              'STX', 'NOW', 'SWKS', 'SEDG', 'SNPS', 'TEL',\n",
    "                              'TDY', 'TER', 'TXN', 'TRMB', 'TYL', 'VRSN',\n",
    "                              'V', 'WDC', 'ZBRA'],\n",
    "    \n",
    "    'CommunicationServices': ['ATVI', 'GOOGL', 'GOOG', 'T', 'CHTR', 'CMCSA',\n",
    "                              'DISH', 'DIS', 'EA', 'FOXA', 'FOX', 'IPG', 'LYV',\n",
    "                              'LUMN', 'MTCH', 'META', 'NFLX', 'NWSA', 'NWS',\n",
    "                              'OMC', 'PARA', 'TMUS', 'TTWO', 'TWTR', 'VZ', 'WBD'],\n",
    "    \n",
    "    'ConsumerStaples': ['ADM', 'MO', 'BF.B', 'CPB', 'CHD', 'CLX', 'KO', 'CL',\n",
    "                        'CAG', 'STZ', 'COST', 'EL', 'GIS', 'HSY', 'HRL', 'K',\n",
    "                        'KDP', 'KMB', 'KHC', 'KR', 'LW', 'MKC', 'TAP', 'MDLZ',\n",
    "                        'MNST', 'PEP', 'PM', 'PG', 'SJM', 'SYY', 'TSN', 'WBA',\n",
    "                        'WMT'],\n",
    "    \n",
    "    'ConsumerDiscretionary': ['AAP', 'AMZN', 'APTV', 'AZO', 'BBWI',\n",
    "                              'BBY', 'BKNG', 'BWA', 'CZR', 'KMX', 'CCL',\n",
    "                              'CMG', 'DHI', 'DRI', 'DG', 'DLTR',\n",
    "                              'DPZ', 'EBAY', 'ETSY', 'EXPE', 'F',\n",
    "                              'GRMN', 'GM', 'GPC', 'HAS', 'HLT', 'HD',\n",
    "                              'LVS', 'LEN', 'LKQ', 'LOW', 'MAR', 'MCD',\n",
    "                              'MGM', 'MHK', 'NWL', 'NKE', 'NCLH', 'NVR',\n",
    "                              'ORLY', 'POOL', 'PHM', 'RL', 'ROST', 'RCL',\n",
    "                              'SBUX', 'TPR', 'TGT', 'TSLA', 'TJX',\n",
    "                              'TSCO', 'ULTA', 'VFC', 'WHR', 'WYNN', 'YUM'],\n",
    "    \n",
    "    'Utilities': ['AES', 'LNT', 'AEE', 'AEP', 'AWK', 'ATO', 'CNP',\n",
    "                  'CMS', 'ED', 'CEG', 'D', 'DTE', 'DUK', 'EIX',\n",
    "                  'ETR', 'EVRG', 'ES', 'EXC', 'FE', 'NEE', 'NI',\n",
    "                  'NRG', 'PCG', 'PNW', 'PPL', 'PEG', 'SRE', 'SO',\n",
    "                  'WEC', 'XEL'],\n",
    "    \n",
    "    'Financials': ['AFL', 'ALL', 'AXP', 'AIG', 'AMP', 'AON', 'AJG',\n",
    "                   'AIZ', 'BAC', 'WRB', 'BRK.B', 'BLK', 'BK', 'BRO',\n",
    "                   'COF', 'CBOE', 'SCHW', 'CB', 'CINF', 'C', 'CFG',\n",
    "                   'CME', 'CMA', 'DFS', 'RE', 'FDS', 'FITB', 'FRC',\n",
    "                   'BEN', 'GL', 'GS', 'HIG', 'HBAN', 'ICE', 'IVZ',\n",
    "                   'JPM', 'KEY', 'LNC', 'L', 'MTB', 'MKTX', 'MMC',\n",
    "                   'MET', 'MCO', 'MS', 'MSCI', 'NDAQ', 'NTRS', 'PNC',\n",
    "                   'PFG', 'PGR', 'PRU', 'RJF', 'RF', 'SPGI', 'SBNY',\n",
    "                   'STT', 'SIVB', 'SYF', 'TROW', 'TRV', 'TFC',\n",
    "                   'USB', 'WFC', 'WTW', 'ZION',  'NLSN'],\n",
    "    \n",
    "    'Materials': ['APD', 'ALB', 'AMCR', 'AVY', 'BALL', 'CE', 'CF',\n",
    "                  'CTVA', 'DOW', 'DD', 'EMN', 'ECL', 'FMC', 'FCX',\n",
    "                  'IP', 'IFF', 'LIN', 'LYB', 'MLM', 'MOS', 'NEM',\n",
    "                  'NUE', 'PKG', 'PPG', 'SEE', 'SHW', 'VMC', 'WRK'],\n",
    "    \n",
    "    'RealEstate': ['ARE', 'AMT', 'AVB', 'BXP', 'CPT', 'CBRE', 'CCI',\n",
    "                   'DLR', 'EQIX', 'EQR', 'ESS', 'EXR', 'FRT', 'PEAK',\n",
    "                   'HST', 'INVH', 'IRM', 'KIM', 'MAA', 'PLD', 'PSA',\n",
    "                   'O', 'REG', 'SBAC', 'SPG', 'UDR', 'VTR', 'VICI',\n",
    "                   'VNO', 'WELL', 'WY'],\n",
    "    \n",
    "    'Energy': ['APA','BKR', 'CVX', 'COP', 'CTRA', 'DVN', 'FANG', 'EOG',\n",
    "               'EQT', 'XOM', 'HAL', 'HES', 'KMI', 'MRO', 'MPC', 'OXY',\n",
    "               'OKE', 'PSX', 'PXD', 'SLB', 'VLO', 'WMB']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ativos = []\n",
    "for setor, empresas in setores.items():\n",
    "    ativos.extend(empresas)\n",
    "    \n",
    "ativos.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if downloadData:\n",
    "\n",
    "    start_date = \"2017-10-21\"\n",
    "    end_date = \"2022-10-21\"\n",
    "\n",
    "    data = web.DataReader(name = '^GSPC', data_source = 'yahoo', start = start_date, end = end_date)\n",
    "    SP500_index = pd.DataFrame(data['Close']).reset_index().rename(columns={'Close': 'SP500', 'Date': 'Dia'})\n",
    "\n",
    "    SP500_close = pd.DataFrame()\n",
    "\n",
    "    for ativo in ativos:\n",
    "  \n",
    "        if ativo == 'BF.B':\n",
    "            ativo = 'BF-B'\n",
    "\n",
    "        if ativo == 'BRK.B':\n",
    "            ativo = 'BRK-B'\n",
    "\n",
    "        data = web.DataReader(name = ativo, data_source = 'yahoo', start = start_date, end = end_date)\n",
    "        temp_close = pd.DataFrame(data['Close'])\n",
    "        SP500_close = pd.concat([SP500_close, temp_close], axis = 1)\n",
    "\n",
    "        \n",
    "    SP500_close.columns = ativos\n",
    "    SP500_close.reset_index(inplace = True)\n",
    "    SP500_close.rename(columns={'Date': 'Dia'}, inplace = True)\n",
    "\n",
    "    time = datetime.now().strftime('%d-%B-%Ih%Mmin')\n",
    "    outdir = './data/{}'.format(time)\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    SP500_close.to_csv(path_or_buf = os.path.join(outdir, 'SP500_close'), index = False)\n",
    "    SP500_index.to_csv(path_or_buf = os.path.join(outdir, 'SP500_index'), index = False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    SP500_close = pd.read_csv('data/2017-10-21-2022-10-21/SP500_close')\n",
    "    SP500_index = pd.read_csv('data/2017-10-21-2022-10-21/SP500_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorTimeframeTable(table, ativo):\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "    \n",
    "    TimeframeTable = pd.DataFrame(np.zeros((len(table[ativo])-steps, steps+1), dtype='float64'), columns = nameColumns)\n",
    "\n",
    "    for index, close in enumerate(table[ativo]):\n",
    "        tempA = index\n",
    "        tempB = 0\n",
    "        for i in range(steps+1):\n",
    "            if tempA < len(table[ativo])-steps and tempA >=0:\n",
    "                TimeframeTable.iloc[tempA, tempB] = close\n",
    "\n",
    "            tempA -= 1\n",
    "            tempB += 1\n",
    "\n",
    "    timeIndex = table.iloc[steps:,0]\n",
    "    TimeframeTable[\"Dia\"] = timeIndex.to_numpy()\n",
    "    TimeframeTable.set_index(\"Dia\", inplace = True)\n",
    "    \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainScaler(df):\n",
    "    \n",
    "    trainScaler = pd.DataFrame()\n",
    " \n",
    "    for _ in range(steps+1):\n",
    "        temp_close = pd.DataFrame(df.iloc[:,-1])\n",
    "        trainScaler = pd.concat([trainScaler, temp_close], axis = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(trainScaler)\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingdata(steps, df, ativos):\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "    \n",
    "\n",
    "    aux = []\n",
    "    \n",
    "    for ativo in ativos:\n",
    "        trainDataAtivo = generatorTimeframeTable(df, ativo)\n",
    "        trainDataAtivo.dropna(axis = 0, inplace = True)\n",
    "        \n",
    "        #----Score-Z--------------------------------------\n",
    "        scaler = createTrainScaler(trainDataAtivo)\n",
    "        trainDataAtivo = scaler.transform(trainDataAtivo)\n",
    "        #-------------------------------------------------\n",
    "        aux.append(trainDataAtivo)\n",
    "    \n",
    "    trainData = np.concatenate(tuple(aux), axis=0)\n",
    "    \n",
    "    X = trainData[:, :-1]\n",
    "    y = trainData[:, -1]\n",
    "    \n",
    "\n",
    "    #------Divisão de dados entre Treino e Validação------------------------------------------------\n",
    "    \n",
    "    X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "\n",
    "    X_treino = X_treino.reshape((-1, steps, 1))\n",
    "    X_teste = X_teste.reshape((-1, steps, 1))\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return [X_treino, X_teste, y_treino, y_teste]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção, Treinamento e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime('%d-%B-%Ih%Mmin')))\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=20,\n",
    "                          verbose = verbose,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              mode=\"min\",\n",
    "                              verbose = verbose,\n",
    "                              min_delta=0.0001,\n",
    "                              min_lr=0)\n",
    "\n",
    "callbacks = [tensorboard, earlystop, reduce_lr, TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = True,\n",
    "                   input_shape = (steps, 1)))  \n",
    "\n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = True))  \n",
    "    \n",
    "    model.add(LSTM(160,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = False)) \n",
    "    \n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    Lmse = MeanSquaredError()\n",
    "    \n",
    "    opt_Adadelta = Adadelta(learning_rate = 0.01, rho = 0.95, epsilon = 1e-07)\n",
    "\n",
    "    model.compile(loss= Lmse, optimizer = opt_Adadelta)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTableFrame(ativo, tablePrevision, model, table = SP500_close):\n",
    "\n",
    "    TimeframeTable = generatorTimeframeTable(table, ativo)\n",
    "    \n",
    "    index_data = TimeframeTable.index\n",
    "    \n",
    "    scaler = createTrainScaler(TimeframeTable)\n",
    "\n",
    "    TimeframeTable = scaler.transform(TimeframeTable)\n",
    "    \n",
    "    nameColumns = []\n",
    "\n",
    "    for i in range(steps,-1,-1):\n",
    "        nameColumns.append('Close-{}'.format(i))\n",
    "\n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    \n",
    "    for day in forecast:\n",
    "        \n",
    "        current_info = TimeframeTable.iloc[-1, 1:].to_numpy()\n",
    "        \n",
    "        standardCurrentInfo = current_info.reshape(1, steps, 1).astype('float32')\n",
    "        \n",
    "        current_forecast = model.predict(standardCurrentInfo, verbose=False).reshape(1,)\n",
    "        \n",
    "        new_line = np.concatenate((current_info, current_forecast), axis = 0)\n",
    "        \n",
    "        TimeframeTable = pd.concat([TimeframeTable,\n",
    "                                    pd.DataFrame(new_line.reshape(1, -1),\n",
    "                                                 columns = nameColumns,\n",
    "                                                 index = [day])], axis = 0)\n",
    "        \n",
    "        \n",
    "    index_data = TimeframeTable.index  \n",
    "    \n",
    "    TimeframeTable = scaler.inverse_transform(TimeframeTable)\n",
    "    \n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    TimeframeTable.index = pd.to_datetime(TimeframeTable.index)\n",
    "    \n",
    "    \n",
    "    #--------Popula tabela de previsão---------------------------------------------------\n",
    "    if ativo in ativos:\n",
    "            for day in forecast:\n",
    "                tablePrevision.loc[day, ativo] = TimeframeTable.loc[day, 'Close-0']\n",
    "    #------------------------------------------------------------------------------------\n",
    "   \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popula tabela de log-Retorno, Previsão de Preços e gera gráficos\n",
    "\n",
    "def fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model):\n",
    "    \n",
    "    lengthTable = len(tableLogRet)\n",
    "    \n",
    "    time = datetime.now().strftime('%d-%B-%Ih%Mmin')\n",
    "\n",
    "    for ativo in setor:\n",
    "\n",
    "        TimeframeSPAux = fillTableFrame(ativo, tablePrevision, model)\n",
    "\n",
    "        #-----------Graphic------------------------------------------------------------------------------------------\n",
    "        if graphic:\n",
    "            \n",
    "            outdir = './graphics/{}-{}'.format(nameSetor, time)\n",
    "\n",
    "            if not os.path.exists(outdir):\n",
    "                os.mkdir(outdir)\n",
    "            \n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(TimeframeSPAux.index[:-len(forecast)], TimeframeSPAux.iloc[:-len(forecast), -1], linewidth=3.0, c = 'b')\n",
    "            ax.plot(TimeframeSPAux.index[-len(forecast):], TimeframeSPAux.iloc[-len(forecast):, -1], linewidth=3.0, c = 'c', ls = '-')\n",
    "            ax.legend(['Atual', 'Previsão'])\n",
    "            ax.set_title('Preço de Fechamento - {}'.format(ativo))\n",
    "            ax.set(xlabel='Tempo (ano)', ylabel='Preço ($)')\n",
    "            nameGraphic = '{}.jpg'.format(ativo)\n",
    "            fullname = os.path.join(outdir, nameGraphic)\n",
    "            plt.savefig(fullname)\n",
    "            plt.close(fig)\n",
    "        #------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        #--------Popula tabela de Log Retorno------------------------------------------------------------------------\n",
    "        for n in range(len(forecast)):\n",
    "            tableLogRet.loc[lengthTable-n-1, ativo] = \\\n",
    "            np.log(TimeframeSPAux.iloc[lengthTable-steps-n-1, -1] / TimeframeSPAux.iloc[lengthTable-steps-n-1-logRetPeriod, -1])\n",
    "        #------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela de Previsão\n",
    "\n",
    "update = pd.DataFrame(index = pd.to_datetime(forecast), columns = ativos) \\\n",
    "    .reset_index().rename(columns={'index': 'Dia'})\n",
    "\n",
    "tablePrevision = pd.concat([SP500_close, update], axis = 0, ignore_index = True).set_index('Dia')\n",
    "\n",
    "tablePrevision.index = pd.to_datetime(tablePrevision.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela de Log-Retorno vazia \n",
    "\n",
    "index_data = pd.to_datetime(SP500_close['Dia'].append(pd.Series(forecast)))\n",
    "\n",
    "tableLogRet = pd.DataFrame(index = index_data,\n",
    "                           columns = ativos).reset_index().rename(columns={'index': 'Dia'})\n",
    "\n",
    "tableLogRet['Dia'] = tableLogRet['Dia'].apply(lambda date: date.strftime('%d/%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "675/675 - 22s - loss: 0.1800 - val_loss: 0.0889 - lr: 0.0100 - 22s/epoch - 33ms/step\n",
      "Epoch 2/2000\n",
      "675/675 - 12s - loss: 0.0758 - val_loss: 0.0667 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 3/2000\n",
      "675/675 - 12s - loss: 0.0601 - val_loss: 0.0570 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "675/675 - 12s - loss: 0.0532 - val_loss: 0.0523 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "675/675 - 12s - loss: 0.0498 - val_loss: 0.0487 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "675/675 - 12s - loss: 0.0472 - val_loss: 0.0467 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 7/2000\n",
      "675/675 - 12s - loss: 0.0449 - val_loss: 0.0438 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 8/2000\n",
      "675/675 - 12s - loss: 0.0428 - val_loss: 0.0418 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 9/2000\n",
      "675/675 - 12s - loss: 0.0409 - val_loss: 0.0399 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 10/2000\n",
      "675/675 - 12s - loss: 0.0392 - val_loss: 0.0385 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 11/2000\n",
      "675/675 - 12s - loss: 0.0376 - val_loss: 0.0373 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 12/2000\n",
      "675/675 - 12s - loss: 0.0362 - val_loss: 0.0353 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "675/675 - 12s - loss: 0.0349 - val_loss: 0.0341 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "675/675 - 12s - loss: 0.0337 - val_loss: 0.0332 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "675/675 - 12s - loss: 0.0329 - val_loss: 0.0321 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "675/675 - 12s - loss: 0.0320 - val_loss: 0.0316 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "675/675 - 12s - loss: 0.0312 - val_loss: 0.0307 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "675/675 - 12s - loss: 0.0305 - val_loss: 0.0300 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "675/675 - 12s - loss: 0.0299 - val_loss: 0.0294 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "675/675 - 12s - loss: 0.0293 - val_loss: 0.0288 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "675/675 - 12s - loss: 0.0288 - val_loss: 0.0283 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 22/2000\n",
      "675/675 - 12s - loss: 0.0283 - val_loss: 0.0278 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 23/2000\n",
      "675/675 - 12s - loss: 0.0279 - val_loss: 0.0274 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "675/675 - 12s - loss: 0.0274 - val_loss: 0.0269 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "675/675 - 12s - loss: 0.0269 - val_loss: 0.0265 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "675/675 - 12s - loss: 0.0265 - val_loss: 0.0265 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "675/675 - 12s - loss: 0.0262 - val_loss: 0.0257 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "675/675 - 12s - loss: 0.0258 - val_loss: 0.0253 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 29/2000\n",
      "675/675 - 12s - loss: 0.0254 - val_loss: 0.0250 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "675/675 - 12s - loss: 0.0251 - val_loss: 0.0247 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "675/675 - 11s - loss: 0.0247 - val_loss: 0.0245 - lr: 0.0100 - 11s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "675/675 - 12s - loss: 0.0244 - val_loss: 0.0240 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "675/675 - 12s - loss: 0.0241 - val_loss: 0.0238 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "675/675 - 12s - loss: 0.0238 - val_loss: 0.0235 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "675/675 - 12s - loss: 0.0235 - val_loss: 0.0231 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 36/2000\n",
      "675/675 - 12s - loss: 0.0232 - val_loss: 0.0228 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "675/675 - 12s - loss: 0.0229 - val_loss: 0.0225 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "675/675 - 12s - loss: 0.0226 - val_loss: 0.0223 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "675/675 - 12s - loss: 0.0224 - val_loss: 0.0222 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "675/675 - 12s - loss: 0.0221 - val_loss: 0.0219 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 41/2000\n",
      "675/675 - 12s - loss: 0.0219 - val_loss: 0.0216 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "675/675 - 12s - loss: 0.0216 - val_loss: 0.0213 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "675/675 - 12s - loss: 0.0214 - val_loss: 0.0211 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "675/675 - 12s - loss: 0.0211 - val_loss: 0.0208 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "675/675 - 12s - loss: 0.0209 - val_loss: 0.0207 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 46/2000\n",
      "675/675 - 12s - loss: 0.0207 - val_loss: 0.0205 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "675/675 - 12s - loss: 0.0205 - val_loss: 0.0205 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "675/675 - 12s - loss: 0.0203 - val_loss: 0.0201 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 49/2000\n",
      "675/675 - 12s - loss: 0.0201 - val_loss: 0.0198 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "675/675 - 12s - loss: 0.0199 - val_loss: 0.0196 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 51/2000\n",
      "675/675 - 12s - loss: 0.0197 - val_loss: 0.0194 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 52/2000\n",
      "675/675 - 12s - loss: 0.0195 - val_loss: 0.0192 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 53/2000\n",
      "675/675 - 12s - loss: 0.0193 - val_loss: 0.0191 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 54/2000\n",
      "675/675 - 11s - loss: 0.0191 - val_loss: 0.0189 - lr: 0.0100 - 11s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "675/675 - 11s - loss: 0.0190 - val_loss: 0.0187 - lr: 0.0100 - 11s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "675/675 - 12s - loss: 0.0188 - val_loss: 0.0187 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 57/2000\n",
      "675/675 - 12s - loss: 0.0186 - val_loss: 0.0184 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 58/2000\n",
      "675/675 - 12s - loss: 0.0185 - val_loss: 0.0184 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "675/675 - 12s - loss: 0.0183 - val_loss: 0.0181 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "675/675 - 12s - loss: 0.0182 - val_loss: 0.0180 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 61/2000\n",
      "675/675 - 12s - loss: 0.0180 - val_loss: 0.0179 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 62/2000\n",
      "675/675 - 12s - loss: 0.0179 - val_loss: 0.0178 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 63/2000\n",
      "675/675 - 12s - loss: 0.0177 - val_loss: 0.0176 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 64/2000\n",
      "675/675 - 12s - loss: 0.0176 - val_loss: 0.0174 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "675/675 - 11s - loss: 0.0175 - val_loss: 0.0179 - lr: 0.0100 - 11s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "675/675 - 11s - loss: 0.0173 - val_loss: 0.0172 - lr: 0.0100 - 11s/epoch - 17ms/step\n",
      "Epoch 67/2000\n",
      "675/675 - 11s - loss: 0.0172 - val_loss: 0.0170 - lr: 0.0100 - 11s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "675/675 - 12s - loss: 0.0171 - val_loss: 0.0169 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "675/675 - 12s - loss: 0.0169 - val_loss: 0.0170 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 70/2000\n",
      "675/675 - 12s - loss: 0.0168 - val_loss: 0.0167 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 71/2000\n",
      "675/675 - 12s - loss: 0.0167 - val_loss: 0.0166 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "675/675 - 12s - loss: 0.0166 - val_loss: 0.0165 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "675/675 - 12s - loss: 0.0165 - val_loss: 0.0164 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "675/675 - 12s - loss: 0.0164 - val_loss: 0.0163 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "675/675 - 12s - loss: 0.0163 - val_loss: 0.0162 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 76/2000\n",
      "675/675 - 12s - loss: 0.0162 - val_loss: 0.0161 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 77/2000\n",
      "675/675 - 12s - loss: 0.0161 - val_loss: 0.0161 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "675/675 - 12s - loss: 0.0160 - val_loss: 0.0160 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "675/675 - 12s - loss: 0.0159 - val_loss: 0.0158 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 80/2000\n",
      "675/675 - 12s - loss: 0.0158 - val_loss: 0.0157 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "675/675 - 12s - loss: 0.0157 - val_loss: 0.0156 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "675/675 - 12s - loss: 0.0156 - val_loss: 0.0155 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 83/2000\n",
      "675/675 - 12s - loss: 0.0155 - val_loss: 0.0154 - lr: 0.0100 - 12s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/2000\n",
      "675/675 - 12s - loss: 0.0154 - val_loss: 0.0154 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 85/2000\n",
      "675/675 - 12s - loss: 0.0153 - val_loss: 0.0154 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 86/2000\n",
      "675/675 - 12s - loss: 0.0152 - val_loss: 0.0153 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 87/2000\n",
      "675/675 - 12s - loss: 0.0151 - val_loss: 0.0152 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 88/2000\n",
      "675/675 - 12s - loss: 0.0151 - val_loss: 0.0151 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "675/675 - 12s - loss: 0.0150 - val_loss: 0.0150 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "675/675 - 12s - loss: 0.0149 - val_loss: 0.0149 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 91/2000\n",
      "675/675 - 12s - loss: 0.0148 - val_loss: 0.0149 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "675/675 - 12s - loss: 0.0147 - val_loss: 0.0147 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "675/675 - 12s - loss: 0.0146 - val_loss: 0.0148 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 94/2000\n",
      "675/675 - 12s - loss: 0.0146 - val_loss: 0.0147 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "675/675 - 12s - loss: 0.0145 - val_loss: 0.0145 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 96/2000\n",
      "675/675 - 12s - loss: 0.0144 - val_loss: 0.0146 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "675/675 - 12s - loss: 0.0144 - val_loss: 0.0144 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 98/2000\n",
      "675/675 - 12s - loss: 0.0143 - val_loss: 0.0143 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 99/2000\n",
      "675/675 - 12s - loss: 0.0142 - val_loss: 0.0142 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 100/2000\n",
      "675/675 - 12s - loss: 0.0142 - val_loss: 0.0142 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 101/2000\n",
      "675/675 - 12s - loss: 0.0141 - val_loss: 0.0141 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "675/675 - 12s - loss: 0.0140 - val_loss: 0.0140 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 103/2000\n",
      "675/675 - 12s - loss: 0.0139 - val_loss: 0.0140 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "675/675 - 12s - loss: 0.0139 - val_loss: 0.0140 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 105/2000\n",
      "675/675 - 12s - loss: 0.0138 - val_loss: 0.0139 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 106/2000\n",
      "675/675 - 12s - loss: 0.0137 - val_loss: 0.0138 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 107/2000\n",
      "675/675 - 12s - loss: 0.0137 - val_loss: 0.0138 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 108/2000\n",
      "675/675 - 12s - loss: 0.0136 - val_loss: 0.0137 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 109/2000\n",
      "675/675 - 12s - loss: 0.0136 - val_loss: 0.0136 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 110/2000\n",
      "675/675 - 12s - loss: 0.0135 - val_loss: 0.0136 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "675/675 - 12s - loss: 0.0134 - val_loss: 0.0135 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 112/2000\n",
      "675/675 - 12s - loss: 0.0134 - val_loss: 0.0135 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 113/2000\n",
      "675/675 - 12s - loss: 0.0133 - val_loss: 0.0134 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 114/2000\n",
      "675/675 - 12s - loss: 0.0133 - val_loss: 0.0133 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 115/2000\n",
      "675/675 - 12s - loss: 0.0132 - val_loss: 0.0133 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 116/2000\n",
      "675/675 - 12s - loss: 0.0132 - val_loss: 0.0133 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 117/2000\n",
      "675/675 - 12s - loss: 0.0131 - val_loss: 0.0132 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 118/2000\n",
      "675/675 - 12s - loss: 0.0130 - val_loss: 0.0131 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 119/2000\n",
      "675/675 - 12s - loss: 0.0130 - val_loss: 0.0131 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "675/675 - 12s - loss: 0.0129 - val_loss: 0.0131 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 121/2000\n",
      "675/675 - 12s - loss: 0.0129 - val_loss: 0.0130 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 122/2000\n",
      "675/675 - 12s - loss: 0.0128 - val_loss: 0.0130 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 123/2000\n",
      "675/675 - 12s - loss: 0.0128 - val_loss: 0.0129 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 124/2000\n",
      "675/675 - 12s - loss: 0.0127 - val_loss: 0.0129 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 125/2000\n",
      "675/675 - 13s - loss: 0.0127 - val_loss: 0.0128 - lr: 0.0100 - 13s/epoch - 19ms/step\n",
      "Epoch 126/2000\n",
      "675/675 - 12s - loss: 0.0126 - val_loss: 0.0127 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 127/2000\n",
      "675/675 - 12s - loss: 0.0126 - val_loss: 0.0128 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 128/2000\n",
      "675/675 - 12s - loss: 0.0125 - val_loss: 0.0127 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 129/2000\n",
      "675/675 - 12s - loss: 0.0125 - val_loss: 0.0127 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 130/2000\n",
      "675/675 - 12s - loss: 0.0124 - val_loss: 0.0126 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 131/2000\n",
      "675/675 - 12s - loss: 0.0124 - val_loss: 0.0125 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 132/2000\n",
      "675/675 - 12s - loss: 0.0124 - val_loss: 0.0125 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 133/2000\n",
      "675/675 - 12s - loss: 0.0123 - val_loss: 0.0124 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 134/2000\n",
      "675/675 - 12s - loss: 0.0123 - val_loss: 0.0124 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 135/2000\n",
      "675/675 - 12s - loss: 0.0122 - val_loss: 0.0123 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 136/2000\n",
      "675/675 - 12s - loss: 0.0122 - val_loss: 0.0123 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 137/2000\n",
      "675/675 - 12s - loss: 0.0121 - val_loss: 0.0123 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 138/2000\n",
      "675/675 - 12s - loss: 0.0121 - val_loss: 0.0122 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 139/2000\n",
      "675/675 - 12s - loss: 0.0120 - val_loss: 0.0122 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 140/2000\n",
      "675/675 - 12s - loss: 0.0120 - val_loss: 0.0122 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 141/2000\n",
      "675/675 - 12s - loss: 0.0120 - val_loss: 0.0121 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 142/2000\n",
      "675/675 - 12s - loss: 0.0119 - val_loss: 0.0121 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 143/2000\n",
      "675/675 - 12s - loss: 0.0119 - val_loss: 0.0120 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 144/2000\n",
      "675/675 - 12s - loss: 0.0119 - val_loss: 0.0120 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 145/2000\n",
      "675/675 - 11s - loss: 0.0118 - val_loss: 0.0120 - lr: 0.0100 - 11s/epoch - 17ms/step\n",
      "Epoch 146/2000\n",
      "675/675 - 12s - loss: 0.0118 - val_loss: 0.0119 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 147/2000\n",
      "675/675 - 12s - loss: 0.0117 - val_loss: 0.0119 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 148/2000\n",
      "675/675 - 12s - loss: 0.0117 - val_loss: 0.0119 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 149/2000\n",
      "675/675 - 12s - loss: 0.0117 - val_loss: 0.0118 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 150/2000\n",
      "675/675 - 14s - loss: 0.0116 - val_loss: 0.0118 - lr: 0.0100 - 14s/epoch - 21ms/step\n",
      "Epoch 151/2000\n",
      "675/675 - 12s - loss: 0.0116 - val_loss: 0.0117 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 152/2000\n",
      "675/675 - 12s - loss: 0.0116 - val_loss: 0.0117 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 153/2000\n",
      "675/675 - 13s - loss: 0.0115 - val_loss: 0.0117 - lr: 0.0100 - 13s/epoch - 19ms/step\n",
      "Epoch 154/2000\n",
      "675/675 - 17s - loss: 0.0115 - val_loss: 0.0116 - lr: 0.0100 - 17s/epoch - 25ms/step\n",
      "Epoch 155/2000\n",
      "675/675 - 13s - loss: 0.0115 - val_loss: 0.0117 - lr: 0.0100 - 13s/epoch - 19ms/step\n",
      "Epoch 156/2000\n",
      "675/675 - 12s - loss: 0.0114 - val_loss: 0.0116 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 157/2000\n",
      "675/675 - 12s - loss: 0.0114 - val_loss: 0.0116 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 158/2000\n",
      "675/675 - 12s - loss: 0.0113 - val_loss: 0.0115 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 159/2000\n",
      "675/675 - 12s - loss: 0.0113 - val_loss: 0.0115 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 160/2000\n",
      "675/675 - 12s - loss: 0.0113 - val_loss: 0.0114 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 161/2000\n",
      "675/675 - 12s - loss: 0.0112 - val_loss: 0.0114 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 162/2000\n",
      "675/675 - 12s - loss: 0.0112 - val_loss: 0.0115 - lr: 0.0100 - 12s/epoch - 17ms/step\n",
      "Epoch 163/2000\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "675/675 - 12s - loss: 0.0112 - val_loss: 0.0114 - lr: 0.0100 - 12s/epoch - 18ms/step\n",
      "Epoch 164/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0114 - lr: 0.0020 - 12s/epoch - 18ms/step\n",
      "Epoch 165/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 0.0020 - 12s/epoch - 18ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 0.0020 - 12s/epoch - 18ms/step\n",
      "Epoch 167/2000\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 0.0020 - 12s/epoch - 17ms/step\n",
      "Epoch 168/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 4.0000e-04 - 12s/epoch - 17ms/step\n",
      "Epoch 169/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 4.0000e-04 - 12s/epoch - 17ms/step\n",
      "Epoch 170/2000\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 4.0000e-04 - 12s/epoch - 17ms/step\n",
      "Epoch 171/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 8.0000e-05 - 12s/epoch - 18ms/step\n",
      "Epoch 172/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 8.0000e-05 - 12s/epoch - 17ms/step\n",
      "Epoch 173/2000\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "675/675 - 13s - loss: 0.0111 - val_loss: 0.0113 - lr: 8.0000e-05 - 13s/epoch - 19ms/step\n",
      "Epoch 174/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.6000e-05 - 12s/epoch - 18ms/step\n",
      "Epoch 175/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.6000e-05 - 12s/epoch - 18ms/step\n",
      "Epoch 176/2000\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.6000e-05 - 12s/epoch - 17ms/step\n",
      "Epoch 177/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 3.2000e-06 - 12s/epoch - 18ms/step\n",
      "Epoch 178/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 3.2000e-06 - 12s/epoch - 18ms/step\n",
      "Epoch 179/2000\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 3.2000e-06 - 12s/epoch - 17ms/step\n",
      "Epoch 180/2000\n",
      "675/675 - 13s - loss: 0.0111 - val_loss: 0.0113 - lr: 6.4000e-07 - 13s/epoch - 19ms/step\n",
      "Epoch 181/2000\n",
      "675/675 - 13s - loss: 0.0111 - val_loss: 0.0113 - lr: 6.4000e-07 - 13s/epoch - 19ms/step\n",
      "Epoch 182/2000\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 6.4000e-07 - 12s/epoch - 17ms/step\n",
      "Epoch 183/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.2800e-07 - 12s/epoch - 18ms/step\n",
      "Epoch 184/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.2800e-07 - 12s/epoch - 19ms/step\n",
      "Epoch 185/2000\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.2800e-07 - 12s/epoch - 18ms/step\n",
      "Epoch 186/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 2.5600e-08 - 12s/epoch - 18ms/step\n",
      "Epoch 187/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 2.5600e-08 - 12s/epoch - 18ms/step\n",
      "Epoch 188/2000\n",
      "\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 2.5600e-08 - 12s/epoch - 18ms/step\n",
      "Epoch 189/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 5.1200e-09 - 12s/epoch - 17ms/step\n",
      "Epoch 190/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 5.1200e-09 - 12s/epoch - 18ms/step\n",
      "Epoch 191/2000\n",
      "\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 5.1200e-09 - 12s/epoch - 17ms/step\n",
      "Epoch 192/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.0240e-09 - 12s/epoch - 18ms/step\n",
      "Epoch 193/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.0240e-09 - 12s/epoch - 17ms/step\n",
      "Epoch 194/2000\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 1.0240e-09 - 12s/epoch - 17ms/step\n",
      "Epoch 195/2000\n",
      "675/675 - 22s - loss: 0.0111 - val_loss: 0.0113 - lr: 2.0480e-10 - 22s/epoch - 32ms/step\n",
      "Epoch 196/2000\n",
      "675/675 - 31s - loss: 0.0111 - val_loss: 0.0113 - lr: 2.0480e-10 - 31s/epoch - 46ms/step\n",
      "Epoch 197/2000\n",
      "\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "675/675 - 14s - loss: 0.0111 - val_loss: 0.0113 - lr: 2.0480e-10 - 14s/epoch - 21ms/step\n",
      "Epoch 198/2000\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 4.0960e-11 - 12s/epoch - 18ms/step\n",
      "Epoch 199/2000\n",
      "Restoring model weights from the end of the best epoch: 179.\n",
      "675/675 - 12s - loss: 0.0111 - val_loss: 0.0113 - lr: 4.0960e-11 - 12s/epoch - 18ms/step\n",
      "Epoch 199: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Energy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Energy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 5s 7ms/step - loss: 0.0111\n",
      "169/169 [==============================] - 1s 7ms/step - loss: 0.0113\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01110\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.01132\n",
      "\n",
      "\n",
      "Epoch 1/2000\n",
      "2054/2054 - 45s - loss: 0.1049 - val_loss: 0.0603 - lr: 0.0100 - 45s/epoch - 22ms/step\n",
      "Epoch 2/2000\n",
      "2054/2054 - 35s - loss: 0.0520 - val_loss: 0.0494 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 3/2000\n",
      "2054/2054 - 35s - loss: 0.0446 - val_loss: 0.0434 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 4/2000\n",
      "2054/2054 - 36s - loss: 0.0391 - val_loss: 0.0386 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 5/2000\n",
      "2054/2054 - 35s - loss: 0.0349 - val_loss: 0.0353 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 6/2000\n",
      "2054/2054 - 36s - loss: 0.0318 - val_loss: 0.0326 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 7/2000\n",
      "2054/2054 - 37s - loss: 0.0298 - val_loss: 0.0308 - lr: 0.0100 - 37s/epoch - 18ms/step\n",
      "Epoch 8/2000\n",
      "2054/2054 - 35s - loss: 0.0282 - val_loss: 0.0294 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 9/2000\n",
      "2054/2054 - 37s - loss: 0.0269 - val_loss: 0.0279 - lr: 0.0100 - 37s/epoch - 18ms/step\n",
      "Epoch 10/2000\n",
      "2054/2054 - 36s - loss: 0.0258 - val_loss: 0.0268 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 11/2000\n",
      "2054/2054 - 36s - loss: 0.0248 - val_loss: 0.0258 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 12/2000\n",
      "2054/2054 - 35s - loss: 0.0239 - val_loss: 0.0251 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 13/2000\n",
      "2054/2054 - 35s - loss: 0.0231 - val_loss: 0.0243 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 14/2000\n",
      "2054/2054 - 35s - loss: 0.0224 - val_loss: 0.0234 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 15/2000\n",
      "2054/2054 - 35s - loss: 0.0218 - val_loss: 0.0227 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 16/2000\n",
      "2054/2054 - 35s - loss: 0.0212 - val_loss: 0.0221 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 17/2000\n",
      "2054/2054 - 36s - loss: 0.0207 - val_loss: 0.0216 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 18/2000\n",
      "2054/2054 - 35s - loss: 0.0202 - val_loss: 0.0211 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 19/2000\n",
      "2054/2054 - 35s - loss: 0.0197 - val_loss: 0.0207 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 20/2000\n",
      "2054/2054 - 35s - loss: 0.0193 - val_loss: 0.0203 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 21/2000\n",
      "2054/2054 - 35s - loss: 0.0189 - val_loss: 0.0199 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 22/2000\n",
      "2054/2054 - 35s - loss: 0.0186 - val_loss: 0.0194 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 23/2000\n",
      "2054/2054 - 35s - loss: 0.0182 - val_loss: 0.0191 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 24/2000\n",
      "2054/2054 - 36s - loss: 0.0179 - val_loss: 0.0188 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 25/2000\n",
      "2054/2054 - 35s - loss: 0.0176 - val_loss: 0.0185 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 26/2000\n",
      "2054/2054 - 35s - loss: 0.0173 - val_loss: 0.0182 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 27/2000\n",
      "2054/2054 - 35s - loss: 0.0170 - val_loss: 0.0179 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 28/2000\n",
      "2054/2054 - 35s - loss: 0.0168 - val_loss: 0.0177 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 29/2000\n",
      "2054/2054 - 35s - loss: 0.0165 - val_loss: 0.0175 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 30/2000\n",
      "2054/2054 - 35s - loss: 0.0163 - val_loss: 0.0172 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 31/2000\n",
      "2054/2054 - 35s - loss: 0.0161 - val_loss: 0.0170 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 32/2000\n",
      "2054/2054 - 35s - loss: 0.0158 - val_loss: 0.0167 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 33/2000\n",
      "2054/2054 - 35s - loss: 0.0156 - val_loss: 0.0165 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 34/2000\n",
      "2054/2054 - 35s - loss: 0.0154 - val_loss: 0.0163 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 35/2000\n",
      "2054/2054 - 35s - loss: 0.0152 - val_loss: 0.0162 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 36/2000\n",
      "2054/2054 - 35s - loss: 0.0150 - val_loss: 0.0160 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 37/2000\n",
      "2054/2054 - 35s - loss: 0.0148 - val_loss: 0.0157 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 38/2000\n",
      "2054/2054 - 35s - loss: 0.0146 - val_loss: 0.0155 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 39/2000\n",
      "2054/2054 - 35s - loss: 0.0145 - val_loss: 0.0154 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 40/2000\n",
      "2054/2054 - 35s - loss: 0.0143 - val_loss: 0.0152 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 41/2000\n",
      "2054/2054 - 36s - loss: 0.0141 - val_loss: 0.0151 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 42/2000\n",
      "2054/2054 - 36s - loss: 0.0140 - val_loss: 0.0149 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 43/2000\n",
      "2054/2054 - 35s - loss: 0.0138 - val_loss: 0.0148 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 44/2000\n",
      "2054/2054 - 35s - loss: 0.0137 - val_loss: 0.0146 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 45/2000\n",
      "2054/2054 - 35s - loss: 0.0135 - val_loss: 0.0145 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 46/2000\n",
      "2054/2054 - 35s - loss: 0.0134 - val_loss: 0.0143 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 47/2000\n",
      "2054/2054 - 35s - loss: 0.0132 - val_loss: 0.0142 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 48/2000\n",
      "2054/2054 - 35s - loss: 0.0131 - val_loss: 0.0141 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 49/2000\n",
      "2054/2054 - 35s - loss: 0.0130 - val_loss: 0.0140 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 50/2000\n",
      "2054/2054 - 35s - loss: 0.0129 - val_loss: 0.0137 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 51/2000\n",
      "2054/2054 - 35s - loss: 0.0127 - val_loss: 0.0136 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 52/2000\n",
      "2054/2054 - 35s - loss: 0.0126 - val_loss: 0.0136 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 53/2000\n",
      "2054/2054 - 35s - loss: 0.0125 - val_loss: 0.0135 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 54/2000\n",
      "2054/2054 - 35s - loss: 0.0124 - val_loss: 0.0133 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 55/2000\n",
      "2054/2054 - 35s - loss: 0.0123 - val_loss: 0.0132 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 56/2000\n",
      "2054/2054 - 35s - loss: 0.0122 - val_loss: 0.0131 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 57/2000\n",
      "2054/2054 - 35s - loss: 0.0121 - val_loss: 0.0130 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 58/2000\n",
      "2054/2054 - 35s - loss: 0.0120 - val_loss: 0.0131 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 59/2000\n",
      "2054/2054 - 36s - loss: 0.0119 - val_loss: 0.0128 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 60/2000\n",
      "2054/2054 - 36s - loss: 0.0118 - val_loss: 0.0128 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 61/2000\n",
      "2054/2054 - 36s - loss: 0.0117 - val_loss: 0.0127 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 62/2000\n",
      "2054/2054 - 37s - loss: 0.0116 - val_loss: 0.0126 - lr: 0.0100 - 37s/epoch - 18ms/step\n",
      "Epoch 63/2000\n",
      "2054/2054 - 37s - loss: 0.0115 - val_loss: 0.0125 - lr: 0.0100 - 37s/epoch - 18ms/step\n",
      "Epoch 64/2000\n",
      "2054/2054 - 35s - loss: 0.0115 - val_loss: 0.0124 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 65/2000\n",
      "2054/2054 - 35s - loss: 0.0114 - val_loss: 0.0123 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 66/2000\n",
      "2054/2054 - 36s - loss: 0.0113 - val_loss: 0.0122 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 67/2000\n",
      "2054/2054 - 35s - loss: 0.0112 - val_loss: 0.0122 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 68/2000\n",
      "2054/2054 - 35s - loss: 0.0112 - val_loss: 0.0121 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 69/2000\n",
      "2054/2054 - 36s - loss: 0.0111 - val_loss: 0.0120 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 70/2000\n",
      "2054/2054 - 36s - loss: 0.0110 - val_loss: 0.0120 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 71/2000\n",
      "2054/2054 - 35s - loss: 0.0110 - val_loss: 0.0119 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 72/2000\n",
      "2054/2054 - 36s - loss: 0.0109 - val_loss: 0.0118 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 73/2000\n",
      "2054/2054 - 36s - loss: 0.0109 - val_loss: 0.0118 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 74/2000\n",
      "2054/2054 - 35s - loss: 0.0108 - val_loss: 0.0117 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 75/2000\n",
      "2054/2054 - 36s - loss: 0.0108 - val_loss: 0.0117 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 76/2000\n",
      "2054/2054 - 36s - loss: 0.0107 - val_loss: 0.0116 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 77/2000\n",
      "2054/2054 - 35s - loss: 0.0107 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 78/2000\n",
      "2054/2054 - 35s - loss: 0.0106 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 79/2000\n",
      "2054/2054 - 35s - loss: 0.0106 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/2000\n",
      "2054/2054 - 35s - loss: 0.0105 - val_loss: 0.0115 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 81/2000\n",
      "2054/2054 - 35s - loss: 0.0105 - val_loss: 0.0114 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 82/2000\n",
      "2054/2054 - 35s - loss: 0.0105 - val_loss: 0.0113 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 83/2000\n",
      "2054/2054 - 35s - loss: 0.0104 - val_loss: 0.0113 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 84/2000\n",
      "2054/2054 - 35s - loss: 0.0104 - val_loss: 0.0114 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 85/2000\n",
      "2054/2054 - 36s - loss: 0.0104 - val_loss: 0.0112 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 86/2000\n",
      "2054/2054 - 36s - loss: 0.0103 - val_loss: 0.0112 - lr: 0.0100 - 36s/epoch - 18ms/step\n",
      "Epoch 87/2000\n",
      "2054/2054 - 35s - loss: 0.0103 - val_loss: 0.0112 - lr: 0.0100 - 35s/epoch - 17ms/step\n",
      "Epoch 88/2000\n",
      "2054/2054 - 36s - loss: 0.0103 - val_loss: 0.0112 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 89/2000\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "2054/2054 - 36s - loss: 0.0102 - val_loss: 0.0111 - lr: 0.0100 - 36s/epoch - 17ms/step\n",
      "Epoch 90/2000\n",
      "2054/2054 - 35s - loss: 0.0102 - val_loss: 0.0111 - lr: 0.0020 - 35s/epoch - 17ms/step\n",
      "Epoch 91/2000\n",
      "2054/2054 - 35s - loss: 0.0102 - val_loss: 0.0111 - lr: 0.0020 - 35s/epoch - 17ms/step\n",
      "Epoch 92/2000\n",
      "2054/2054 - 36s - loss: 0.0102 - val_loss: 0.0111 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 93/2000\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "2054/2054 - 36s - loss: 0.0102 - val_loss: 0.0110 - lr: 0.0020 - 36s/epoch - 17ms/step\n",
      "Epoch 94/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 95/2000\n",
      "2054/2054 - 37s - loss: 0.0101 - val_loss: 0.0110 - lr: 4.0000e-04 - 37s/epoch - 18ms/step\n",
      "Epoch 96/2000\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 4.0000e-04 - 36s/epoch - 17ms/step\n",
      "Epoch 97/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 98/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 99/2000\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 8.0000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 100/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 101/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.6000e-05 - 36s/epoch - 17ms/step\n",
      "Epoch 102/2000\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "2054/2054 - 37s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.6000e-05 - 37s/epoch - 18ms/step\n",
      "Epoch 103/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 104/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 105/2000\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 3.2000e-06 - 36s/epoch - 17ms/step\n",
      "Epoch 106/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 107/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 108/2000\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 6.4000e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 109/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 110/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.2800e-07 - 36s/epoch - 17ms/step\n",
      "Epoch 111/2000\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.2800e-07 - 35s/epoch - 17ms/step\n",
      "Epoch 112/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 113/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 114/2000\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 2.5600e-08 - 36s/epoch - 17ms/step\n",
      "Epoch 115/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 116/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 5.1200e-09 - 36s/epoch - 18ms/step\n",
      "Epoch 117/2000\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 5.1200e-09 - 36s/epoch - 17ms/step\n",
      "Epoch 118/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.0240e-09 - 36s/epoch - 18ms/step\n",
      "Epoch 119/2000\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.0240e-09 - 35s/epoch - 17ms/step\n",
      "Epoch 120/2000\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 1.0240e-09 - 35s/epoch - 17ms/step\n",
      "Epoch 121/2000\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 2.0480e-10 - 35s/epoch - 17ms/step\n",
      "Epoch 122/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 123/2000\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 2.0480e-10 - 36s/epoch - 17ms/step\n",
      "Epoch 124/2000\n",
      "2054/2054 - 36s - loss: 0.0101 - val_loss: 0.0110 - lr: 4.0960e-11 - 36s/epoch - 17ms/step\n",
      "Epoch 125/2000\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 4.0960e-11 - 35s/epoch - 17ms/step\n",
      "Epoch 126/2000\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 4.0960e-11 - 35s/epoch - 17ms/step\n",
      "Epoch 127/2000\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 8.1920e-12 - 35s/epoch - 17ms/step\n",
      "Epoch 128/2000\n",
      "Restoring model weights from the end of the best epoch: 108.\n",
      "2054/2054 - 35s - loss: 0.0101 - val_loss: 0.0110 - lr: 8.1920e-12 - 35s/epoch - 17ms/step\n",
      "Epoch 128: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Financials/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saveModel/Financials/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2054/2054 [==============================] - 15s 7ms/step - loss: 0.0101\n",
      "514/514 [==============================] - 4s 7ms/step - loss: 0.0110\n",
      "\n",
      "\n",
      "Erro quadrático médio em dados de treinamento: 0.01014\n",
      "\n",
      "Erro quadrático médio em dados de teste: 0.01104\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for nameSetor, setor in setores.items():\n",
    "\n",
    "    if trainModel:\n",
    "        X_treino, X_teste, y_treino, y_teste = preprocessingdata(steps, SP500_close, setor)\n",
    "        model = create_model()\n",
    "\n",
    "        model.fit(x = X_treino,\n",
    "                  y = y_treino,\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  verbose = verbose,\n",
    "                  validation_data = (X_teste, y_teste),\n",
    "                  callbacks = callbacks)\n",
    "        \n",
    "        #---------Save Model---------------------------\n",
    "        model.save('saveModel/{}/'.format(nameSetor),\n",
    "                   overwrite=True,\n",
    "                   include_optimizer=True,\n",
    "                   save_format = 'tf')\n",
    "        #----------------------------------------------\n",
    "\n",
    "        scoreTrain = model.evaluate(X_treino, y_treino)\n",
    "        scoreTest = model.evaluate(X_teste, y_teste)\n",
    "\n",
    "        print('\\n\\nErro quadrático médio em dados de treinamento: {:.5f}\\n\\nErro quadrático médio em dados de teste: {:.5f}\\n\\n'\\\n",
    "            .format(scoreTrain, scoreTest))\n",
    "\n",
    "        fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model)\n",
    "        \n",
    "    else:\n",
    "        model = load_model('saveModel/{}/'.format(nameSetor))\n",
    "        fillPrediction(tableLogRet, tablePrevision, nameSetor, setor, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dia</th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>...</th>\n",
       "      <th>WYNN</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24/10</td>\n",
       "      <td>-0.028104</td>\n",
       "      <td>-0.070165</td>\n",
       "      <td>0.009660</td>\n",
       "      <td>-0.097270</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.005414</td>\n",
       "      <td>-0.013053</td>\n",
       "      <td>-0.023760</td>\n",
       "      <td>-0.071460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137958</td>\n",
       "      <td>-0.230894</td>\n",
       "      <td>0.051548</td>\n",
       "      <td>-0.093068</td>\n",
       "      <td>-0.104269</td>\n",
       "      <td>-0.071549</td>\n",
       "      <td>-0.047608</td>\n",
       "      <td>-0.103562</td>\n",
       "      <td>-0.051527</td>\n",
       "      <td>-0.067073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25/10</td>\n",
       "      <td>-0.005556</td>\n",
       "      <td>-0.047145</td>\n",
       "      <td>0.021813</td>\n",
       "      <td>-0.113248</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.015086</td>\n",
       "      <td>-0.013310</td>\n",
       "      <td>-0.004750</td>\n",
       "      <td>-0.049512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150742</td>\n",
       "      <td>-0.215916</td>\n",
       "      <td>0.072632</td>\n",
       "      <td>-0.051262</td>\n",
       "      <td>-0.087136</td>\n",
       "      <td>-0.061975</td>\n",
       "      <td>-0.032195</td>\n",
       "      <td>-0.083463</td>\n",
       "      <td>-0.105818</td>\n",
       "      <td>-0.039219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26/10</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>-0.091805</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.012493</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.013773</td>\n",
       "      <td>-0.027337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075544</td>\n",
       "      <td>-0.201938</td>\n",
       "      <td>0.045874</td>\n",
       "      <td>-0.040985</td>\n",
       "      <td>-0.069650</td>\n",
       "      <td>-0.036896</td>\n",
       "      <td>-0.015892</td>\n",
       "      <td>-0.078709</td>\n",
       "      <td>-0.152533</td>\n",
       "      <td>-0.026923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27/10</td>\n",
       "      <td>0.028459</td>\n",
       "      <td>0.052098</td>\n",
       "      <td>0.032720</td>\n",
       "      <td>-0.084410</td>\n",
       "      <td>-0.005042</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.036605</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>-0.012944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>-0.201103</td>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>-0.020342</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>-0.033439</td>\n",
       "      <td>-0.160053</td>\n",
       "      <td>-0.014552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28/10</td>\n",
       "      <td>0.036344</td>\n",
       "      <td>0.093614</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>-0.068235</td>\n",
       "      <td>-0.007491</td>\n",
       "      <td>0.024106</td>\n",
       "      <td>0.042770</td>\n",
       "      <td>0.016374</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>-0.188726</td>\n",
       "      <td>0.027623</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>-0.027812</td>\n",
       "      <td>-0.019185</td>\n",
       "      <td>0.024322</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-0.152814</td>\n",
       "      <td>-0.018454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31/10</td>\n",
       "      <td>0.045507</td>\n",
       "      <td>0.124326</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>-0.069606</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.053558</td>\n",
       "      <td>0.025336</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091738</td>\n",
       "      <td>-0.165874</td>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>-0.016350</td>\n",
       "      <td>-0.006185</td>\n",
       "      <td>0.035285</td>\n",
       "      <td>-0.006605</td>\n",
       "      <td>-0.179427</td>\n",
       "      <td>-0.009033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01/11</td>\n",
       "      <td>0.050406</td>\n",
       "      <td>0.092714</td>\n",
       "      <td>0.065926</td>\n",
       "      <td>-0.075277</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>0.035336</td>\n",
       "      <td>0.043985</td>\n",
       "      <td>0.040730</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087953</td>\n",
       "      <td>-0.142530</td>\n",
       "      <td>0.071612</td>\n",
       "      <td>0.030471</td>\n",
       "      <td>-0.010561</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>0.038960</td>\n",
       "      <td>-0.003777</td>\n",
       "      <td>-0.153670</td>\n",
       "      <td>-0.014755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02/11</td>\n",
       "      <td>0.032255</td>\n",
       "      <td>0.057506</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>-0.061538</td>\n",
       "      <td>-0.020733</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>-0.012344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092810</td>\n",
       "      <td>-0.149421</td>\n",
       "      <td>0.062182</td>\n",
       "      <td>0.020829</td>\n",
       "      <td>-0.035924</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>-0.140438</td>\n",
       "      <td>-0.033945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>03/11</td>\n",
       "      <td>0.039415</td>\n",
       "      <td>0.101251</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>-0.010038</td>\n",
       "      <td>-0.007575</td>\n",
       "      <td>0.029612</td>\n",
       "      <td>0.023758</td>\n",
       "      <td>0.040842</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058066</td>\n",
       "      <td>-0.095021</td>\n",
       "      <td>0.076603</td>\n",
       "      <td>0.047070</td>\n",
       "      <td>-0.009049</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.017818</td>\n",
       "      <td>-0.006753</td>\n",
       "      <td>-0.118040</td>\n",
       "      <td>-0.025269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>04/11</td>\n",
       "      <td>0.048674</td>\n",
       "      <td>0.122385</td>\n",
       "      <td>0.087184</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.053954</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0.037326</td>\n",
       "      <td>0.056410</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038301</td>\n",
       "      <td>-0.072026</td>\n",
       "      <td>0.086579</td>\n",
       "      <td>0.079956</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.018575</td>\n",
       "      <td>0.042375</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>-0.104398</td>\n",
       "      <td>-0.007140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>07/11</td>\n",
       "      <td>0.009628</td>\n",
       "      <td>0.136294</td>\n",
       "      <td>0.043281</td>\n",
       "      <td>-0.007156</td>\n",
       "      <td>0.023703</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.028250</td>\n",
       "      <td>-0.023913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062096</td>\n",
       "      <td>-0.093568</td>\n",
       "      <td>0.102789</td>\n",
       "      <td>0.056266</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>-0.006827</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>-0.030569</td>\n",
       "      <td>-0.087726</td>\n",
       "      <td>-0.026929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>08/11</td>\n",
       "      <td>-0.029209</td>\n",
       "      <td>0.056875</td>\n",
       "      <td>0.016102</td>\n",
       "      <td>-0.031011</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.020011</td>\n",
       "      <td>-0.032884</td>\n",
       "      <td>-0.006787</td>\n",
       "      <td>-0.057865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149642</td>\n",
       "      <td>-0.105286</td>\n",
       "      <td>0.109574</td>\n",
       "      <td>0.036146</td>\n",
       "      <td>-0.066386</td>\n",
       "      <td>-0.027205</td>\n",
       "      <td>-0.022760</td>\n",
       "      <td>-0.064236</td>\n",
       "      <td>-0.061425</td>\n",
       "      <td>-0.048330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>09/11</td>\n",
       "      <td>-0.038152</td>\n",
       "      <td>0.065958</td>\n",
       "      <td>0.016793</td>\n",
       "      <td>-0.031609</td>\n",
       "      <td>-0.012939</td>\n",
       "      <td>-0.018856</td>\n",
       "      <td>-0.032738</td>\n",
       "      <td>-0.009493</td>\n",
       "      <td>-0.056586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164788</td>\n",
       "      <td>-0.070213</td>\n",
       "      <td>0.165727</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>-0.061058</td>\n",
       "      <td>-0.034909</td>\n",
       "      <td>-0.029246</td>\n",
       "      <td>-0.063943</td>\n",
       "      <td>-0.039198</td>\n",
       "      <td>-0.046381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10/11</td>\n",
       "      <td>-0.034238</td>\n",
       "      <td>0.079405</td>\n",
       "      <td>0.031230</td>\n",
       "      <td>-0.023515</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>-0.001764</td>\n",
       "      <td>-0.042889</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>-0.037113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185549</td>\n",
       "      <td>-0.028177</td>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.063785</td>\n",
       "      <td>-0.049749</td>\n",
       "      <td>-0.014534</td>\n",
       "      <td>-0.011553</td>\n",
       "      <td>-0.057286</td>\n",
       "      <td>-0.022012</td>\n",
       "      <td>-0.032442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11/11</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.125607</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>0.017432</td>\n",
       "      <td>0.015774</td>\n",
       "      <td>-0.003527</td>\n",
       "      <td>0.006562</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163630</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.166799</td>\n",
       "      <td>0.103762</td>\n",
       "      <td>-0.022643</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>-0.006081</td>\n",
       "      <td>-0.008613</td>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14/11</td>\n",
       "      <td>0.014789</td>\n",
       "      <td>0.138069</td>\n",
       "      <td>0.023407</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>0.012856</td>\n",
       "      <td>0.036901</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025538</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.130591</td>\n",
       "      <td>0.120263</td>\n",
       "      <td>-0.016773</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.025810</td>\n",
       "      <td>-0.018556</td>\n",
       "      <td>0.014899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15/11</td>\n",
       "      <td>0.017498</td>\n",
       "      <td>0.122242</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.025735</td>\n",
       "      <td>-0.004471</td>\n",
       "      <td>-0.017318</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.030422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054406</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.131516</td>\n",
       "      <td>0.122072</td>\n",
       "      <td>-0.018519</td>\n",
       "      <td>0.031538</td>\n",
       "      <td>0.011610</td>\n",
       "      <td>0.060609</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.012371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16/11</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.088483</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>0.031456</td>\n",
       "      <td>0.005349</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.054819</td>\n",
       "      <td>0.024170</td>\n",
       "      <td>0.043150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076758</td>\n",
       "      <td>0.043445</td>\n",
       "      <td>0.145503</td>\n",
       "      <td>0.131499</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.028862</td>\n",
       "      <td>0.024037</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.030035</td>\n",
       "      <td>0.015628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17/11</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.062635</td>\n",
       "      <td>-0.009192</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>-0.016415</td>\n",
       "      <td>-0.021327</td>\n",
       "      <td>-0.006185</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110591</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.092095</td>\n",
       "      <td>0.101332</td>\n",
       "      <td>-0.009013</td>\n",
       "      <td>-0.009600</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>-0.008285</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>-0.002719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18/11</td>\n",
       "      <td>0.017387</td>\n",
       "      <td>0.059581</td>\n",
       "      <td>0.008285</td>\n",
       "      <td>0.033098</td>\n",
       "      <td>-0.017396</td>\n",
       "      <td>-0.011757</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>0.016194</td>\n",
       "      <td>0.034388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129343</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.054085</td>\n",
       "      <td>0.123488</td>\n",
       "      <td>0.016101</td>\n",
       "      <td>0.017269</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>0.040734</td>\n",
       "      <td>-0.034555</td>\n",
       "      <td>0.019796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 504 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dia         A       AAL       AAP      AAPL      ABBV       ABC  \\\n",
       "0   24/10 -0.028104 -0.070165  0.009660 -0.097270  0.002473  0.005414   \n",
       "1   25/10 -0.005556 -0.047145  0.021813 -0.113248  0.008050  0.015086   \n",
       "2   26/10  0.010191  0.010363  0.023840 -0.091805  0.016371  0.012493   \n",
       "3   27/10  0.028459  0.052098  0.032720 -0.084410 -0.005042  0.004940   \n",
       "4   28/10  0.036344  0.093614  0.055926 -0.068235 -0.007491  0.024106   \n",
       "5   31/10  0.045507  0.124326  0.074900 -0.069606  0.004067  0.029380   \n",
       "6   01/11  0.050406  0.092714  0.065926 -0.075277 -0.000356  0.035336   \n",
       "7   02/11  0.032255  0.057506  0.042218 -0.061538 -0.020733  0.013072   \n",
       "8   03/11  0.039415  0.101251  0.065728 -0.010038 -0.007575  0.029612   \n",
       "9   04/11  0.048674  0.122385  0.087184  0.021729  0.053954  0.034543   \n",
       "10  07/11  0.009628  0.136294  0.043281 -0.007156  0.023703  0.004959   \n",
       "11  08/11 -0.029209  0.056875  0.016102 -0.031011 -0.002884 -0.020011   \n",
       "12  09/11 -0.038152  0.065958  0.016793 -0.031609 -0.012939 -0.018856   \n",
       "13  10/11 -0.034238  0.079405  0.031230 -0.023515  0.007562 -0.001764   \n",
       "14  11/11  0.002704  0.125607  0.054054  0.015301  0.017432  0.015774   \n",
       "15  14/11  0.014789  0.138069  0.023407  0.014236  0.019524  0.012856   \n",
       "16  15/11  0.017498  0.122242  0.007395  0.025735 -0.004471 -0.017318   \n",
       "17  16/11  0.017272  0.088483  0.004346  0.031456  0.005349  0.001570   \n",
       "18  17/11 -0.000051  0.062635 -0.009192 -0.000638 -0.016415 -0.021327   \n",
       "19  18/11  0.017387  0.059581  0.008285  0.033098 -0.017396 -0.011757   \n",
       "\n",
       "        ABMD       ABT       ACN  ...      WYNN       XEL       XOM      XRAY  \\\n",
       "0  -0.013053 -0.023760 -0.071460  ... -0.137958 -0.230894  0.051548 -0.093068   \n",
       "1  -0.013310 -0.004750 -0.049512  ... -0.150742 -0.215916  0.072632 -0.051262   \n",
       "2   0.013720  0.013773 -0.027337  ... -0.075544 -0.201938  0.045874 -0.040985   \n",
       "3   0.036605  0.011346 -0.012944  ... -0.000355 -0.201103  0.012522  0.007024   \n",
       "4   0.042770  0.016374 -0.002916  ...  0.017530 -0.188726  0.027623 -0.001207   \n",
       "5   0.053558  0.025336  0.006397  ... -0.091738 -0.165874  0.007751  0.013968   \n",
       "6   0.043985  0.040730  0.010195  ... -0.087953 -0.142530  0.071612  0.030471   \n",
       "7   0.019900  0.036667 -0.012344  ... -0.092810 -0.149421  0.062182  0.020829   \n",
       "8   0.023758  0.040842  0.001055  ... -0.058066 -0.095021  0.076603  0.047070   \n",
       "9   0.037326  0.056410  0.004693  ... -0.038301 -0.072026  0.086579  0.079956   \n",
       "10  0.000174  0.028250 -0.023913  ... -0.062096 -0.093568  0.102789  0.056266   \n",
       "11 -0.032884 -0.006787 -0.057865  ... -0.149642 -0.105286  0.109574  0.036146   \n",
       "12 -0.032738 -0.009493 -0.056586  ... -0.164788 -0.070213  0.165727  0.031121   \n",
       "13 -0.042889 -0.000194 -0.037113  ... -0.185549 -0.028177  0.187400  0.063785   \n",
       "14 -0.003527  0.006562  0.001374  ... -0.163630  0.006068  0.166799  0.103762   \n",
       "15  0.036901  0.013331  0.010054  ... -0.025538  0.002048  0.130591  0.120263   \n",
       "16  0.053100  0.007439  0.030422  ...  0.054406  0.009958  0.131516  0.122072   \n",
       "17  0.054819  0.024170  0.043150  ...  0.076758  0.043445  0.145503  0.131499   \n",
       "18 -0.006185  0.000719  0.014953  ...  0.110591  0.014054  0.092095  0.101332   \n",
       "19  0.028640  0.016194  0.034388  ...  0.129343  0.030000  0.054085  0.123488   \n",
       "\n",
       "         XYL       YUM       ZBH      ZBRA      ZION       ZTS  \n",
       "0  -0.104269 -0.071549 -0.047608 -0.103562 -0.051527 -0.067073  \n",
       "1  -0.087136 -0.061975 -0.032195 -0.083463 -0.105818 -0.039219  \n",
       "2  -0.069650 -0.036896 -0.015892 -0.078709 -0.152533 -0.026923  \n",
       "3  -0.038498 -0.020342  0.006086 -0.033439 -0.160053 -0.014552  \n",
       "4  -0.027812 -0.019185  0.024322 -0.014636 -0.152814 -0.018454  \n",
       "5  -0.016350 -0.006185  0.035285 -0.006605 -0.179427 -0.009033  \n",
       "6  -0.010561  0.005947  0.038960 -0.003777 -0.153670 -0.014755  \n",
       "7  -0.035924  0.002183  0.013752 -0.028148 -0.140438 -0.033945  \n",
       "8  -0.009049  0.010371  0.017818 -0.006753 -0.118040 -0.025269  \n",
       "9  -0.001549  0.018575  0.042375  0.006645 -0.104398 -0.007140  \n",
       "10 -0.034091 -0.006827  0.016612 -0.030569 -0.087726 -0.026929  \n",
       "11 -0.066386 -0.027205 -0.022760 -0.064236 -0.061425 -0.048330  \n",
       "12 -0.061058 -0.034909 -0.029246 -0.063943 -0.039198 -0.046381  \n",
       "13 -0.049749 -0.014534 -0.011553 -0.057286 -0.022012 -0.032442  \n",
       "14 -0.022643 -0.003277  0.010606 -0.006081 -0.008613  0.003258  \n",
       "15 -0.016773  0.007533  0.023658  0.025810 -0.018556  0.014899  \n",
       "16 -0.018519  0.031538  0.011610  0.060609  0.010490  0.012371  \n",
       "17  0.003045  0.028862  0.024037  0.018331  0.030035  0.015628  \n",
       "18 -0.009013 -0.009600  0.002824 -0.008285  0.006157 -0.002719  \n",
       "19  0.016101  0.017269  0.026748  0.040734 -0.034555  0.019796  \n",
       "\n",
       "[20 rows x 504 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tableLogRet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva tabela de previsão\n",
    "\n",
    "outdirP = './previsao/{}'.format(datetime.now().strftime('%d-%B-%Ih%Mmin'))\n",
    "\n",
    "if not os.path.exists(outdirP):\n",
    "    os.mkdir(outdirP)\n",
    "\n",
    "fullnameP = os.path.join(outdirP, 'previsao.csv')\n",
    "\n",
    "tablePrevision.to_csv(fullnameP, index = True, decimal = '.', sep=',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva tabela Entregável do Log-Retorno no padrão\n",
    "\n",
    "outdirLR = './logRetorno/{}'.format(datetime.now().strftime('%d-%B-%Ih%Mmin'))\n",
    "\n",
    "if not os.path.exists(outdirLR):\n",
    "    os.mkdir(outdirLR)\n",
    "\n",
    "fullnameLR = os.path.join(outdirLR, 'logRetorno.csv')\n",
    "\n",
    "tableLogRet.iloc[-len(forecast):, :].to_csv(fullnameLR, index = False, decimal = '.', sep=',')       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
