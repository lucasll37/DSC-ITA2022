{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Challenge @ ITA 2022</font>\n",
    "# <font color='blue'>Equipe DIOMGIS</font>\n",
    "\n",
    "## <font color='blue'>Fase 1</font>\n",
    "\n",
    "### <font color='blue'>Predição de pregões futuros de ativos que compõem o índice SP500.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data\\image\\logo.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas e Frameworks\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorboard import notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Equipe DIOMGIS\n",
      "\n",
      "numpy            : 1.22.3\n",
      "matplotlib       : 3.5.1\n",
      "keras            : 2.10.0\n",
      "tensorflow       : 2.10.0\n",
      "seaborn          : 0.11.2\n",
      "pandas           : 1.4.2\n",
      "pandas_datareader: 0.10.0\n",
      "tensorboard      : 2.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Equipe DIOMGIS\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "plt.rcParams['figure.figsize'] = (15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros fixos de treinamento\n",
    "\n",
    "verbose = 2\n",
    "seed = 25\n",
    "steps = 30\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "nKFold = 5\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Confirma se o TensorFlow pode acessar a GPU\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "    raise SystemError('GPU device not found')\n",
    "    \n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 16 15:17:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.94       Driver Version: 516.94       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:65:00.0  On |                  N/A |\n",
      "| 40%   30C    P2    40W / 220W |   1284MiB /  8192MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       732    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      2592    C+G   ...txyewy\\MiniSearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3516    C+G   ...o Webcam\\GoPro Webcam.exe    N/A      |\n",
      "|    0   N/A  N/A      6380    C+G   ...\\app-1.0.9006\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      7452    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     12216    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13784    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14424    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14448    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15244    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     16788    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     17300    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     17636    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     20344    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20628    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     21560    C+G   ...persky VPN 5.7\\ksdeui.exe    N/A      |\n",
      "|    0   N/A  N/A     22980    C+G   ...x64__pc75e8sa7ep4e\\XD.exe    N/A      |\n",
      "|    0   N/A  N/A     27156    C+G   ...bat\\acrocef_2\\AcroCEF.exe    N/A      |\n",
      "|    0   N/A  N/A     28216    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     29316    C+G   ...370.42\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     29740    C+G   ...8bbwe\\Notepad\\Notepad.exe    N/A      |\n",
      "|    0   N/A  N/A     30040    C+G   ...370.42\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     31576    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     34552    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     35280    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     41996      C   ...ucas\\anaconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     48616    C+G   ...8bbwe\\Notepad\\Notepad.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Estado da GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = ['2022-10-24', '2022-10-25', '2022-10-26', '2022-10-27', '2022-10-28', \n",
    "            '2022-10-31', '2022-11-01', '2022-11-02', '2022-11-03', '2022-11-04', \n",
    "            '2022-11-07', '2022-11-08', '2022-11-09', '2022-11-10', '2022-11-11',\n",
    "            '2022-11-14', '2022-11-15', '2022-11-16', '2022-11-17', '2022-11-18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ativos = ['A', 'AAL', 'AAP', 'AAPL', 'ABBV', 'ABC', 'ABMD', 'ABT',\n",
    "       'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES',\n",
    "       'AFL', 'AIG', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL',\n",
    "       'ALLE', 'AMAT', 'AMCR', 'AMD', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN',\n",
    "       'ANET', 'ANSS', 'AON', 'AOS', 'APA', 'APD', 'APH', 'APTV', 'ARE',\n",
    "       'ATO', 'ATVI', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AZO', 'BA',\n",
    "       'BAC', 'BALL', 'BAX', 'BBWI', 'BBY', 'BDX', 'BEN', 'BF.B', 'BIIB',\n",
    "       'BIO', 'BK', 'BKNG', 'BKR', 'BLK', 'BMY', 'BR', 'BRK.B', 'BRO',\n",
    "       'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CARR', 'CAT', 'CB',\n",
    "       'CBOE', 'CBRE', 'CCI', 'CCL', 'CDAY', 'CDNS', 'CDW', 'CE', 'CEG',\n",
    "       'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX',\n",
    "       'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF',\n",
    "       'COO', 'COP', 'COST', 'CPB', 'CPRT', 'CPT', 'CRL', 'CRM', 'CSCO',\n",
    "       'CSGP', 'CSX', 'CTAS', 'CTLT', 'CTRA', 'CTSH', 'CTVA', 'CVS',\n",
    "       'CVX', 'CZR', 'D', 'DAL', 'DD', 'DE', 'DFS', 'DG', 'DGX', 'DHI',\n",
    "       'DHR', 'DIS', 'DISH', 'DLR', 'DLTR', 'DOV', 'DOW', 'DPZ', 'DRI',\n",
    "       'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'DXCM', 'EA', 'EBAY', 'ECL',\n",
    "       'ED', 'EFX', 'EIX', 'EL', 'ELV', 'EMN', 'EMR', 'ENPH', 'EOG',\n",
    "       'EPAM', 'EQIX', 'EQR', 'EQT', 'ES', 'ESS', 'ETN', 'ETR', 'ETSY',\n",
    "       'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG', 'FAST',\n",
    "       'FBHS', 'FCX', 'FDS', 'FDX', 'FE', 'FFIV', 'FIS', 'FISV', 'FITB',\n",
    "       'FLT', 'FMC', 'FOX', 'FOXA', 'FRC', 'FRT', 'FTNT', 'FTV', 'GD',\n",
    "       'GE', 'GILD', 'GIS', 'GL', 'GLW', 'GM', 'GNRC', 'GOOG', 'GOOGL',\n",
    "       'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HCA',\n",
    "       'HD', 'HES', 'HIG', 'HII', 'HLT', 'HOLX', 'HON', 'HPE', 'HPQ',\n",
    "       'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'HWM', 'IBM', 'ICE', 'IDXX',\n",
    "       'IEX', 'IFF', 'ILMN', 'INCY', 'INTC', 'INTU', 'INVH', 'IP', 'IPG',\n",
    "       'IQV', 'IR', 'IRM', 'ISRG', 'IT', 'ITW', 'IVZ', 'J', 'JBHT', 'JCI',\n",
    "       'JKHY', 'JNJ', 'JNPR', 'JPM', 'K', 'KDP', 'KEY', 'KEYS', 'KHC',\n",
    "       'KIM', 'KLAC', 'KMB', 'KMI', 'KMX', 'KO', 'KR', 'L', 'LDOS', 'LEN',\n",
    "       'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW',\n",
    "       'LRCX', 'LUMN', 'LUV', 'LVS', 'LW', 'LYB', 'LYV', 'MA', 'MAA',\n",
    "       'MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET',\n",
    "       'META', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST',\n",
    "       'MO', 'MOH', 'MOS', 'MPC', 'MPWR', 'MRK', 'MRNA', 'MRO', 'MS',\n",
    "       'MSCI', 'MSFT', 'MSI', 'MTB', 'MTCH', 'MTD', 'MU', 'NCLH', 'NDAQ',\n",
    "       'NDSN', 'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NLOK', 'NLSN', 'NOC',\n",
    "       'NOW', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWL',\n",
    "       'NWS', 'NWSA', 'NXPI', 'O', 'ODFL', 'OGN', 'OKE', 'OMC', 'ON',\n",
    "       'ORCL', 'ORLY', 'OTIS', 'OXY', 'PARA', 'PAYC', 'PAYX', 'PCAR',\n",
    "       'PCG', 'PEAK', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH',\n",
    "       'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'POOL',\n",
    "       'PPG', 'PPL', 'PRU', 'PSA', 'PSX', 'PTC', 'PWR', 'PXD', 'PYPL',\n",
    "       'QCOM', 'QRVO', 'RCL', 'RE', 'REG', 'REGN', 'RF', 'RHI', 'RJF',\n",
    "       'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX', 'SBAC',\n",
    "       'SBNY', 'SBUX', 'SCHW', 'SEDG', 'SEE', 'SHW', 'SIVB', 'SJM', 'SLB',\n",
    "       'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE', 'STE', 'STT', 'STX',\n",
    "       'STZ', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYY', 'T', 'TAP', 'TDG',\n",
    "       'TDY', 'TECH', 'TEL', 'TER', 'TFC', 'TFX', 'TGT', 'TJX', 'TMO',\n",
    "       'TMUS', 'TPR', 'TRMB', 'TROW', 'TRV', 'TSCO', 'TSLA', 'TSN', 'TT',\n",
    "       'TTWO', 'TWTR', 'TXN', 'TXT', 'TYL', 'UAL', 'UDR', 'UHS', 'ULTA',\n",
    "       'UNH', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VFC', 'VICI', 'VLO',\n",
    "       'VMC', 'VNO', 'VRSK', 'VRSN', 'VRTX', 'VTR', 'VTRS', 'VZ', 'WAB',\n",
    "       'WAT', 'WBA', 'WBD', 'WDC', 'WEC', 'WELL', 'WFC', 'WHR', 'WM',\n",
    "       'WMB', 'WMT', 'WRB', 'WRK', 'WST', 'WTW', 'WY', 'WYNN', 'XEL',\n",
    "       'XOM', 'XRAY', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>> Execute trecho comentado somente na primeira vez que rodar o Notebook <<<<<<<<<<\n",
    "#\n",
    "# start_date = \"2017-10-21\"\n",
    "# end_date = \"2022-10-21\"\n",
    "# \n",
    "# data = web.DataReader(name = '^GSPC', data_source = 'yahoo', start = start_date, end = end_date)\n",
    "# SP500_index = pd.DataFrame(data['Close']).reset_index().rename(columns={'Close': 'SP500', 'Date': 'Dia'})\n",
    "# \n",
    "# SP500_close = pd.DataFrame()\n",
    "# \n",
    "# for ativo in ativos:\n",
    "#     \n",
    "#     if ativo == 'BF.B':\n",
    "#         ativo = 'BF-B'\n",
    "#         \n",
    "#     if ativo == 'BRK.B':\n",
    "#         ativo = 'BRK-B'\n",
    "#     print(ativo)\n",
    "#     data = web.DataReader(name = ativo, data_source = 'yahoo', start = start_date, end = end_date)\n",
    "#     temp_close = pd.DataFrame(data['Close'])\n",
    "#     SP500_close = pd.concat([SP500_close, temp_close], axis = 1)\n",
    "# \n",
    "# \n",
    "# SP500_close.columns = ativos\n",
    "# SP500_close.reset_index(inplace = True)\n",
    "# SP500_close.rename(columns={'Date': 'Dia'}, inplace = True)\n",
    "# \n",
    "# assert SP500_close.isna().sum().mean() == 0,  \"Valores Faltantes\"\n",
    "# assert SP500_index.isna().sum().mean() == 0,  \"Valores Faltantes\"\n",
    "# \n",
    "# SP500_close.to_csv(path_or_buf = 'data/SP500_close', index = False)\n",
    "# SP500_index.to_csv(path_or_buf = 'data/SP500_index', index = False)\n",
    "\n",
    "df = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "Industrials = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "HealthCare = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "InformationTechnology = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "CommunicationServices = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "ConsumerStaples = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "ConsumerDiscretionary = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "Utilities = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "Financials = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "Materials = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()\n",
    "RealEstate = df.loc[df['GICS Sector'] == 'Industrials']['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_close = pd.read_csv('data/SP500_close')\n",
    "SP500_index = pd.read_csv('data/SP500_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-Processamento e Análise dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameColumns = []\n",
    "\n",
    "for i in range(steps,-1,-1):\n",
    "    nameColumns.append('Close-{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorTimeframeTable(table, ativo):\n",
    "    TimeframeTable = pd.DataFrame(np.zeros((len(table[ativo])-steps, steps+1), dtype='float64'), columns=nameColumns)\n",
    "\n",
    "    for index, close in enumerate(table[ativo]):\n",
    "        tempA = index\n",
    "        tempB = 0\n",
    "        for i in range(steps+1):\n",
    "            if tempA < len(table[ativo])-steps and tempA >=0:\n",
    "                TimeframeTable.iloc[tempA, tempB] = close\n",
    "\n",
    "            tempA -= 1\n",
    "            tempB += 1\n",
    "\n",
    "    timeIndex = table.iloc[steps:,0]\n",
    "    TimeframeTable[\"Dia\"] = timeIndex.to_numpy()\n",
    "    TimeframeTable.set_index(\"Dia\", inplace = True)\n",
    "    \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera dados para treinamento com base unicamente na S&P500\n",
    "\n",
    "TimeframeSP500 = generatorTimeframeTable(SP500_index, 'SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = TimeframeSP500['Close-0'].plot(title='S&P500')\n",
    "fig.set_xlabel('Data')\n",
    "fig.set_ylabel('Preço de Fechamento')\n",
    "fig.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainScaler(df):\n",
    "    \n",
    "    trainScaler = pd.DataFrame()\n",
    " \n",
    "    for _ in range(steps+1):\n",
    "        temp_close = pd.DataFrame(df.iloc[:,-1])\n",
    "        trainScaler = pd.concat([trainScaler, temp_close], axis = 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(trainScaler)\n",
    "\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score-Z\n",
    "\n",
    "scaler = createTrainScaler(TimeframeSP500)\n",
    "\n",
    "TimeframeSP500 = scaler.transform(TimeframeSP500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TimeframeSP500[:,-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão de dados entre Treino e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = TimeframeSP500[:, :-1]\n",
    "y = TimeframeSP500[:, -1]\n",
    "\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "\n",
    "X_treino = X_treino.reshape((-1, steps, 1))\n",
    "X_teste = X_teste.reshape((-1, steps, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Construção, Treinamento e Avaliação do Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "   \n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime('%d-%B-%Ih%Mmin')))\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=20,\n",
    "                          verbose = verbose,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              mode=\"min\",\n",
    "                              verbose = verbose,\n",
    "                              min_delta=0.00001,\n",
    "                              min_lr=0)\n",
    "\n",
    "callbacks = [tensorboard, earlystop, reduce_lr, TerminateOnNaN()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer, layers, n_lstm, dropoutFoward):\n",
    "     \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(n_lstm,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = True,\n",
    "                   input_shape = (steps, 1)))  \n",
    "\n",
    "    \n",
    "    #################################################################\n",
    "    \n",
    "    for layer in range(layers):\n",
    "                \n",
    "        model.add(Dropout(dropoutFoward))\n",
    "        \n",
    "        model.add(LSTM(n_lstm,\n",
    "                       activation = 'tanh',\n",
    "                       recurrent_activation = 'sigmoid',\n",
    "                       return_sequences = True))  \n",
    "    \n",
    "    \n",
    "    ##################################################################\n",
    "    \n",
    "    model.add(LSTM(n_lstm,\n",
    "                   activation = 'tanh',\n",
    "                   recurrent_activation = 'sigmoid',\n",
    "                   return_sequences = False)) \n",
    "    \n",
    "    \n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    Lmse = MeanSquaredError()\n",
    "    #Lmse = keras.losses.MeanSquaredError()\n",
    "\n",
    "    model.compile(loss= Lmse, optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "\n",
    "model = KerasRegressor(build_fn = create_model,\n",
    "                        verbose = verbose,\n",
    "                        callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "estimator = Pipeline([(\"model\", model)], verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos parametros (GridSearch)\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "opt_SGD = SGD(\n",
    "    learning_rate = learning_rate,\n",
    "    momentum = 0.0,\n",
    "    nesterov = False)\n",
    "\n",
    "opt_RMSprop = RMSprop(\n",
    "    learning_rate = learning_rate,\n",
    "    rho = 0.9,\n",
    "    momentum = 0.0,\n",
    "    epsilon = 1e-07,\n",
    "    centered = False)\n",
    "\n",
    "opt_Adam = Adam(\n",
    "    learning_rate = learning_rate,\n",
    "    beta_1 = 0.9,\n",
    "    beta_2 = 0.999,\n",
    "    epsilon = 1e-07,\n",
    "    amsgrad = False)\n",
    "\n",
    "opt_Adadelta = Adadelta(\n",
    "    learning_rate = learning_rate,\n",
    "    rho = 0.95,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Adagrad = Adagrad(\n",
    "    learning_rate = learning_rate,\n",
    "    initial_accumulator_value = 0.1,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Adamax = Adamax(\n",
    "    learning_rate = learning_rate,\n",
    "    beta_1 = 0.9,\n",
    "    beta_2 = 0.999,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Nadam = Nadam(\n",
    "    learning_rate = learning_rate,\n",
    "    beta_1 = 0.9,\n",
    "    beta_2 = 0.999,\n",
    "    epsilon = 1e-07)\n",
    "\n",
    "opt_Ftrl = Ftrl(\n",
    "    learning_rate = learning_rate,\n",
    "    learning_rate_power = -0.5,\n",
    "    initial_accumulator_value = 0.1,\n",
    "    l1_regularization_strength = 0.0,\n",
    "    l2_regularization_strength = 0.0,\n",
    "    l2_shrinkage_regularization_strength = 0.0,\n",
    "    beta = 0.0)\n",
    "\n",
    "params_grid = {\n",
    "    # [opt_SGD, opt_RMSprop, opt_Adam, opt_Adadelta, opt_Adagrad, opt_Adamax, opt_Nadam, opt_Ftrl]\n",
    "    'model__optimizer': [opt_SGD, opt_RMSprop, opt_Adam, opt_Adadelta, opt_Adagrad, opt_Adamax, opt_Nadam, opt_Ftrl],\n",
    "    'model__layers': [3, 4], # + 2 Por padrão já possui duas camadas LSTM\n",
    "    'model__n_lstm': [100, 120],\n",
    "    'model__dropoutFoward': [0, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search e Cross Validation\n",
    "\n",
    "grid = GridSearchCV(estimator = estimator,\n",
    "                    scoring = 'neg_root_mean_squared_error',\n",
    "                    verbose = verbose,\n",
    "                    return_train_score = False,\n",
    "                    cv = nKFold,\n",
    "                    # n_jobs = -2 # \"-2\": mantem 1 processador livre\n",
    "                    # pre_dispatch = '2*n_jobs',\n",
    "                    refit = True,\n",
    "                    param_grid = params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Monitoramento de Otimização\n",
    "\n",
    "# tensorboard --logdir=logs/\n",
    "# notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.6505 - val_loss: 1.0208 - lr: 0.0100 - 19s/epoch - 762ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6455 - val_loss: 0.6536 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3804 - val_loss: 0.2475 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1358 - val_loss: 0.2011 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0878 - val_loss: 0.1980 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0787 - val_loss: 0.1932 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0755 - val_loss: 0.1898 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0749 - val_loss: 0.1869 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0741 - val_loss: 0.1828 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0740 - val_loss: 0.1805 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0745 - val_loss: 0.1788 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0743 - val_loss: 0.1760 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0739 - val_loss: 0.1741 - lr: 0.0100 - 758ms/epoch - 30ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0742 - val_loss: 0.1738 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0732 - val_loss: 0.1719 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0728 - val_loss: 0.1702 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0740 - val_loss: 0.1695 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0733 - val_loss: 0.1677 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0722 - val_loss: 0.1668 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0721 - val_loss: 0.1644 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0714 - val_loss: 0.1644 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0717 - val_loss: 0.1640 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0708 - val_loss: 0.1617 - lr: 0.0100 - 749ms/epoch - 30ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0706 - val_loss: 0.1621 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0705 - val_loss: 0.1617 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0703 - val_loss: 0.1609 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0697 - val_loss: 0.1597 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0695 - val_loss: 0.1595 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0692 - val_loss: 0.1586 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0685 - val_loss: 0.1592 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0691 - val_loss: 0.1573 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0689 - val_loss: 0.1573 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0681 - val_loss: 0.1560 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0671 - val_loss: 0.1571 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0678 - val_loss: 0.1553 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0670 - val_loss: 0.1554 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0675 - val_loss: 0.1549 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0670 - val_loss: 0.1526 - lr: 0.0100 - 760ms/epoch - 30ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0665 - val_loss: 0.1545 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0655 - val_loss: 0.1526 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.1544 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0653 - val_loss: 0.1517 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0648 - val_loss: 0.1529 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0649 - val_loss: 0.1522 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0648 - val_loss: 0.1522 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0642 - val_loss: 0.1518 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0644 - val_loss: 0.1504 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0638 - val_loss: 0.1501 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0642 - val_loss: 0.1496 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0638 - val_loss: 0.1511 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0628 - val_loss: 0.1488 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1514 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1494 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1490 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1464 - lr: 0.0100 - 758ms/epoch - 30ms/step\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0625 - val_loss: 0.1486 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0643 - val_loss: 0.1425 - lr: 0.0020 - 759ms/epoch - 30ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1427 - lr: 0.0020 - 748ms/epoch - 30ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1432 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1434 - lr: 0.0020 - 815ms/epoch - 33ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1435 - lr: 0.0020 - 808ms/epoch - 32ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1437 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1436 - lr: 0.0020 - 800ms/epoch - 32ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1429 - lr: 0.0020 - 766ms/epoch - 31ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.1429 - lr: 0.0020 - 790ms/epoch - 32ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1428 - lr: 0.0020 - 807ms/epoch - 32ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1429 - lr: 0.0020 - 782ms/epoch - 31ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1433 - lr: 0.0020 - 795ms/epoch - 32ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1428 - lr: 0.0020 - 804ms/epoch - 32ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1423 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1418 - lr: 0.0020 - 775ms/epoch - 31ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1415 - lr: 0.0020 - 755ms/epoch - 30ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1411 - lr: 0.0020 - 802ms/epoch - 32ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.1421 - lr: 0.0020 - 772ms/epoch - 31ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1414 - lr: 0.0020 - 845ms/epoch - 34ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.1414 - lr: 0.0020 - 792ms/epoch - 32ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1414 - lr: 0.0020 - 779ms/epoch - 31ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1415 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1407 - lr: 0.0020 - 776ms/epoch - 31ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1409 - lr: 0.0020 - 775ms/epoch - 31ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1401 - lr: 0.0020 - 778ms/epoch - 31ms/step\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1407 - lr: 0.0020 - 806ms/epoch - 32ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1404 - lr: 4.0000e-04 - 798ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.1403 - lr: 4.0000e-04 - 786ms/epoch - 31ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1403 - lr: 4.0000e-04 - 781ms/epoch - 31ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1403 - lr: 4.0000e-04 - 750ms/epoch - 30ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1403 - lr: 4.0000e-04 - 748ms/epoch - 30ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1403 - lr: 4.0000e-04 - 795ms/epoch - 32ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1405 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1406 - lr: 4.0000e-04 - 788ms/epoch - 32ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1408 - lr: 4.0000e-04 - 799ms/epoch - 32ms/step\n",
      "Epoch 92/500\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1408 - lr: 4.0000e-04 - 794ms/epoch - 32ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1408 - lr: 8.0000e-05 - 797ms/epoch - 32ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1408 - lr: 8.0000e-05 - 746ms/epoch - 30ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1408 - lr: 8.0000e-05 - 772ms/epoch - 31ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1408 - lr: 8.0000e-05 - 774ms/epoch - 31ms/step\n",
      "Epoch 97/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1408 - lr: 8.0000e-05 - 758ms/epoch - 30ms/step\n",
      "Epoch 98/500\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1408 - lr: 8.0000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1408 - lr: 1.6000e-05 - 754ms/epoch - 30ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1408 - lr: 1.6000e-05 - 759ms/epoch - 30ms/step\n",
      "Epoch 101/500\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1408 - lr: 1.6000e-05 - 777ms/epoch - 31ms/step\n",
      "Epoch 101: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.7min\n",
      "7/7 - 2s - 2s/epoch - 333ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.6376 - val_loss: 0.9409 - lr: 0.0100 - 16s/epoch - 639ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7196 - val_loss: 0.7613 - lr: 0.0100 - 840ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6025 - val_loss: 0.4267 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2973 - val_loss: 0.1971 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1091 - val_loss: 0.1921 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0749 - val_loss: 0.1862 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0689 - val_loss: 0.1817 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0671 - val_loss: 0.1777 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0669 - val_loss: 0.1736 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0661 - val_loss: 0.1705 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0654 - val_loss: 0.1677 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.1668 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.1631 - lr: 0.0100 - 757ms/epoch - 30ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0653 - val_loss: 0.1616 - lr: 0.0100 - 763ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0653 - val_loss: 0.1603 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0642 - val_loss: 0.1593 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1592 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.1574 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1551 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1548 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1557 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0666 - val_loss: 0.1484 - lr: 0.0020 - 766ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1479 - lr: 0.0020 - 819ms/epoch - 33ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0576 - val_loss: 0.1481 - lr: 0.0020 - 794ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1486 - lr: 0.0020 - 770ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.1480 - lr: 0.0020 - 755ms/epoch - 30ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1474 - lr: 0.0020 - 761ms/epoch - 30ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1475 - lr: 0.0020 - 801ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1477 - lr: 0.0020 - 793ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1475 - lr: 0.0020 - 801ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1470 - lr: 0.0020 - 845ms/epoch - 34ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1469 - lr: 0.0020 - 909ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1471 - lr: 0.0020 - 819ms/epoch - 33ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1466 - lr: 0.0020 - 804ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1467 - lr: 0.0020 - 816ms/epoch - 33ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1468 - lr: 0.0020 - 858ms/epoch - 34ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1464 - lr: 0.0020 - 824ms/epoch - 33ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1460 - lr: 0.0020 - 814ms/epoch - 33ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1455 - lr: 4.0000e-04 - 803ms/epoch - 32ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1452 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0507 - val_loss: 0.1451 - lr: 4.0000e-04 - 792ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1452 - lr: 4.0000e-04 - 864ms/epoch - 35ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1453 - lr: 4.0000e-04 - 778ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1453 - lr: 4.0000e-04 - 795ms/epoch - 32ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1453 - lr: 4.0000e-04 - 776ms/epoch - 31ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1454 - lr: 4.0000e-04 - 827ms/epoch - 33ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1456 - lr: 4.0000e-04 - 822ms/epoch - 33ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1456 - lr: 4.0000e-04 - 778ms/epoch - 31ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1456 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1455 - lr: 4.0000e-04 - 840ms/epoch - 34ms/step\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1455 - lr: 4.0000e-04 - 766ms/epoch - 31ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1455 - lr: 8.0000e-05 - 813ms/epoch - 33ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1456 - lr: 8.0000e-05 - 807ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1456 - lr: 8.0000e-05 - 829ms/epoch - 33ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1456 - lr: 8.0000e-05 - 809ms/epoch - 32ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1456 - lr: 8.0000e-05 - 788ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1456 - lr: 8.0000e-05 - 811ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1456 - lr: 8.0000e-05 - 786ms/epoch - 31ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.1456 - lr: 8.0000e-05 - 828ms/epoch - 33ms/step\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1456 - lr: 8.0000e-05 - 823ms/epoch - 33ms/step\n",
      "Epoch 61/500\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1456 - lr: 1.6000e-05 - 853ms/epoch - 34ms/step\n",
      "Epoch 61: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 3s - 3s/epoch - 389ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.2min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.6572 - val_loss: 0.7452 - lr: 0.0100 - 17s/epoch - 689ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5388 - val_loss: 0.2806 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1505 - val_loss: 0.2052 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.2008 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0349 - val_loss: 0.1978 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1924 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1914 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0349 - val_loss: 0.1872 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1842 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0435 - val_loss: 0.1759 - lr: 0.0020 - 766ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1736 - lr: 0.0020 - 801ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0391 - val_loss: 0.1722 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0381 - val_loss: 0.1720 - lr: 4.0000e-04 - 785ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0377 - val_loss: 0.1719 - lr: 4.0000e-04 - 793ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0381 - val_loss: 0.1719 - lr: 4.0000e-04 - 777ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1719 - lr: 8.0000e-05 - 797ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.1720 - lr: 8.0000e-05 - 801ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0363 - val_loss: 0.1720 - lr: 8.0000e-05 - 765ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.1720 - lr: 1.6000e-05 - 786ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.1720 - lr: 1.6000e-05 - 774ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0364 - val_loss: 0.1720 - lr: 1.6000e-05 - 783ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.1720 - lr: 3.2000e-06 - 772ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0370 - val_loss: 0.1720 - lr: 3.2000e-06 - 794ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.1720 - lr: 3.2000e-06 - 790ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1720 - lr: 6.4000e-07 - 803ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.1720 - lr: 6.4000e-07 - 827ms/epoch - 33ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.1720 - lr: 6.4000e-07 - 817ms/epoch - 33ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0368 - val_loss: 0.1720 - lr: 1.2800e-07 - 811ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0367 - val_loss: 0.1720 - lr: 1.2800e-07 - 885ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.1720 - lr: 1.2800e-07 - 825ms/epoch - 33ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0372 - val_loss: 0.1720 - lr: 2.5600e-08 - 849ms/epoch - 34ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0367 - val_loss: 0.1720 - lr: 2.5600e-08 - 815ms/epoch - 33ms/step\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0373 - val_loss: 0.1720 - lr: 2.5600e-08 - 843ms/epoch - 34ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1720 - lr: 5.1200e-09 - 860ms/epoch - 34ms/step\n",
      "Epoch 35/500\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.1720 - lr: 5.1200e-09 - 804ms/epoch - 32ms/step\n",
      "Epoch 35: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  46.3s\n",
      "7/7 - 3s - 3s/epoch - 405ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  49.1s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.6488 - val_loss: 0.7129 - lr: 0.0100 - 17s/epoch - 680ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5311 - val_loss: 0.2358 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1473 - val_loss: 0.1991 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1907 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1854 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.1787 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1755 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1711 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1685 - lr: 0.0100 - 845ms/epoch - 34ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1665 - lr: 0.0100 - 868ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1643 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1648 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1591 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1563 - lr: 0.0020 - 771ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1552 - lr: 0.0020 - 759ms/epoch - 30ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1551 - lr: 4.0000e-04 - 795ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1550 - lr: 4.0000e-04 - 863ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.1550 - lr: 4.0000e-04 - 813ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1550 - lr: 8.0000e-05 - 806ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1550 - lr: 8.0000e-05 - 769ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1550 - lr: 8.0000e-05 - 799ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1550 - lr: 1.6000e-05 - 837ms/epoch - 33ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1550 - lr: 1.6000e-05 - 817ms/epoch - 33ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1550 - lr: 1.6000e-05 - 851ms/epoch - 34ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1550 - lr: 3.2000e-06 - 810ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1550 - lr: 3.2000e-06 - 792ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1550 - lr: 3.2000e-06 - 807ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1550 - lr: 6.4000e-07 - 787ms/epoch - 31ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1550 - lr: 6.4000e-07 - 790ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1550 - lr: 6.4000e-07 - 800ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1550 - lr: 1.2800e-07 - 814ms/epoch - 33ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1550 - lr: 1.2800e-07 - 837ms/epoch - 33ms/step\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1550 - lr: 1.2800e-07 - 778ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1550 - lr: 2.5600e-08 - 793ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1550 - lr: 2.5600e-08 - 800ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0491 - val_loss: 0.1550 - lr: 2.5600e-08 - 764ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1550 - lr: 5.1200e-09 - 788ms/epoch - 32ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1550 - lr: 5.1200e-09 - 788ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1550 - lr: 5.1200e-09 - 787ms/epoch - 31ms/step\n",
      "Epoch 39: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  49.6s\n",
      "7/7 - 3s - 3s/epoch - 385ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  52.3s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.2928 - val_loss: 2.4237 - lr: 0.0100 - 17s/epoch - 667ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2038 - val_loss: 2.3951 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1966 - val_loss: 2.3230 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1908 - val_loss: 2.2274 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1831 - val_loss: 2.0964 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1732 - val_loss: 1.9208 - lr: 0.0100 - 756ms/epoch - 30ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1599 - val_loss: 1.6932 - lr: 0.0100 - 753ms/epoch - 30ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1425 - val_loss: 1.4163 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1201 - val_loss: 1.1187 - lr: 0.0100 - 758ms/epoch - 30ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0969 - val_loss: 0.8623 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0788 - val_loss: 0.6919 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0680 - val_loss: 0.5965 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.5491 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.5242 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.5117 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.5042 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.4981 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.4956 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.4922 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.4857 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.4793 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.4731 - lr: 0.0100 - 753ms/epoch - 30ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.4723 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.4692 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0584 - val_loss: 0.4655 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.4635 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.4609 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.4556 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.4519 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.4494 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.4479 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.4468 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.4452 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.4410 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.4374 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.4356 - lr: 0.0100 - 731ms/epoch - 29ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.4319 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.4300 - lr: 0.0100 - 768ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.4278 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.4245 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.4244 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.4290 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.4247 - lr: 0.0020 - 780ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.4194 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.4145 - lr: 0.0020 - 752ms/epoch - 30ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.4107 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.4078 - lr: 0.0020 - 790ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.4052 - lr: 0.0020 - 760ms/epoch - 30ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.4030 - lr: 0.0020 - 762ms/epoch - 30ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.4034 - lr: 4.0000e-04 - 801ms/epoch - 32ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.4034 - lr: 4.0000e-04 - 807ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.4035 - lr: 4.0000e-04 - 775ms/epoch - 31ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.4032 - lr: 4.0000e-04 - 777ms/epoch - 31ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.4029 - lr: 4.0000e-04 - 768ms/epoch - 31ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.4026 - lr: 4.0000e-04 - 758ms/epoch - 30ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4025 - lr: 8.0000e-05 - 780ms/epoch - 31ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4025 - lr: 8.0000e-05 - 782ms/epoch - 31ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 8.0000e-05 - 825ms/epoch - 33ms/step\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4024 - lr: 8.0000e-05 - 759ms/epoch - 30ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.4024 - lr: 1.6000e-05 - 760ms/epoch - 30ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 1.6000e-05 - 771ms/epoch - 31ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 1.6000e-05 - 771ms/epoch - 31ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.4024 - lr: 3.2000e-06 - 758ms/epoch - 30ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.4024 - lr: 3.2000e-06 - 761ms/epoch - 30ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 3.2000e-06 - 784ms/epoch - 31ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.4024 - lr: 3.2000e-06 - 795ms/epoch - 32ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4024 - lr: 6.4000e-07 - 751ms/epoch - 30ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4024 - lr: 6.4000e-07 - 768ms/epoch - 31ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0470 - val_loss: 0.4024 - lr: 6.4000e-07 - 748ms/epoch - 30ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.4024 - lr: 6.4000e-07 - 771ms/epoch - 31ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.4024 - lr: 6.4000e-07 - 771ms/epoch - 31ms/step\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4024 - lr: 6.4000e-07 - 753ms/epoch - 30ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.4024 - lr: 1.2800e-07 - 788ms/epoch - 32ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.4024 - lr: 1.2800e-07 - 752ms/epoch - 30ms/step\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 1.2800e-07 - 789ms/epoch - 32ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.4024 - lr: 2.5600e-08 - 754ms/epoch - 30ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.4024 - lr: 2.5600e-08 - 764ms/epoch - 31ms/step\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 2.5600e-08 - 781ms/epoch - 31ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 5.1200e-09 - 766ms/epoch - 31ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4024 - lr: 5.1200e-09 - 780ms/epoch - 31ms/step\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.4024 - lr: 5.1200e-09 - 815ms/epoch - 33ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.4024 - lr: 1.0240e-09 - 770ms/epoch - 31ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.4024 - lr: 1.0240e-09 - 767ms/epoch - 31ms/step\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4024 - lr: 1.0240e-09 - 775ms/epoch - 31ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.4024 - lr: 2.0480e-10 - 774ms/epoch - 31ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.4024 - lr: 2.0480e-10 - 776ms/epoch - 31ms/step\n",
      "Epoch 87/500\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.4024 - lr: 2.0480e-10 - 783ms/epoch - 31ms/step\n",
      "Epoch 87: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.4min\n",
      "7/7 - 2s - 2s/epoch - 351ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.5min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.2926 - val_loss: 0.4221 - lr: 0.0100 - 17s/epoch - 687ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4454 - val_loss: 0.3258 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4481 - val_loss: 0.3265 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.4392 - val_loss: 0.3635 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 1.2028 - val_loss: 0.2501 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2386 - val_loss: 0.2791 - lr: 0.0020 - 908ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1751 - val_loss: 0.3095 - lr: 0.0020 - 882ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1792 - val_loss: 0.2933 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2139 - val_loss: 0.2696 - lr: 0.0020 - 907ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1746 - val_loss: 0.3024 - lr: 0.0020 - 911ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1516 - val_loss: 0.3154 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1552 - val_loss: 0.3009 - lr: 0.0020 - 902ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1474 - val_loss: 0.2892 - lr: 0.0020 - 886ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1463 - val_loss: 0.2813 - lr: 0.0020 - 910ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1650 - val_loss: 0.2941 - lr: 0.0020 - 895ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1255 - val_loss: 0.2891 - lr: 0.0020 - 886ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1072 - val_loss: 0.3016 - lr: 0.0020 - 934ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1013 - val_loss: 0.2963 - lr: 0.0020 - 875ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0928 - val_loss: 0.2751 - lr: 0.0020 - 895ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0786 - val_loss: 0.2945 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0799 - val_loss: 0.2719 - lr: 0.0020 - 879ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0804 - val_loss: 0.2779 - lr: 0.0020 - 912ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0809 - val_loss: 0.2969 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0764 - val_loss: 0.2513 - lr: 4.0000e-04 - 932ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0508 - val_loss: 0.2299 - lr: 4.0000e-04 - 924ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.2262 - lr: 4.0000e-04 - 882ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0430 - val_loss: 0.2241 - lr: 4.0000e-04 - 870ms/epoch - 35ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0406 - val_loss: 0.2204 - lr: 4.0000e-04 - 889ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0353 - val_loss: 0.2168 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0328 - val_loss: 0.2162 - lr: 4.0000e-04 - 943ms/epoch - 38ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.2149 - lr: 4.0000e-04 - 917ms/epoch - 37ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0292 - val_loss: 0.2128 - lr: 4.0000e-04 - 886ms/epoch - 35ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.2163 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0260 - val_loss: 0.2121 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0253 - val_loss: 0.2119 - lr: 4.0000e-04 - 892ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0251 - val_loss: 0.2160 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0246 - val_loss: 0.2122 - lr: 4.0000e-04 - 932ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0235 - val_loss: 0.2122 - lr: 4.0000e-04 - 903ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0236 - val_loss: 0.2089 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0228 - val_loss: 0.2121 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0233 - val_loss: 0.2102 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0222 - val_loss: 0.2114 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0220 - val_loss: 0.2084 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0215 - val_loss: 0.2077 - lr: 4.0000e-04 - 935ms/epoch - 37ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0213 - val_loss: 0.2085 - lr: 4.0000e-04 - 882ms/epoch - 35ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0215 - val_loss: 0.2087 - lr: 4.0000e-04 - 902ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0207 - val_loss: 0.2073 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0210 - val_loss: 0.2118 - lr: 4.0000e-04 - 922ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0209 - val_loss: 0.2095 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0200 - val_loss: 0.2075 - lr: 4.0000e-04 - 918ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0214 - val_loss: 0.2052 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0207 - val_loss: 0.2044 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0208 - val_loss: 0.2063 - lr: 4.0000e-04 - 892ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0209 - val_loss: 0.2021 - lr: 8.0000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0163 - val_loss: 0.1999 - lr: 8.0000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0161 - val_loss: 0.1990 - lr: 8.0000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0161 - val_loss: 0.1983 - lr: 8.0000e-05 - 944ms/epoch - 38ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0167 - val_loss: 0.1971 - lr: 8.0000e-05 - 927ms/epoch - 37ms/step\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0167 - val_loss: 0.1954 - lr: 8.0000e-05 - 882ms/epoch - 35ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0160 - val_loss: 0.1949 - lr: 1.6000e-05 - 893ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0157 - val_loss: 0.1947 - lr: 1.6000e-05 - 914ms/epoch - 37ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0157 - val_loss: 0.1945 - lr: 1.6000e-05 - 887ms/epoch - 35ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0155 - val_loss: 0.1943 - lr: 1.6000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0158 - val_loss: 0.1945 - lr: 1.6000e-05 - 925ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0155 - val_loss: 0.1939 - lr: 1.6000e-05 - 893ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0150 - val_loss: 0.1937 - lr: 1.6000e-05 - 906ms/epoch - 36ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0152 - val_loss: 0.1939 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0151 - val_loss: 0.1937 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0155 - val_loss: 0.1941 - lr: 1.6000e-05 - 899ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0149 - val_loss: 0.1941 - lr: 3.2000e-06 - 953ms/epoch - 38ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0151 - val_loss: 0.1941 - lr: 3.2000e-06 - 895ms/epoch - 36ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0151 - val_loss: 0.1939 - lr: 3.2000e-06 - 923ms/epoch - 37ms/step\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0156 - val_loss: 0.1938 - lr: 3.2000e-06 - 878ms/epoch - 35ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0155 - val_loss: 0.1938 - lr: 6.4000e-07 - 879ms/epoch - 35ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0152 - val_loss: 0.1938 - lr: 6.4000e-07 - 947ms/epoch - 38ms/step\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0150 - val_loss: 0.1938 - lr: 6.4000e-07 - 930ms/epoch - 37ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0152 - val_loss: 0.1938 - lr: 1.2800e-07 - 927ms/epoch - 37ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0150 - val_loss: 0.1938 - lr: 1.2800e-07 - 910ms/epoch - 36ms/step\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0153 - val_loss: 0.1938 - lr: 1.2800e-07 - 907ms/epoch - 36ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0156 - val_loss: 0.1938 - lr: 2.5600e-08 - 936ms/epoch - 37ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0156 - val_loss: 0.1938 - lr: 2.5600e-08 - 886ms/epoch - 35ms/step\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0156 - val_loss: 0.1938 - lr: 2.5600e-08 - 905ms/epoch - 36ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0151 - val_loss: 0.1938 - lr: 5.1200e-09 - 924ms/epoch - 37ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0156 - val_loss: 0.1938 - lr: 5.1200e-09 - 890ms/epoch - 36ms/step\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0152 - val_loss: 0.1938 - lr: 5.1200e-09 - 902ms/epoch - 36ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0151 - val_loss: 0.1938 - lr: 1.0240e-09 - 912ms/epoch - 36ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0154 - val_loss: 0.1938 - lr: 1.0240e-09 - 911ms/epoch - 36ms/step\n",
      "Epoch 88/500\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0156 - val_loss: 0.1938 - lr: 1.0240e-09 - 917ms/epoch - 37ms/step\n",
      "Epoch 88: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.6min\n",
      "7/7 - 2s - 2s/epoch - 338ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.6152 - val_loss: 0.2994 - lr: 0.0100 - 17s/epoch - 694ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4326 - val_loss: 0.4748 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5354 - val_loss: 0.4226 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6332 - val_loss: 1.0813 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.6076 - val_loss: 0.4041 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.2443 - val_loss: 0.2560 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6086 - val_loss: 0.2494 - lr: 0.0020 - 887ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.4353 - val_loss: 0.3239 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 1.8083 - val_loss: 0.3069 - lr: 4.0000e-04 - 888ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3709 - val_loss: 0.2651 - lr: 4.0000e-04 - 908ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1977 - val_loss: 0.2433 - lr: 4.0000e-04 - 917ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1670 - val_loss: 0.2392 - lr: 4.0000e-04 - 928ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1581 - val_loss: 0.2432 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1534 - val_loss: 0.2458 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1419 - val_loss: 0.2477 - lr: 4.0000e-04 - 911ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1275 - val_loss: 0.2404 - lr: 4.0000e-04 - 918ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1187 - val_loss: 0.2505 - lr: 4.0000e-04 - 895ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1131 - val_loss: 0.2443 - lr: 4.0000e-04 - 922ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1068 - val_loss: 0.2423 - lr: 4.0000e-04 - 908ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1085 - val_loss: 0.2462 - lr: 4.0000e-04 - 900ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1115 - val_loss: 0.2493 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1073 - val_loss: 0.2411 - lr: 4.0000e-04 - 885ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0985 - val_loss: 0.2086 - lr: 8.0000e-05 - 935ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0877 - val_loss: 0.2026 - lr: 8.0000e-05 - 930ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0875 - val_loss: 0.2041 - lr: 8.0000e-05 - 929ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0841 - val_loss: 0.2047 - lr: 8.0000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0840 - val_loss: 0.2010 - lr: 8.0000e-05 - 902ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0827 - val_loss: 0.1995 - lr: 8.0000e-05 - 925ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0826 - val_loss: 0.2004 - lr: 8.0000e-05 - 894ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0805 - val_loss: 0.1967 - lr: 8.0000e-05 - 909ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0792 - val_loss: 0.1989 - lr: 8.0000e-05 - 933ms/epoch - 37ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0778 - val_loss: 0.1979 - lr: 8.0000e-05 - 923ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0776 - val_loss: 0.1991 - lr: 8.0000e-05 - 889ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0785 - val_loss: 0.1956 - lr: 8.0000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0794 - val_loss: 0.1950 - lr: 8.0000e-05 - 934ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0789 - val_loss: 0.1991 - lr: 8.0000e-05 - 914ms/epoch - 37ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0769 - val_loss: 0.1954 - lr: 1.6000e-05 - 911ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0738 - val_loss: 0.1925 - lr: 1.6000e-05 - 935ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0738 - val_loss: 0.1895 - lr: 1.6000e-05 - 932ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0733 - val_loss: 0.1897 - lr: 1.6000e-05 - 928ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0740 - val_loss: 0.1897 - lr: 1.6000e-05 - 911ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0724 - val_loss: 0.1895 - lr: 1.6000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0724 - val_loss: 0.1893 - lr: 1.6000e-05 - 914ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0714 - val_loss: 0.1884 - lr: 1.6000e-05 - 940ms/epoch - 38ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0721 - val_loss: 0.1890 - lr: 1.6000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0721 - val_loss: 0.1881 - lr: 1.6000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0728 - val_loss: 0.1892 - lr: 1.6000e-05 - 877ms/epoch - 35ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0707 - val_loss: 0.1890 - lr: 3.2000e-06 - 862ms/epoch - 34ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0720 - val_loss: 0.1888 - lr: 3.2000e-06 - 896ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0699 - val_loss: 0.1887 - lr: 3.2000e-06 - 903ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0694 - val_loss: 0.1885 - lr: 3.2000e-06 - 920ms/epoch - 37ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0711 - val_loss: 0.1885 - lr: 3.2000e-06 - 892ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0704 - val_loss: 0.1883 - lr: 3.2000e-06 - 911ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0699 - val_loss: 0.1883 - lr: 3.2000e-06 - 901ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0706 - val_loss: 0.1883 - lr: 6.4000e-07 - 878ms/epoch - 35ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0709 - val_loss: 0.1882 - lr: 6.4000e-07 - 909ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0698 - val_loss: 0.1882 - lr: 6.4000e-07 - 937ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0718 - val_loss: 0.1882 - lr: 1.2800e-07 - 905ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0683 - val_loss: 0.1882 - lr: 1.2800e-07 - 881ms/epoch - 35ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0717 - val_loss: 0.1882 - lr: 1.2800e-07 - 893ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0721 - val_loss: 0.1881 - lr: 1.2800e-07 - 896ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0684 - val_loss: 0.1881 - lr: 1.2800e-07 - 891ms/epoch - 36ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0703 - val_loss: 0.1881 - lr: 2.5600e-08 - 898ms/epoch - 36ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0705 - val_loss: 0.1881 - lr: 2.5600e-08 - 932ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0701 - val_loss: 0.1881 - lr: 2.5600e-08 - 907ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "25/25 - 1s - loss: 0.0719 - val_loss: 0.1881 - lr: 5.1200e-09 - 905ms/epoch - 36ms/step\n",
      "Epoch 66: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.3min\n",
      "7/7 - 3s - 3s/epoch - 380ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.3min\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.6542 - val_loss: 0.4489 - lr: 0.0100 - 18s/epoch - 705ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4699 - val_loss: 0.4393 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4577 - val_loss: 0.3993 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4663 - val_loss: 0.3694 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.5286 - val_loss: 0.2482 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4168 - val_loss: 0.3812 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.5876 - val_loss: 0.3105 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.5706 - val_loss: 0.4022 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.6323 - val_loss: 0.3836 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 1.8713 - val_loss: 0.3981 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2746 - val_loss: 0.2463 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0949 - val_loss: 0.2644 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0951 - val_loss: 0.2687 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1044 - val_loss: 0.2855 - lr: 0.0020 - 929ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0833 - val_loss: 0.2931 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0850 - val_loss: 0.3050 - lr: 0.0020 - 923ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1058 - val_loss: 0.2381 - lr: 0.0020 - 908ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0674 - val_loss: 0.2480 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0905 - val_loss: 0.2435 - lr: 0.0020 - 868ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0818 - val_loss: 0.3187 - lr: 0.0020 - 925ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0764 - val_loss: 0.3229 - lr: 0.0020 - 912ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1076 - val_loss: 0.2692 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.2654 - lr: 4.0000e-04 - 901ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.2648 - lr: 4.0000e-04 - 904ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.2641 - lr: 4.0000e-04 - 914ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0410 - val_loss: 0.2695 - lr: 4.0000e-04 - 908ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2682 - lr: 4.0000e-04 - 909ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0411 - val_loss: 0.2717 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0423 - val_loss: 0.2664 - lr: 4.0000e-04 - 912ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0372 - val_loss: 0.2580 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0365 - val_loss: 0.2579 - lr: 8.0000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0351 - val_loss: 0.2555 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0356 - val_loss: 0.2544 - lr: 8.0000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.2511 - lr: 8.0000e-05 - 886ms/epoch - 35ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0348 - val_loss: 0.2513 - lr: 8.0000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.2493 - lr: 8.0000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.2485 - lr: 8.0000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 37: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  52.2s\n",
      "7/7 - 2s - 2s/epoch - 334ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  54.5s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 1.0715 - val_loss: 0.2464 - lr: 0.0100 - 18s/epoch - 703ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1983 - val_loss: 0.5217 - lr: 0.0100 - 961ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0875 - val_loss: 0.3522 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0757 - val_loss: 0.3668 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.3342 - val_loss: 0.3783 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.5805 - val_loss: 0.3273 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.5875 - val_loss: 0.2926 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 1.9208 - val_loss: 0.8101 - lr: 0.0020 - 918ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7428 - val_loss: 0.5046 - lr: 0.0020 - 931ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2330 - val_loss: 0.2493 - lr: 0.0020 - 901ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1582 - val_loss: 0.2461 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1035 - val_loss: 0.2457 - lr: 4.0000e-04 - 944ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0953 - val_loss: 0.2443 - lr: 4.0000e-04 - 885ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1002 - val_loss: 0.2437 - lr: 8.0000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2430 - lr: 8.0000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0898 - val_loss: 0.2422 - lr: 8.0000e-05 - 918ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0868 - val_loss: 0.2420 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0861 - val_loss: 0.2417 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0852 - val_loss: 0.2414 - lr: 1.6000e-05 - 907ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0854 - val_loss: 0.2414 - lr: 3.2000e-06 - 913ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0853 - val_loss: 0.2414 - lr: 3.2000e-06 - 897ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0851 - val_loss: 0.2413 - lr: 3.2000e-06 - 882ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0849 - val_loss: 0.2413 - lr: 6.4000e-07 - 875ms/epoch - 35ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0847 - val_loss: 0.2413 - lr: 6.4000e-07 - 904ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0848 - val_loss: 0.2413 - lr: 6.4000e-07 - 936ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0854 - val_loss: 0.2413 - lr: 1.2800e-07 - 930ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0851 - val_loss: 0.2413 - lr: 1.2800e-07 - 905ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0856 - val_loss: 0.2413 - lr: 1.2800e-07 - 885ms/epoch - 35ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0846 - val_loss: 0.2413 - lr: 2.5600e-08 - 886ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0849 - val_loss: 0.2413 - lr: 2.5600e-08 - 883ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0853 - val_loss: 0.2413 - lr: 2.5600e-08 - 908ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0846 - val_loss: 0.2413 - lr: 5.1200e-09 - 951ms/epoch - 38ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0849 - val_loss: 0.2413 - lr: 5.1200e-09 - 900ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0852 - val_loss: 0.2413 - lr: 5.1200e-09 - 891ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0846 - val_loss: 0.2413 - lr: 1.0240e-09 - 913ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0856 - val_loss: 0.2413 - lr: 1.0240e-09 - 884ms/epoch - 35ms/step\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0847 - val_loss: 0.2413 - lr: 1.0240e-09 - 907ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0850 - val_loss: 0.2413 - lr: 2.0480e-10 - 933ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0849 - val_loss: 0.2413 - lr: 2.0480e-10 - 926ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0852 - val_loss: 0.2413 - lr: 2.0480e-10 - 916ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0854 - val_loss: 0.2413 - lr: 4.0960e-11 - 902ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0847 - val_loss: 0.2413 - lr: 4.0960e-11 - 895ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0856 - val_loss: 0.2413 - lr: 4.0960e-11 - 916ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0857 - val_loss: 0.2413 - lr: 8.1920e-12 - 867ms/epoch - 35ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0850 - val_loss: 0.2413 - lr: 8.1920e-12 - 916ms/epoch - 37ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0851 - val_loss: 0.2413 - lr: 8.1920e-12 - 899ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0853 - val_loss: 0.2413 - lr: 1.6384e-12 - 919ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0851 - val_loss: 0.2413 - lr: 1.6384e-12 - 923ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0844 - val_loss: 0.2413 - lr: 1.6384e-12 - 911ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0849 - val_loss: 0.2413 - lr: 3.2768e-13 - 923ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0846 - val_loss: 0.2413 - lr: 3.2768e-13 - 939ms/epoch - 38ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.553599351567796e-14.\n",
      "25/25 - 1s - loss: 0.0851 - val_loss: 0.2413 - lr: 3.2768e-13 - 889ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0853 - val_loss: 0.2413 - lr: 6.5536e-14 - 925ms/epoch - 37ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0844 - val_loss: 0.2413 - lr: 6.5536e-14 - 914ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.310719924523668e-14.\n",
      "25/25 - 1s - loss: 0.0852 - val_loss: 0.2413 - lr: 6.5536e-14 - 920ms/epoch - 37ms/step\n",
      "Epoch 55: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 2s - 2s/epoch - 338ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.5522 - val_loss: 1.5453 - lr: 0.0100 - 17s/epoch - 699ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1972 - val_loss: 1.2856 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1923 - val_loss: 2.5608 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2568 - val_loss: 1.8385 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2300 - val_loss: 1.7639 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2150 - val_loss: 2.0562 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.4960 - val_loss: 1.0811 - lr: 0.0020 - 901ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2422 - val_loss: 2.1352 - lr: 0.0020 - 936ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2178 - val_loss: 2.7440 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2814 - val_loss: 2.9444 - lr: 4.0000e-04 - 932ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2482 - val_loss: 3.1318 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.2282 - val_loss: 3.2640 - lr: 4.0000e-04 - 948ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2229 - val_loss: 3.2973 - lr: 8.0000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2200 - val_loss: 3.3297 - lr: 8.0000e-05 - 917ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.2175 - val_loss: 3.3599 - lr: 8.0000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2160 - val_loss: 3.3662 - lr: 1.6000e-05 - 916ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2155 - val_loss: 3.3723 - lr: 1.6000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.2150 - val_loss: 3.3786 - lr: 1.6000e-05 - 877ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2147 - val_loss: 3.3798 - lr: 3.2000e-06 - 958ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2145 - val_loss: 3.3810 - lr: 3.2000e-06 - 892ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.2145 - val_loss: 3.3822 - lr: 3.2000e-06 - 883ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.2144 - val_loss: 3.3824 - lr: 6.4000e-07 - 907ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.2145 - val_loss: 3.3827 - lr: 6.4000e-07 - 904ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.2145 - val_loss: 3.3830 - lr: 6.4000e-07 - 894ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.2144 - val_loss: 3.3830 - lr: 1.2800e-07 - 906ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.2144 - val_loss: 3.3831 - lr: 1.2800e-07 - 909ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.2144 - val_loss: 3.3831 - lr: 1.2800e-07 - 923ms/epoch - 37ms/step\n",
      "Epoch 27: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.1s\n",
      "7/7 - 2s - 2s/epoch - 323ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  45.3s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.2491 - val_loss: 0.5026 - lr: 0.0100 - 16s/epoch - 648ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.9654 - val_loss: 1.5300 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9467 - val_loss: 1.6087 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.8745 - val_loss: 1.8805 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7652 - val_loss: 1.9000 - lr: 0.0020 - 777ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7643 - val_loss: 1.9197 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7631 - val_loss: 1.9361 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7507 - val_loss: 1.9391 - lr: 4.0000e-04 - 759ms/epoch - 30ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7507 - val_loss: 1.9423 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7505 - val_loss: 1.9455 - lr: 4.0000e-04 - 815ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7480 - val_loss: 1.9461 - lr: 8.0000e-05 - 771ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7480 - val_loss: 1.9468 - lr: 8.0000e-05 - 764ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7480 - val_loss: 1.9474 - lr: 8.0000e-05 - 773ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7474 - val_loss: 1.9475 - lr: 1.6000e-05 - 772ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7474 - val_loss: 1.9477 - lr: 1.6000e-05 - 790ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7474 - val_loss: 1.9478 - lr: 1.6000e-05 - 787ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7473 - val_loss: 1.9478 - lr: 3.2000e-06 - 776ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7473 - val_loss: 1.9479 - lr: 3.2000e-06 - 796ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7473 - val_loss: 1.9479 - lr: 3.2000e-06 - 774ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7473 - val_loss: 1.9479 - lr: 6.4000e-07 - 777ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7473 - val_loss: 1.9479 - lr: 6.4000e-07 - 769ms/epoch - 31ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.5s\n",
      "7/7 - 2s - 2s/epoch - 327ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  36.7s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.7011 - val_loss: 0.3008 - lr: 0.0100 - 17s/epoch - 682ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.8309 - val_loss: 2.1380 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8301 - val_loss: 1.9511 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.8358 - val_loss: 1.9464 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7752 - val_loss: 1.9532 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7764 - val_loss: 1.9685 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7757 - val_loss: 1.9816 - lr: 0.0020 - 839ms/epoch - 34ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7627 - val_loss: 1.9840 - lr: 4.0000e-04 - 819ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7626 - val_loss: 1.9867 - lr: 4.0000e-04 - 809ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7626 - val_loss: 1.9895 - lr: 4.0000e-04 - 799ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7599 - val_loss: 1.9900 - lr: 8.0000e-05 - 797ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7599 - val_loss: 1.9906 - lr: 8.0000e-05 - 801ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7599 - val_loss: 1.9911 - lr: 8.0000e-05 - 803ms/epoch - 32ms/step\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.7593 - val_loss: 1.9913 - lr: 1.6000e-05 - 780ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7593 - val_loss: 1.9914 - lr: 1.6000e-05 - 811ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7593 - val_loss: 1.9915 - lr: 1.6000e-05 - 820ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7591 - val_loss: 1.9915 - lr: 3.2000e-06 - 791ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7592 - val_loss: 1.9916 - lr: 3.2000e-06 - 814ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7592 - val_loss: 1.9916 - lr: 3.2000e-06 - 847ms/epoch - 34ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7591 - val_loss: 1.9916 - lr: 6.4000e-07 - 798ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7591 - val_loss: 1.9916 - lr: 6.4000e-07 - 822ms/epoch - 33ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.1s\n",
      "7/7 - 2s - 2s/epoch - 334ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  37.4s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.2265 - val_loss: 0.4564 - lr: 0.0100 - 16s/epoch - 641ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 2.2053 - val_loss: 1.7471 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.1062 - val_loss: 2.1222 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9131 - val_loss: 2.1414 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8614 - val_loss: 2.1480 - lr: 0.0020 - 816ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8621 - val_loss: 2.1620 - lr: 0.0020 - 843ms/epoch - 34ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8615 - val_loss: 2.1752 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8509 - val_loss: 2.1778 - lr: 4.0000e-04 - 810ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8509 - val_loss: 2.1809 - lr: 4.0000e-04 - 815ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8508 - val_loss: 2.1839 - lr: 4.0000e-04 - 792ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8485 - val_loss: 2.1845 - lr: 8.0000e-05 - 808ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8485 - val_loss: 2.1852 - lr: 8.0000e-05 - 798ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8484 - val_loss: 2.1858 - lr: 8.0000e-05 - 818ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8479 - val_loss: 2.1859 - lr: 1.6000e-05 - 791ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8479 - val_loss: 2.1861 - lr: 1.6000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8479 - val_loss: 2.1862 - lr: 1.6000e-05 - 796ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8478 - val_loss: 2.1862 - lr: 3.2000e-06 - 791ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8478 - val_loss: 2.1863 - lr: 3.2000e-06 - 791ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8478 - val_loss: 2.1863 - lr: 3.2000e-06 - 787ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8478 - val_loss: 2.1863 - lr: 6.4000e-07 - 834ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8478 - val_loss: 2.1863 - lr: 6.4000e-07 - 848ms/epoch - 34ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.7s\n",
      "7/7 - 3s - 3s/epoch - 379ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  37.3s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.2776 - val_loss: 0.4228 - lr: 0.0100 - 17s/epoch - 664ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9720 - val_loss: 0.3254 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9311 - val_loss: 2.1235 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.0326 - val_loss: 2.2226 - lr: 0.0100 - 831ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8880 - val_loss: 2.2402 - lr: 0.0020 - 776ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8874 - val_loss: 2.2935 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8819 - val_loss: 2.3432 - lr: 0.0020 - 824ms/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8617 - val_loss: 2.3526 - lr: 4.0000e-04 - 812ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8606 - val_loss: 2.3627 - lr: 4.0000e-04 - 806ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8595 - val_loss: 2.3724 - lr: 4.0000e-04 - 823ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8548 - val_loss: 2.3743 - lr: 8.0000e-05 - 804ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8547 - val_loss: 2.3762 - lr: 8.0000e-05 - 805ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8542 - val_loss: 2.3780 - lr: 8.0000e-05 - 786ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8531 - val_loss: 2.3784 - lr: 1.6000e-05 - 791ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8531 - val_loss: 2.3788 - lr: 1.6000e-05 - 784ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8532 - val_loss: 2.3792 - lr: 1.6000e-05 - 799ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8529 - val_loss: 2.3792 - lr: 3.2000e-06 - 791ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8529 - val_loss: 2.3793 - lr: 3.2000e-06 - 827ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8527 - val_loss: 2.3794 - lr: 3.2000e-06 - 829ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8528 - val_loss: 2.3794 - lr: 6.4000e-07 - 790ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.8528 - val_loss: 2.3794 - lr: 6.4000e-07 - 794ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.8528 - val_loss: 2.3794 - lr: 6.4000e-07 - 788ms/epoch - 32ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.3s\n",
      "7/7 - 3s - 3s/epoch - 376ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  37.9s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.3163 - val_loss: 0.8570 - lr: 0.0100 - 16s/epoch - 655ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3119 - val_loss: 2.4972 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3320 - val_loss: 2.8729 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3272 - val_loss: 3.5633 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2486 - val_loss: 3.6391 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2369 - val_loss: 3.6447 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2340 - val_loss: 3.6590 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2316 - val_loss: 3.6642 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2300 - val_loss: 3.6705 - lr: 0.0100 - 798ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2285 - val_loss: 3.6760 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2272 - val_loss: 3.6807 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2261 - val_loss: 3.6851 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2249 - val_loss: 3.6896 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2238 - val_loss: 3.6933 - lr: 0.0100 - 771ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2229 - val_loss: 3.6974 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2222 - val_loss: 3.7006 - lr: 0.0100 - 830ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2215 - val_loss: 3.7027 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2208 - val_loss: 3.7048 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2203 - val_loss: 3.7084 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2194 - val_loss: 3.7107 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2189 - val_loss: 3.7138 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.2s\n",
      "7/7 - 3s - 3s/epoch - 397ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  36.9s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.7586 - val_loss: 1.7386 - lr: 0.0100 - 17s/epoch - 677ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7437 - val_loss: 1.7044 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7284 - val_loss: 1.6659 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7091 - val_loss: 1.6193 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6858 - val_loss: 1.5612 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.6551 - val_loss: 1.4867 - lr: 0.0100 - 831ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6154 - val_loss: 1.3904 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.5618 - val_loss: 1.2663 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.4923 - val_loss: 1.1112 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.4064 - val_loss: 0.9310 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3129 - val_loss: 0.7459 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2290 - val_loss: 0.5840 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1660 - val_loss: 0.4625 - lr: 0.0100 - 853ms/epoch - 34ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1306 - val_loss: 0.3787 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1098 - val_loss: 0.3228 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0976 - val_loss: 0.2845 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0896 - val_loss: 0.2583 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0839 - val_loss: 0.2393 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0794 - val_loss: 0.2251 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0765 - val_loss: 0.2144 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0744 - val_loss: 0.2061 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0716 - val_loss: 0.1996 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0698 - val_loss: 0.1944 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0685 - val_loss: 0.1901 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0673 - val_loss: 0.1866 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0665 - val_loss: 0.1838 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.1813 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0648 - val_loss: 0.1793 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0644 - val_loss: 0.1775 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.1761 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1749 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.1738 - lr: 0.0100 - 852ms/epoch - 34ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.1729 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1721 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1714 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1708 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1703 - lr: 0.0100 - 830ms/epoch - 33ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1698 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1697 - lr: 0.0020 - 813ms/epoch - 33ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1696 - lr: 0.0020 - 841ms/epoch - 34ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1695 - lr: 0.0020 - 827ms/epoch - 33ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1695 - lr: 0.0020 - 941ms/epoch - 38ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1694 - lr: 0.0020 - 863ms/epoch - 35ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1693 - lr: 0.0020 - 806ms/epoch - 32ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1692 - lr: 0.0020 - 810ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1692 - lr: 0.0020 - 810ms/epoch - 32ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1691 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1690 - lr: 0.0020 - 843ms/epoch - 34ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1690 - lr: 4.0000e-04 - 854ms/epoch - 34ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1690 - lr: 4.0000e-04 - 869ms/epoch - 35ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1690 - lr: 4.0000e-04 - 857ms/epoch - 34ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1689 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 8.0000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1689 - lr: 8.0000e-05 - 842ms/epoch - 34ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1689 - lr: 8.0000e-05 - 839ms/epoch - 34ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1689 - lr: 1.6000e-05 - 888ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1689 - lr: 1.6000e-05 - 888ms/epoch - 36ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 1.6000e-05 - 857ms/epoch - 34ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1689 - lr: 1.6000e-05 - 863ms/epoch - 35ms/step\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1689 - lr: 1.6000e-05 - 800ms/epoch - 32ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 3.2000e-06 - 815ms/epoch - 33ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1689 - lr: 3.2000e-06 - 825ms/epoch - 33ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1689 - lr: 3.2000e-06 - 822ms/epoch - 33ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1689 - lr: 3.2000e-06 - 844ms/epoch - 34ms/step\n",
      "Epoch 65/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 3.2000e-06 - 805ms/epoch - 32ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 3.2000e-06 - 852ms/epoch - 34ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1689 - lr: 6.4000e-07 - 874ms/epoch - 35ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1689 - lr: 6.4000e-07 - 849ms/epoch - 34ms/step\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1689 - lr: 6.4000e-07 - 840ms/epoch - 34ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1689 - lr: 1.2800e-07 - 937ms/epoch - 37ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1689 - lr: 1.2800e-07 - 882ms/epoch - 35ms/step\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1689 - lr: 1.2800e-07 - 835ms/epoch - 33ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1689 - lr: 2.5600e-08 - 840ms/epoch - 34ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1689 - lr: 2.5600e-08 - 839ms/epoch - 34ms/step\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1689 - lr: 2.5600e-08 - 862ms/epoch - 34ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1689 - lr: 5.1200e-09 - 833ms/epoch - 33ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1689 - lr: 5.1200e-09 - 835ms/epoch - 33ms/step\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1689 - lr: 5.1200e-09 - 856ms/epoch - 34ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1689 - lr: 1.0240e-09 - 825ms/epoch - 33ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 1.0240e-09 - 840ms/epoch - 34ms/step\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1689 - lr: 1.0240e-09 - 802ms/epoch - 32ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1689 - lr: 2.0480e-10 - 814ms/epoch - 33ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1689 - lr: 2.0480e-10 - 830ms/epoch - 33ms/step\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1689 - lr: 2.0480e-10 - 868ms/epoch - 35ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1689 - lr: 4.0960e-11 - 895ms/epoch - 36ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1689 - lr: 4.0960e-11 - 851ms/epoch - 34ms/step\n",
      "Epoch 87/500\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 4.0960e-11 - 839ms/epoch - 34ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1689 - lr: 8.1920e-12 - 853ms/epoch - 34ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1689 - lr: 8.1920e-12 - 844ms/epoch - 34ms/step\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1689 - lr: 8.1920e-12 - 833ms/epoch - 33ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1689 - lr: 1.6384e-12 - 785ms/epoch - 31ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 1.6384e-12 - 867ms/epoch - 35ms/step\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1689 - lr: 1.6384e-12 - 858ms/epoch - 34ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1689 - lr: 3.2768e-13 - 808ms/epoch - 32ms/step\n",
      "Epoch 95/500\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1689 - lr: 3.2768e-13 - 833ms/epoch - 33ms/step\n",
      "Epoch 95: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.6min\n",
      "7/7 - 2s - 2s/epoch - 330ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.7427 - val_loss: 1.6729 - lr: 0.0100 - 16s/epoch - 657ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7314 - val_loss: 1.6482 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7182 - val_loss: 1.6180 - lr: 0.0100 - 848ms/epoch - 34ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7017 - val_loss: 1.5798 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6808 - val_loss: 1.5304 - lr: 0.0100 - 840ms/epoch - 34ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.6537 - val_loss: 1.4654 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6183 - val_loss: 1.3793 - lr: 0.0100 - 866ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.5703 - val_loss: 1.2646 - lr: 0.0100 - 843ms/epoch - 34ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.5084 - val_loss: 1.1153 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.4281 - val_loss: 0.9311 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3359 - val_loss: 0.7275 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2416 - val_loss: 0.5367 - lr: 0.0100 - 827ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1650 - val_loss: 0.3907 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1165 - val_loss: 0.2967 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0898 - val_loss: 0.2423 - lr: 0.0100 - 858ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0764 - val_loss: 0.2121 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0694 - val_loss: 0.1950 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.1851 - lr: 0.0100 - 849ms/epoch - 34ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1791 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0625 - val_loss: 0.1753 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1727 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1709 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.1696 - lr: 0.0100 - 859ms/epoch - 34ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1686 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1679 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1673 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.1668 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.1664 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1661 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1658 - lr: 0.0100 - 843ms/epoch - 34ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1656 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1654 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1653 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.1651 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1648 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1646 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1644 - lr: 0.0100 - 878ms/epoch - 35ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1643 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1643 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1640 - lr: 0.0100 - 879ms/epoch - 35ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1637 - lr: 0.0100 - 816ms/epoch - 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1635 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1632 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1631 - lr: 0.0100 - 839ms/epoch - 34ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1630 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1629 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1628 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1625 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1623 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1620 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1619 - lr: 0.0020 - 806ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.1619 - lr: 0.0020 - 860ms/epoch - 34ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1619 - lr: 0.0020 - 803ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1617 - lr: 0.0020 - 811ms/epoch - 32ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1617 - lr: 0.0020 - 823ms/epoch - 33ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1617 - lr: 0.0020 - 800ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1616 - lr: 0.0020 - 795ms/epoch - 32ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1615 - lr: 0.0020 - 825ms/epoch - 33ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1614 - lr: 0.0020 - 869ms/epoch - 35ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.1614 - lr: 0.0020 - 812ms/epoch - 32ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1613 - lr: 0.0020 - 825ms/epoch - 33ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1612 - lr: 0.0020 - 836ms/epoch - 33ms/step\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.1612 - lr: 0.0020 - 821ms/epoch - 33ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1612 - lr: 4.0000e-04 - 809ms/epoch - 32ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1612 - lr: 4.0000e-04 - 771ms/epoch - 31ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1612 - lr: 4.0000e-04 - 823ms/epoch - 33ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1611 - lr: 8.0000e-05 - 830ms/epoch - 33ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1611 - lr: 8.0000e-05 - 815ms/epoch - 33ms/step\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1611 - lr: 8.0000e-05 - 825ms/epoch - 33ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1611 - lr: 1.6000e-05 - 814ms/epoch - 33ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.1611 - lr: 1.6000e-05 - 794ms/epoch - 32ms/step\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1611 - lr: 1.6000e-05 - 802ms/epoch - 32ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1611 - lr: 3.2000e-06 - 838ms/epoch - 34ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1611 - lr: 3.2000e-06 - 810ms/epoch - 32ms/step\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1611 - lr: 3.2000e-06 - 807ms/epoch - 32ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1611 - lr: 6.4000e-07 - 787ms/epoch - 31ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1611 - lr: 6.4000e-07 - 869ms/epoch - 35ms/step\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1611 - lr: 6.4000e-07 - 787ms/epoch - 31ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1611 - lr: 1.2800e-07 - 871ms/epoch - 35ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1611 - lr: 1.2800e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1611 - lr: 1.2800e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1611 - lr: 2.5600e-08 - 934ms/epoch - 37ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1611 - lr: 2.5600e-08 - 832ms/epoch - 33ms/step\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.1611 - lr: 2.5600e-08 - 843ms/epoch - 34ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1611 - lr: 5.1200e-09 - 808ms/epoch - 32ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1611 - lr: 5.1200e-09 - 857ms/epoch - 34ms/step\n",
      "Epoch 87/500\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1611 - lr: 5.1200e-09 - 875ms/epoch - 35ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1611 - lr: 1.0240e-09 - 841ms/epoch - 34ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1611 - lr: 1.0240e-09 - 824ms/epoch - 33ms/step\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1611 - lr: 1.0240e-09 - 822ms/epoch - 33ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1611 - lr: 2.0480e-10 - 827ms/epoch - 33ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1611 - lr: 2.0480e-10 - 821ms/epoch - 33ms/step\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1611 - lr: 2.0480e-10 - 925ms/epoch - 37ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.1611 - lr: 4.0960e-11 - 839ms/epoch - 34ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1611 - lr: 4.0960e-11 - 943ms/epoch - 38ms/step\n",
      "Epoch 96/500\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1611 - lr: 4.0960e-11 - 879ms/epoch - 35ms/step\n",
      "Epoch 96: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.6min\n",
      "7/7 - 2s - 2s/epoch - 326ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.8910 - val_loss: 1.7366 - lr: 0.0100 - 17s/epoch - 692ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8671 - val_loss: 1.6958 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8414 - val_loss: 1.6479 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8090 - val_loss: 1.5861 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7645 - val_loss: 1.5000 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7032 - val_loss: 1.3743 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6083 - val_loss: 1.1844 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.4712 - val_loss: 0.9164 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3018 - val_loss: 0.6173 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1601 - val_loss: 0.3940 - lr: 0.0100 - 839ms/epoch - 34ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0873 - val_loss: 0.2770 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.2230 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1971 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0414 - val_loss: 0.1837 - lr: 0.0100 - 791ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0367 - val_loss: 0.1760 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1716 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0321 - val_loss: 0.1690 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1675 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0302 - val_loss: 0.1666 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0293 - val_loss: 0.1661 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1657 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.1654 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0282 - val_loss: 0.1652 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0286 - val_loss: 0.1651 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0284 - val_loss: 0.1651 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0284 - val_loss: 0.1648 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0278 - val_loss: 0.1647 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.1647 - lr: 0.0020 - 801ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.1646 - lr: 0.0020 - 775ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.1646 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0278 - val_loss: 0.1646 - lr: 0.0020 - 775ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1645 - lr: 0.0020 - 809ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1645 - lr: 0.0020 - 804ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1644 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1644 - lr: 0.0020 - 810ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1643 - lr: 0.0020 - 793ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.1642 - lr: 0.0020 - 808ms/epoch - 32ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1642 - lr: 4.0000e-04 - 793ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0278 - val_loss: 0.1642 - lr: 4.0000e-04 - 822ms/epoch - 33ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1642 - lr: 4.0000e-04 - 820ms/epoch - 33ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 4.0000e-04 - 822ms/epoch - 33ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1642 - lr: 4.0000e-04 - 792ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 4.0000e-04 - 801ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1642 - lr: 8.0000e-05 - 822ms/epoch - 33ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 8.0000e-05 - 783ms/epoch - 31ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.1642 - lr: 8.0000e-05 - 802ms/epoch - 32ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1642 - lr: 1.6000e-05 - 816ms/epoch - 33ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0278 - val_loss: 0.1642 - lr: 1.6000e-05 - 780ms/epoch - 31ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.1642 - lr: 1.6000e-05 - 809ms/epoch - 32ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 3.2000e-06 - 785ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1642 - lr: 3.2000e-06 - 810ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1642 - lr: 3.2000e-06 - 768ms/epoch - 31ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1642 - lr: 6.4000e-07 - 776ms/epoch - 31ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 6.4000e-07 - 821ms/epoch - 33ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1642 - lr: 6.4000e-07 - 821ms/epoch - 33ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1642 - lr: 1.2800e-07 - 783ms/epoch - 31ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1642 - lr: 1.2800e-07 - 769ms/epoch - 31ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 1.2800e-07 - 786ms/epoch - 31ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1642 - lr: 2.5600e-08 - 803ms/epoch - 32ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 2.5600e-08 - 816ms/epoch - 33ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1642 - lr: 2.5600e-08 - 803ms/epoch - 32ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1642 - lr: 2.5600e-08 - 826ms/epoch - 33ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1642 - lr: 5.1200e-09 - 789ms/epoch - 32ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1642 - lr: 5.1200e-09 - 807ms/epoch - 32ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1642 - lr: 5.1200e-09 - 794ms/epoch - 32ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1642 - lr: 1.0240e-09 - 778ms/epoch - 31ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1642 - lr: 1.0240e-09 - 783ms/epoch - 31ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1642 - lr: 1.0240e-09 - 804ms/epoch - 32ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0278 - val_loss: 0.1642 - lr: 2.0480e-10 - 807ms/epoch - 32ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1642 - lr: 2.0480e-10 - 770ms/epoch - 31ms/step\n",
      "Epoch 71/500\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1642 - lr: 2.0480e-10 - 834ms/epoch - 33ms/step\n",
      "Epoch 71: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.3min\n",
      "7/7 - 2s - 2s/epoch - 327ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.3min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.9468 - val_loss: 1.7223 - lr: 0.0100 - 16s/epoch - 640ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9326 - val_loss: 1.7027 - lr: 0.0100 - 842ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9159 - val_loss: 1.6775 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8937 - val_loss: 1.6419 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8629 - val_loss: 1.5880 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8145 - val_loss: 1.4995 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7338 - val_loss: 1.3450 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.5956 - val_loss: 1.0786 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3846 - val_loss: 0.7122 - lr: 0.0100 - 830ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1897 - val_loss: 0.4279 - lr: 0.0100 - 847ms/epoch - 34ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1091 - val_loss: 0.2975 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0861 - val_loss: 0.2415 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0763 - val_loss: 0.2130 - lr: 0.0100 - 821ms/epoch - 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0694 - val_loss: 0.1961 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0637 - val_loss: 0.1860 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1795 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1753 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1727 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1710 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1699 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1692 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1686 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1683 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1680 - lr: 0.0100 - 840ms/epoch - 34ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1679 - lr: 0.0100 - 847ms/epoch - 34ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1676 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1676 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1674 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.1673 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.1672 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1671 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1670 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0510 - val_loss: 0.1666 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1665 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0510 - val_loss: 0.1663 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1661 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1658 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0507 - val_loss: 0.1658 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1654 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1653 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1650 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1648 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1643 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1642 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1640 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1641 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1638 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1632 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1632 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1627 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1624 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1620 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1618 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1615 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1615 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1612 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1610 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1609 - lr: 0.0020 - 806ms/epoch - 32ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1607 - lr: 0.0020 - 807ms/epoch - 32ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1606 - lr: 0.0020 - 785ms/epoch - 31ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1605 - lr: 0.0020 - 831ms/epoch - 33ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1604 - lr: 0.0020 - 824ms/epoch - 33ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1604 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1603 - lr: 4.0000e-04 - 786ms/epoch - 31ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 4.0000e-04 - 815ms/epoch - 33ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 8.0000e-05 - 796ms/epoch - 32ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 8.0000e-05 - 795ms/epoch - 32ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1603 - lr: 8.0000e-05 - 826ms/epoch - 33ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1603 - lr: 1.6000e-05 - 848ms/epoch - 34ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1603 - lr: 1.6000e-05 - 859ms/epoch - 34ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 1.6000e-05 - 889ms/epoch - 36ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1603 - lr: 1.6000e-05 - 844ms/epoch - 34ms/step\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 1.6000e-05 - 816ms/epoch - 33ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1603 - lr: 3.2000e-06 - 1s/epoch - 54ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 3.2000e-06 - 946ms/epoch - 38ms/step\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1603 - lr: 3.2000e-06 - 857ms/epoch - 34ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1603 - lr: 6.4000e-07 - 841ms/epoch - 34ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.1603 - lr: 6.4000e-07 - 859ms/epoch - 34ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1603 - lr: 6.4000e-07 - 827ms/epoch - 33ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1603 - lr: 6.4000e-07 - 846ms/epoch - 34ms/step\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1603 - lr: 6.4000e-07 - 821ms/epoch - 33ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1603 - lr: 1.2800e-07 - 881ms/epoch - 35ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 1.2800e-07 - 823ms/epoch - 33ms/step\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1603 - lr: 1.2800e-07 - 819ms/epoch - 33ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1603 - lr: 2.5600e-08 - 837ms/epoch - 33ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.1603 - lr: 2.5600e-08 - 818ms/epoch - 33ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1603 - lr: 2.5600e-08 - 815ms/epoch - 33ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1603 - lr: 2.5600e-08 - 979ms/epoch - 39ms/step\n",
      "Epoch 89/500\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1603 - lr: 2.5600e-08 - 989ms/epoch - 40ms/step\n",
      "Epoch 90/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1603 - lr: 5.1200e-09 - 909ms/epoch - 36ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1603 - lr: 5.1200e-09 - 897ms/epoch - 36ms/step\n",
      "Epoch 92/500\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1603 - lr: 5.1200e-09 - 857ms/epoch - 34ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1603 - lr: 1.0240e-09 - 857ms/epoch - 34ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 1.0240e-09 - 863ms/epoch - 35ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.1603 - lr: 1.0240e-09 - 782ms/epoch - 31ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1603 - lr: 1.0240e-09 - 816ms/epoch - 33ms/step\n",
      "Epoch 97/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 1.0240e-09 - 813ms/epoch - 33ms/step\n",
      "Epoch 98/500\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1603 - lr: 1.0240e-09 - 800ms/epoch - 32ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1603 - lr: 2.0480e-10 - 788ms/epoch - 32ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1603 - lr: 2.0480e-10 - 797ms/epoch - 32ms/step\n",
      "Epoch 101/500\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1603 - lr: 2.0480e-10 - 815ms/epoch - 33ms/step\n",
      "Epoch 101: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.7min\n",
      "7/7 - 2s - 2s/epoch - 341ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.6869 - val_loss: 1.8477 - lr: 0.0100 - 19s/epoch - 769ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6693 - val_loss: 1.8372 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6518 - val_loss: 1.8264 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6332 - val_loss: 1.8139 - lr: 0.0100 - 881ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6121 - val_loss: 1.7976 - lr: 0.0100 - 830ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.5856 - val_loss: 1.7744 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.5499 - val_loss: 1.7387 - lr: 0.0100 - 844ms/epoch - 34ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.4995 - val_loss: 1.6814 - lr: 0.0100 - 841ms/epoch - 34ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.4250 - val_loss: 1.5876 - lr: 0.0100 - 994ms/epoch - 40ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3223 - val_loss: 1.4449 - lr: 0.0100 - 844ms/epoch - 34ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2086 - val_loss: 1.2748 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1310 - val_loss: 1.1371 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0993 - val_loss: 1.0523 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0880 - val_loss: 1.0003 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0831 - val_loss: 0.9653 - lr: 0.0100 - 841ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0810 - val_loss: 0.9357 - lr: 0.0100 - 842ms/epoch - 34ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0792 - val_loss: 0.9094 - lr: 0.0100 - 853ms/epoch - 34ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0770 - val_loss: 0.8870 - lr: 0.0100 - 839ms/epoch - 34ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0752 - val_loss: 0.8659 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0737 - val_loss: 0.8458 - lr: 0.0100 - 879ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0725 - val_loss: 0.8278 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0706 - val_loss: 0.8105 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0695 - val_loss: 0.7940 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0683 - val_loss: 0.7788 - lr: 0.0100 - 866ms/epoch - 35ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0676 - val_loss: 0.7630 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0668 - val_loss: 0.7497 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.7360 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.7241 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.7123 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.7018 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.6913 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.6807 - lr: 0.0100 - 843ms/epoch - 34ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.6705 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.6618 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.6531 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.6445 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.6365 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.6293 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.6220 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.6149 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.6086 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0580 - val_loss: 0.6023 - lr: 0.0100 - 839ms/epoch - 34ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.5957 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.5899 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.5843 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.5789 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.5734 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.5692 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.5646 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.5600 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.5557 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.5519 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.5476 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.5440 - lr: 0.0100 - 854ms/epoch - 34ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.5401 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.5367 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.5336 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.5299 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.5268 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.5237 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.5207 - lr: 0.0100 - 856ms/epoch - 34ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.5185 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.5157 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.5130 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.5116 - lr: 0.0020 - 842ms/epoch - 34ms/step\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.5103 - lr: 0.0020 - 828ms/epoch - 33ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.5091 - lr: 0.0020 - 809ms/epoch - 32ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.5081 - lr: 0.0020 - 839ms/epoch - 34ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.5070 - lr: 0.0020 - 800ms/epoch - 32ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.5062 - lr: 0.0020 - 828ms/epoch - 33ms/step\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.5053 - lr: 0.0020 - 802ms/epoch - 32ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.5051 - lr: 4.0000e-04 - 803ms/epoch - 32ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.5049 - lr: 4.0000e-04 - 812ms/epoch - 32ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.5047 - lr: 4.0000e-04 - 801ms/epoch - 32ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.5045 - lr: 4.0000e-04 - 853ms/epoch - 34ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.5043 - lr: 4.0000e-04 - 819ms/epoch - 33ms/step\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.5041 - lr: 4.0000e-04 - 810ms/epoch - 32ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.5041 - lr: 8.0000e-05 - 809ms/epoch - 32ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.5040 - lr: 8.0000e-05 - 809ms/epoch - 32ms/step\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.5040 - lr: 8.0000e-05 - 803ms/epoch - 32ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.5040 - lr: 1.6000e-05 - 801ms/epoch - 32ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.5040 - lr: 1.6000e-05 - 803ms/epoch - 32ms/step\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.5040 - lr: 1.6000e-05 - 809ms/epoch - 32ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.5040 - lr: 3.2000e-06 - 791ms/epoch - 32ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.5040 - lr: 3.2000e-06 - 806ms/epoch - 32ms/step\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.5040 - lr: 3.2000e-06 - 809ms/epoch - 32ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.5040 - lr: 6.4000e-07 - 810ms/epoch - 32ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.5040 - lr: 6.4000e-07 - 795ms/epoch - 32ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.5040 - lr: 6.4000e-07 - 793ms/epoch - 32ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.5040 - lr: 6.4000e-07 - 826ms/epoch - 33ms/step\n",
      "Epoch 91/500\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.5040 - lr: 6.4000e-07 - 818ms/epoch - 33ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.5040 - lr: 1.2800e-07 - 804ms/epoch - 32ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.5040 - lr: 1.2800e-07 - 793ms/epoch - 32ms/step\n",
      "Epoch 94/500\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.5040 - lr: 1.2800e-07 - 775ms/epoch - 31ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.5040 - lr: 2.5600e-08 - 815ms/epoch - 33ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.5040 - lr: 2.5600e-08 - 813ms/epoch - 33ms/step\n",
      "Epoch 97/500\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.5040 - lr: 2.5600e-08 - 843ms/epoch - 34ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.5040 - lr: 5.1200e-09 - 811ms/epoch - 32ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.5040 - lr: 5.1200e-09 - 798ms/epoch - 32ms/step\n",
      "Epoch 100/500\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.5040 - lr: 5.1200e-09 - 782ms/epoch - 31ms/step\n",
      "Epoch 101/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.5040 - lr: 1.0240e-09 - 762ms/epoch - 30ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.5040 - lr: 1.0240e-09 - 782ms/epoch - 31ms/step\n",
      "Epoch 103/500\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.5040 - lr: 1.0240e-09 - 778ms/epoch - 31ms/step\n",
      "Epoch 104/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.5040 - lr: 2.0480e-10 - 811ms/epoch - 32ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.5040 - lr: 2.0480e-10 - 798ms/epoch - 32ms/step\n",
      "Epoch 106/500\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.5040 - lr: 2.0480e-10 - 784ms/epoch - 31ms/step\n",
      "Epoch 107/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.5040 - lr: 4.0960e-11 - 802ms/epoch - 32ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.5040 - lr: 4.0960e-11 - 772ms/epoch - 31ms/step\n",
      "Epoch 109/500\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.5040 - lr: 4.0960e-11 - 763ms/epoch - 31ms/step\n",
      "Epoch 110/500\n",
      "Restoring model weights from the end of the best epoch: 90.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.5040 - lr: 8.1920e-12 - 804ms/epoch - 32ms/step\n",
      "Epoch 110: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.9min\n",
      "7/7 - 2s - 2s/epoch - 333ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.9min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.4122 - val_loss: 0.2038 - lr: 0.0100 - 17s/epoch - 676ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1395 - val_loss: 0.2244 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0648 - val_loss: 0.2160 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0625 - val_loss: 0.2047 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1933 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1917 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1800 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1775 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0710 - val_loss: 0.1669 - lr: 0.0020 - 772ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1599 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1575 - lr: 0.0020 - 775ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.1585 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1589 - lr: 4.0000e-04 - 775ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1590 - lr: 4.0000e-04 - 790ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.1587 - lr: 4.0000e-04 - 765ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0576 - val_loss: 0.1586 - lr: 4.0000e-04 - 790ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.1583 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.1578 - lr: 4.0000e-04 - 824ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.1575 - lr: 4.0000e-04 - 838ms/epoch - 34ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1569 - lr: 4.0000e-04 - 788ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1569 - lr: 8.0000e-05 - 809ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1569 - lr: 8.0000e-05 - 805ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1569 - lr: 8.0000e-05 - 772ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1569 - lr: 8.0000e-05 - 784ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1569 - lr: 8.0000e-05 - 806ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1568 - lr: 8.0000e-05 - 796ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1568 - lr: 8.0000e-05 - 799ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.1568 - lr: 8.0000e-05 - 813ms/epoch - 33ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1568 - lr: 1.6000e-05 - 804ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.1568 - lr: 1.6000e-05 - 810ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1568 - lr: 1.6000e-05 - 784ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1568 - lr: 3.2000e-06 - 800ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.1568 - lr: 3.2000e-06 - 794ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1568 - lr: 3.2000e-06 - 787ms/epoch - 31ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.1568 - lr: 6.4000e-07 - 772ms/epoch - 31ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1568 - lr: 6.4000e-07 - 798ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1568 - lr: 6.4000e-07 - 845ms/epoch - 34ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1568 - lr: 1.2800e-07 - 780ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.1568 - lr: 1.2800e-07 - 792ms/epoch - 32ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1568 - lr: 1.2800e-07 - 812ms/epoch - 32ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1568 - lr: 1.2800e-07 - 802ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.1568 - lr: 1.2800e-07 - 789ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.1568 - lr: 1.2800e-07 - 775ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.1568 - lr: 2.5600e-08 - 888ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1568 - lr: 2.5600e-08 - 808ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1568 - lr: 2.5600e-08 - 780ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1568 - lr: 5.1200e-09 - 810ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1568 - lr: 5.1200e-09 - 778ms/epoch - 31ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1568 - lr: 5.1200e-09 - 798ms/epoch - 32ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1568 - lr: 1.0240e-09 - 804ms/epoch - 32ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1568 - lr: 1.0240e-09 - 783ms/epoch - 31ms/step\n",
      "Epoch 52/500\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1568 - lr: 1.0240e-09 - 826ms/epoch - 33ms/step\n",
      "Epoch 52: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  59.5s\n",
      "7/7 - 2s - 2s/epoch - 334ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.0min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.5450 - val_loss: 0.4950 - lr: 0.0100 - 17s/epoch - 679ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2169 - val_loss: 0.2342 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0704 - val_loss: 0.2165 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.2048 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1930 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1911 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1766 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1745 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1676 - lr: 0.0100 - 840ms/epoch - 34ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.1666 - lr: 0.0100 - 849ms/epoch - 34ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1624 - lr: 0.0100 - 865ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.1555 - lr: 0.0100 - 830ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.1561 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.1508 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1501 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1492 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1447 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1459 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1428 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1399 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1374 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1374 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1406 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1353 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1361 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1330 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1335 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1335 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1343 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1271 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1305 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1265 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.1295 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0468 - val_loss: 0.1277 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1266 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1232 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0457 - val_loss: 0.1261 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0450 - val_loss: 0.1231 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.1236 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.1200 - lr: 0.0100 - 842ms/epoch - 34ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0444 - val_loss: 0.1188 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0439 - val_loss: 0.1246 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0434 - val_loss: 0.1200 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0435 - val_loss: 0.1207 - lr: 0.0100 - 783ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0432 - val_loss: 0.1181 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0429 - val_loss: 0.1167 - lr: 0.0100 - 827ms/epoch - 33ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0427 - val_loss: 0.1205 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0421 - val_loss: 0.1194 - lr: 0.0100 - 839ms/epoch - 34ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0420 - val_loss: 0.1164 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0419 - val_loss: 0.1225 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0414 - val_loss: 0.1163 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0416 - val_loss: 0.1137 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0416 - val_loss: 0.1120 - lr: 0.0100 - 844ms/epoch - 34ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0411 - val_loss: 0.1125 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0411 - val_loss: 0.1126 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0406 - val_loss: 0.1065 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0408 - val_loss: 0.1092 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1080 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0394 - val_loss: 0.1154 - lr: 0.0100 - 863ms/epoch - 35ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0389 - val_loss: 0.1077 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0393 - val_loss: 0.1122 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0395 - val_loss: 0.1102 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0385 - val_loss: 0.1107 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0385 - val_loss: 0.1089 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0386 - val_loss: 0.1068 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0381 - val_loss: 0.1060 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0384 - val_loss: 0.1062 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0375 - val_loss: 0.1058 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0372 - val_loss: 0.1064 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.1075 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1009 - lr: 0.0100 - 852ms/epoch - 34ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1017 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0373 - val_loss: 0.1057 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0388 - val_loss: 0.1038 - lr: 0.0020 - 821ms/epoch - 33ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0363 - val_loss: 0.1020 - lr: 0.0020 - 819ms/epoch - 33ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0355 - val_loss: 0.1003 - lr: 0.0020 - 803ms/epoch - 32ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0356 - val_loss: 0.0991 - lr: 0.0020 - 883ms/epoch - 35ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0352 - val_loss: 0.0993 - lr: 0.0020 - 813ms/epoch - 33ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0353 - val_loss: 0.0994 - lr: 0.0020 - 937ms/epoch - 37ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0344 - val_loss: 0.0995 - lr: 0.0020 - 834ms/epoch - 33ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.0995 - lr: 0.0020 - 791ms/epoch - 32ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.0979 - lr: 0.0020 - 795ms/epoch - 32ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.0994 - lr: 0.0020 - 793ms/epoch - 32ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.0985 - lr: 0.0020 - 837ms/epoch - 33ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.0980 - lr: 0.0020 - 882ms/epoch - 35ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.0982 - lr: 0.0020 - 840ms/epoch - 34ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.0971 - lr: 0.0020 - 921ms/epoch - 37ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.0972 - lr: 0.0020 - 872ms/epoch - 35ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0336 - val_loss: 0.0978 - lr: 0.0020 - 820ms/epoch - 33ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.0968 - lr: 0.0020 - 966ms/epoch - 39ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.0971 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.0970 - lr: 0.0020 - 889ms/epoch - 36ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.0968 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0332 - val_loss: 0.0981 - lr: 0.0020 - 814ms/epoch - 33ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.0971 - lr: 0.0020 - 827ms/epoch - 33ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0334 - val_loss: 0.0965 - lr: 0.0020 - 820ms/epoch - 33ms/step\n",
      "Epoch 97/500\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0334 - val_loss: 0.0959 - lr: 0.0020 - 836ms/epoch - 33ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0330 - val_loss: 0.0965 - lr: 4.0000e-04 - 816ms/epoch - 33ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0328 - val_loss: 0.0968 - lr: 4.0000e-04 - 877ms/epoch - 35ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0334 - val_loss: 0.0972 - lr: 4.0000e-04 - 863ms/epoch - 35ms/step\n",
      "Epoch 101/500\n",
      "25/25 - 1s - loss: 0.0329 - val_loss: 0.0969 - lr: 4.0000e-04 - 811ms/epoch - 32ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.0969 - lr: 4.0000e-04 - 797ms/epoch - 32ms/step\n",
      "Epoch 103/500\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.0971 - lr: 4.0000e-04 - 798ms/epoch - 32ms/step\n",
      "Epoch 104/500\n",
      "25/25 - 1s - loss: 0.0330 - val_loss: 0.0969 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.0967 - lr: 4.0000e-04 - 786ms/epoch - 31ms/step\n",
      "Epoch 106/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.0965 - lr: 4.0000e-04 - 834ms/epoch - 33ms/step\n",
      "Epoch 107/500\n",
      "25/25 - 1s - loss: 0.0331 - val_loss: 0.0964 - lr: 4.0000e-04 - 831ms/epoch - 33ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.0963 - lr: 4.0000e-04 - 800ms/epoch - 32ms/step\n",
      "Epoch 109/500\n",
      "25/25 - 1s - loss: 0.0322 - val_loss: 0.0961 - lr: 4.0000e-04 - 805ms/epoch - 32ms/step\n",
      "Epoch 110/500\n",
      "25/25 - 1s - loss: 0.0325 - val_loss: 0.0963 - lr: 4.0000e-04 - 810ms/epoch - 32ms/step\n",
      "Epoch 111/500\n",
      "25/25 - 1s - loss: 0.0322 - val_loss: 0.0961 - lr: 4.0000e-04 - 802ms/epoch - 32ms/step\n",
      "Epoch 112/500\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0329 - val_loss: 0.0962 - lr: 4.0000e-04 - 819ms/epoch - 33ms/step\n",
      "Epoch 113/500\n",
      "25/25 - 1s - loss: 0.0328 - val_loss: 0.0962 - lr: 8.0000e-05 - 841ms/epoch - 34ms/step\n",
      "Epoch 114/500\n",
      "25/25 - 1s - loss: 0.0325 - val_loss: 0.0963 - lr: 8.0000e-05 - 812ms/epoch - 32ms/step\n",
      "Epoch 115/500\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0324 - val_loss: 0.0963 - lr: 8.0000e-05 - 817ms/epoch - 33ms/step\n",
      "Epoch 116/500\n",
      "25/25 - 1s - loss: 0.0323 - val_loss: 0.0963 - lr: 1.6000e-05 - 804ms/epoch - 32ms/step\n",
      "Epoch 117/500\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "25/25 - 1s - loss: 0.0330 - val_loss: 0.0963 - lr: 1.6000e-05 - 799ms/epoch - 32ms/step\n",
      "Epoch 117: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.9min\n",
      "7/7 - 2s - 2s/epoch - 351ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.9min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.3597 - val_loss: 0.2111 - lr: 0.0100 - 16s/epoch - 652ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.0742 - val_loss: 0.2257 - lr: 0.0100 - 831ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0282 - val_loss: 0.2163 - lr: 0.0100 - 846ms/epoch - 34ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0286 - val_loss: 0.2065 - lr: 0.0100 - 836ms/epoch - 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0290 - val_loss: 0.1988 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1967 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0438 - val_loss: 0.1756 - lr: 0.0020 - 932ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0383 - val_loss: 0.1682 - lr: 0.0020 - 860ms/epoch - 34ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0361 - val_loss: 0.1656 - lr: 0.0020 - 868ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0361 - val_loss: 0.1667 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0352 - val_loss: 0.1672 - lr: 4.0000e-04 - 808ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0352 - val_loss: 0.1674 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1675 - lr: 8.0000e-05 - 824ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1676 - lr: 8.0000e-05 - 818ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1677 - lr: 8.0000e-05 - 797ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.1677 - lr: 1.6000e-05 - 774ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1677 - lr: 1.6000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.1677 - lr: 1.6000e-05 - 776ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1677 - lr: 3.2000e-06 - 789ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0344 - val_loss: 0.1677 - lr: 3.2000e-06 - 788ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1677 - lr: 3.2000e-06 - 807ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1677 - lr: 6.4000e-07 - 857ms/epoch - 34ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1677 - lr: 6.4000e-07 - 805ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0344 - val_loss: 0.1677 - lr: 6.4000e-07 - 804ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1677 - lr: 1.2800e-07 - 788ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1677 - lr: 1.2800e-07 - 788ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1677 - lr: 1.2800e-07 - 787ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1677 - lr: 2.5600e-08 - 823ms/epoch - 33ms/step\n",
      "Epoch 29/500\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "25/25 - 1s - loss: 0.0345 - val_loss: 0.1677 - lr: 2.5600e-08 - 825ms/epoch - 33ms/step\n",
      "Epoch 29: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  42.0s\n",
      "7/7 - 3s - 3s/epoch - 408ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time=  44.8s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.4960 - val_loss: 0.2167 - lr: 0.0100 - 17s/epoch - 693ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.0768 - val_loss: 0.2224 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.2160 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0510 - val_loss: 0.2078 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1996 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.1926 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1876 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.1828 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1762 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1721 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1623 - lr: 0.0020 - 832ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1585 - lr: 0.0020 - 791ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1558 - lr: 0.0020 - 811ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1563 - lr: 4.0000e-04 - 773ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0491 - val_loss: 0.1563 - lr: 4.0000e-04 - 780ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1564 - lr: 4.0000e-04 - 770ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1561 - lr: 4.0000e-04 - 775ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1561 - lr: 4.0000e-04 - 801ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1557 - lr: 4.0000e-04 - 824ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1554 - lr: 4.0000e-04 - 801ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1551 - lr: 4.0000e-04 - 792ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1548 - lr: 4.0000e-04 - 790ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1544 - lr: 4.0000e-04 - 779ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1539 - lr: 4.0000e-04 - 779ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1539 - lr: 8.0000e-05 - 770ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1539 - lr: 8.0000e-05 - 797ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1539 - lr: 8.0000e-05 - 771ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1539 - lr: 8.0000e-05 - 794ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1539 - lr: 8.0000e-05 - 774ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1538 - lr: 8.0000e-05 - 776ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1538 - lr: 1.6000e-05 - 768ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.1538 - lr: 1.6000e-05 - 798ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1538 - lr: 1.6000e-05 - 807ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1538 - lr: 3.2000e-06 - 815ms/epoch - 33ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1538 - lr: 3.2000e-06 - 784ms/epoch - 31ms/step\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1538 - lr: 3.2000e-06 - 803ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.1538 - lr: 6.4000e-07 - 792ms/epoch - 32ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.1538 - lr: 6.4000e-07 - 770ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.1538 - lr: 6.4000e-07 - 810ms/epoch - 32ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1538 - lr: 1.2800e-07 - 797ms/epoch - 32ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.1538 - lr: 1.2800e-07 - 807ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.1538 - lr: 1.2800e-07 - 811ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1538 - lr: 2.5600e-08 - 796ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.1538 - lr: 2.5600e-08 - 782ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1538 - lr: 2.5600e-08 - 812ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1538 - lr: 2.5600e-08 - 789ms/epoch - 32ms/step\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.1538 - lr: 2.5600e-08 - 793ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1538 - lr: 5.1200e-09 - 791ms/epoch - 32ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1538 - lr: 5.1200e-09 - 825ms/epoch - 33ms/step\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1538 - lr: 5.1200e-09 - 783ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1538 - lr: 1.0240e-09 - 769ms/epoch - 31ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1538 - lr: 1.0240e-09 - 797ms/epoch - 32ms/step\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1538 - lr: 1.0240e-09 - 827ms/epoch - 33ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0470 - val_loss: 0.1538 - lr: 2.0480e-10 - 774ms/epoch - 31ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1538 - lr: 2.0480e-10 - 776ms/epoch - 31ms/step\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.1538 - lr: 2.0480e-10 - 831ms/epoch - 33ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.1538 - lr: 4.0960e-11 - 792ms/epoch - 32ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.1538 - lr: 4.0960e-11 - 821ms/epoch - 33ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.1538 - lr: 4.0960e-11 - 789ms/epoch - 32ms/step\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.1538 - lr: 4.0960e-11 - 775ms/epoch - 31ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.1538 - lr: 8.1920e-12 - 806ms/epoch - 32ms/step\n",
      "Epoch 62/500\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1538 - lr: 8.1920e-12 - 793ms/epoch - 32ms/step\n",
      "Epoch 62: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 2s - 2s/epoch - 329ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.2min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.2635 - val_loss: 2.3473 - lr: 0.0100 - 17s/epoch - 668ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1949 - val_loss: 2.3221 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1742 - val_loss: 2.1177 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1459 - val_loss: 1.6753 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1079 - val_loss: 1.0653 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0745 - val_loss: 0.6694 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0628 - val_loss: 0.5524 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.5148 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.4943 - lr: 0.0100 - 756ms/epoch - 30ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.4783 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.4646 - lr: 0.0100 - 758ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.4501 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.4371 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.4259 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.4127 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.4041 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.3937 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.3840 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.3755 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.3673 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.3582 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.3517 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0509 - val_loss: 0.3431 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.3360 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.3316 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.3237 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.3173 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.3119 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.3073 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.3006 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.2956 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.2905 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.2854 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.2805 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.2758 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.2719 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.2658 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0451 - val_loss: 0.2646 - lr: 0.0100 - 771ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0453 - val_loss: 0.2637 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2571 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0442 - val_loss: 0.2517 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0439 - val_loss: 0.2470 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0439 - val_loss: 0.2421 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0437 - val_loss: 0.2406 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0430 - val_loss: 0.2398 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0429 - val_loss: 0.2351 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0426 - val_loss: 0.2305 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0423 - val_loss: 0.2276 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0418 - val_loss: 0.2242 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0415 - val_loss: 0.2205 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0410 - val_loss: 0.2167 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0407 - val_loss: 0.2153 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0404 - val_loss: 0.2137 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.2106 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.2085 - lr: 0.0100 - 784ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0391 - val_loss: 0.2049 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0389 - val_loss: 0.2016 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0385 - val_loss: 0.1988 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0380 - val_loss: 0.1981 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0378 - val_loss: 0.1947 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0377 - val_loss: 0.1918 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0373 - val_loss: 0.1901 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0368 - val_loss: 0.1847 - lr: 0.0100 - 749ms/epoch - 30ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.1836 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0365 - val_loss: 0.1780 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0360 - val_loss: 0.1794 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0360 - val_loss: 0.1746 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0357 - val_loss: 0.1736 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0353 - val_loss: 0.1697 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0352 - val_loss: 0.1685 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1663 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1631 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0335 - val_loss: 0.1615 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0332 - val_loss: 0.1602 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.1547 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0328 - val_loss: 0.1529 - lr: 0.0100 - 758ms/epoch - 30ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.1510 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0319 - val_loss: 0.1493 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0319 - val_loss: 0.1503 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0318 - val_loss: 0.1466 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0314 - val_loss: 0.1445 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0315 - val_loss: 0.1406 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0310 - val_loss: 0.1411 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0308 - val_loss: 0.1393 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0306 - val_loss: 0.1389 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0301 - val_loss: 0.1384 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0302 - val_loss: 0.1327 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1331 - lr: 0.0100 - 754ms/epoch - 30ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1321 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.1283 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.1300 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.1275 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0290 - val_loss: 0.1268 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0288 - val_loss: 0.1243 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1240 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1228 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 97/500\n",
      "25/25 - 1s - loss: 0.0283 - val_loss: 0.1221 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0284 - val_loss: 0.1206 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.1215 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.1202 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 101/500\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.1188 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1183 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 103/500\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.1178 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 104/500\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1127 - lr: 0.0020 - 802ms/epoch - 32ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0280 - val_loss: 0.1124 - lr: 0.0020 - 803ms/epoch - 32ms/step\n",
      "Epoch 106/500\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.1122 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 107/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1120 - lr: 4.0000e-04 - 792ms/epoch - 32ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.1119 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 109/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1119 - lr: 4.0000e-04 - 812ms/epoch - 32ms/step\n",
      "Epoch 110/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1118 - lr: 4.0000e-04 - 805ms/epoch - 32ms/step\n",
      "Epoch 111/500\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.1118 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 112/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1118 - lr: 4.0000e-04 - 800ms/epoch - 32ms/step\n",
      "Epoch 113/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1117 - lr: 4.0000e-04 - 794ms/epoch - 32ms/step\n",
      "Epoch 114/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1117 - lr: 4.0000e-04 - 781ms/epoch - 31ms/step\n",
      "Epoch 115/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1116 - lr: 4.0000e-04 - 761ms/epoch - 30ms/step\n",
      "Epoch 116/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1116 - lr: 4.0000e-04 - 768ms/epoch - 31ms/step\n",
      "Epoch 117/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1115 - lr: 4.0000e-04 - 776ms/epoch - 31ms/step\n",
      "Epoch 118/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1114 - lr: 4.0000e-04 - 794ms/epoch - 32ms/step\n",
      "Epoch 119/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1114 - lr: 4.0000e-04 - 789ms/epoch - 32ms/step\n",
      "Epoch 120/500\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.1113 - lr: 4.0000e-04 - 787ms/epoch - 31ms/step\n",
      "Epoch 121/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1113 - lr: 4.0000e-04 - 770ms/epoch - 31ms/step\n",
      "Epoch 122/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1113 - lr: 4.0000e-04 - 803ms/epoch - 32ms/step\n",
      "Epoch 123/500\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 4.0000e-04 - 782ms/epoch - 31ms/step\n",
      "Epoch 124/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1112 - lr: 8.0000e-05 - 794ms/epoch - 32ms/step\n",
      "Epoch 125/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 8.0000e-05 - 783ms/epoch - 31ms/step\n",
      "Epoch 126/500\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1112 - lr: 8.0000e-05 - 779ms/epoch - 31ms/step\n",
      "Epoch 127/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 1.6000e-05 - 812ms/epoch - 32ms/step\n",
      "Epoch 128/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1112 - lr: 1.6000e-05 - 794ms/epoch - 32ms/step\n",
      "Epoch 129/500\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 1.6000e-05 - 772ms/epoch - 31ms/step\n",
      "Epoch 130/500\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.1112 - lr: 3.2000e-06 - 781ms/epoch - 31ms/step\n",
      "Epoch 131/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 3.2000e-06 - 780ms/epoch - 31ms/step\n",
      "Epoch 132/500\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.1112 - lr: 3.2000e-06 - 782ms/epoch - 31ms/step\n",
      "Epoch 133/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1112 - lr: 3.2000e-06 - 791ms/epoch - 32ms/step\n",
      "Epoch 134/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1112 - lr: 3.2000e-06 - 800ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1112 - lr: 3.2000e-06 - 817ms/epoch - 33ms/step\n",
      "Epoch 136/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 6.4000e-07 - 764ms/epoch - 31ms/step\n",
      "Epoch 137/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1112 - lr: 6.4000e-07 - 779ms/epoch - 31ms/step\n",
      "Epoch 138/500\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.1112 - lr: 6.4000e-07 - 818ms/epoch - 33ms/step\n",
      "Epoch 139/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1112 - lr: 1.2800e-07 - 790ms/epoch - 32ms/step\n",
      "Epoch 140/500\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.1112 - lr: 1.2800e-07 - 829ms/epoch - 33ms/step\n",
      "Epoch 141/500\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1112 - lr: 1.2800e-07 - 806ms/epoch - 32ms/step\n",
      "Epoch 142/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 2.5600e-08 - 827ms/epoch - 33ms/step\n",
      "Epoch 143/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1112 - lr: 2.5600e-08 - 793ms/epoch - 32ms/step\n",
      "Epoch 144/500\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1112 - lr: 2.5600e-08 - 767ms/epoch - 31ms/step\n",
      "Epoch 145/500\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.1112 - lr: 5.1200e-09 - 780ms/epoch - 31ms/step\n",
      "Epoch 146/500\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1112 - lr: 5.1200e-09 - 783ms/epoch - 31ms/step\n",
      "Epoch 147/500\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.1112 - lr: 5.1200e-09 - 792ms/epoch - 32ms/step\n",
      "Epoch 148/500\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1112 - lr: 5.1200e-09 - 761ms/epoch - 30ms/step\n",
      "Epoch 149/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1112 - lr: 1.0240e-09 - 796ms/epoch - 32ms/step\n",
      "Epoch 150/500\n",
      "25/25 - 1s - loss: 0.0265 - val_loss: 0.1112 - lr: 1.0240e-09 - 783ms/epoch - 31ms/step\n",
      "Epoch 151/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 1.0240e-09 - 755ms/epoch - 30ms/step\n",
      "Epoch 152/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1112 - lr: 1.0240e-09 - 800ms/epoch - 32ms/step\n",
      "Epoch 153/500\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 1.0240e-09 - 777ms/epoch - 31ms/step\n",
      "Epoch 154/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1112 - lr: 2.0480e-10 - 787ms/epoch - 31ms/step\n",
      "Epoch 155/500\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1112 - lr: 2.0480e-10 - 762ms/epoch - 30ms/step\n",
      "Epoch 156/500\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1112 - lr: 2.0480e-10 - 750ms/epoch - 30ms/step\n",
      "Epoch 157/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1112 - lr: 4.0960e-11 - 808ms/epoch - 32ms/step\n",
      "Epoch 158/500\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.1112 - lr: 4.0960e-11 - 769ms/epoch - 31ms/step\n",
      "Epoch 159/500\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1112 - lr: 4.0960e-11 - 767ms/epoch - 31ms/step\n",
      "Epoch 160/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1112 - lr: 8.1920e-12 - 798ms/epoch - 32ms/step\n",
      "Epoch 161/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1112 - lr: 8.1920e-12 - 758ms/epoch - 30ms/step\n",
      "Epoch 162/500\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1112 - lr: 8.1920e-12 - 767ms/epoch - 31ms/step\n",
      "Epoch 163/500\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1112 - lr: 1.6384e-12 - 774ms/epoch - 31ms/step\n",
      "Epoch 164/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1112 - lr: 1.6384e-12 - 766ms/epoch - 31ms/step\n",
      "Epoch 165/500\n",
      "Restoring model weights from the end of the best epoch: 145.\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1112 - lr: 1.6384e-12 - 809ms/epoch - 32ms/step\n",
      "Epoch 165: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 2.5min\n",
      "7/7 - 2s - 2s/epoch - 326ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 2.5min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.4459 - val_loss: 0.7965 - lr: 0.0100 - 16s/epoch - 643ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8825 - val_loss: 0.3504 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4529 - val_loss: 0.2789 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3126 - val_loss: 0.3781 - lr: 0.0100 - 872ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2055 - val_loss: 0.2554 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1983 - val_loss: 0.2569 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1934 - val_loss: 0.2630 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1930 - val_loss: 0.2627 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1970 - val_loss: 0.2713 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1976 - val_loss: 0.2916 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1949 - val_loss: 0.2971 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1764 - val_loss: 0.2938 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1751 - val_loss: 0.2890 - lr: 0.0020 - 846ms/epoch - 34ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1742 - val_loss: 0.2853 - lr: 0.0020 - 827ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1765 - val_loss: 0.2837 - lr: 0.0020 - 799ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.2828 - lr: 0.0020 - 777ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1763 - val_loss: 0.2837 - lr: 0.0020 - 795ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1720 - val_loss: 0.2840 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1717 - val_loss: 0.2843 - lr: 4.0000e-04 - 849ms/epoch - 34ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1713 - val_loss: 0.2845 - lr: 4.0000e-04 - 762ms/epoch - 30ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1718 - val_loss: 0.2848 - lr: 4.0000e-04 - 785ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1719 - val_loss: 0.2850 - lr: 4.0000e-04 - 802ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1722 - val_loss: 0.2852 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.1704 - val_loss: 0.2853 - lr: 8.0000e-05 - 783ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "25/25 - 1s - loss: 0.1702 - val_loss: 0.2853 - lr: 8.0000e-05 - 814ms/epoch - 33ms/step\n",
      "Epoch 25: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  38.0s\n",
      "7/7 - 3s - 3s/epoch - 398ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  40.8s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.4027 - val_loss: 0.5116 - lr: 0.0100 - 17s/epoch - 661ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5097 - val_loss: 0.2531 - lr: 0.0100 - 841ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6020 - val_loss: 0.2448 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2626 - val_loss: 0.2914 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2186 - val_loss: 0.2589 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2126 - val_loss: 0.2693 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2104 - val_loss: 0.2646 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2075 - val_loss: 0.2661 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2063 - val_loss: 0.2656 - lr: 0.0100 - 821ms/epoch - 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2050 - val_loss: 0.2657 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2044 - val_loss: 0.2658 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2034 - val_loss: 0.2659 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2028 - val_loss: 0.2657 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2021 - val_loss: 0.2657 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 0.2659 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2012 - val_loss: 0.2659 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2014 - val_loss: 0.2659 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2009 - val_loss: 0.2659 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2005 - val_loss: 0.2659 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2003 - val_loss: 0.2658 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.2001 - val_loss: 0.2659 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.2001 - val_loss: 0.2659 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "25/25 - 1s - loss: 0.2001 - val_loss: 0.2662 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  36.1s\n",
      "7/7 - 3s - 3s/epoch - 376ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  38.7s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.4530 - val_loss: 0.2720 - lr: 0.0100 - 16s/epoch - 654ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.8511 - val_loss: 0.9060 - lr: 0.0100 - 859ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.4162 - val_loss: 1.6549 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.2921 - val_loss: 2.3345 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8502 - val_loss: 2.2293 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8484 - val_loss: 2.1874 - lr: 0.0020 - 803ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7923 - val_loss: 2.0482 - lr: 0.0020 - 782ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7268 - val_loss: 2.0023 - lr: 4.0000e-04 - 811ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7049 - val_loss: 1.9540 - lr: 4.0000e-04 - 819ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.6797 - val_loss: 1.8983 - lr: 4.0000e-04 - 815ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.6584 - val_loss: 1.8858 - lr: 8.0000e-05 - 820ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.6514 - val_loss: 1.8728 - lr: 8.0000e-05 - 795ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.6444 - val_loss: 1.8592 - lr: 8.0000e-05 - 812ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.6398 - val_loss: 1.8564 - lr: 1.6000e-05 - 810ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.6376 - val_loss: 1.8535 - lr: 1.6000e-05 - 816ms/epoch - 33ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.6372 - val_loss: 1.8507 - lr: 1.6000e-05 - 838ms/epoch - 34ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.6359 - val_loss: 1.8501 - lr: 3.2000e-06 - 791ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.6352 - val_loss: 1.8495 - lr: 3.2000e-06 - 809ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.6345 - val_loss: 1.8488 - lr: 3.2000e-06 - 813ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.6345 - val_loss: 1.8487 - lr: 6.4000e-07 - 801ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.6342 - val_loss: 1.8486 - lr: 6.4000e-07 - 805ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.4s\n",
      "7/7 - 2s - 2s/epoch - 331ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  36.7s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.3039 - val_loss: 0.5901 - lr: 0.0100 - 16s/epoch - 648ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2600 - val_loss: 0.2465 - lr: 0.0100 - 863ms/epoch - 35ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3539 - val_loss: 0.2771 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2109 - val_loss: 0.3361 - lr: 0.0100 - 844ms/epoch - 34ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1430 - val_loss: 0.2503 - lr: 0.0100 - 847ms/epoch - 34ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1125 - val_loss: 0.2668 - lr: 0.0100 - 841ms/epoch - 34ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1122 - val_loss: 0.2488 - lr: 0.0100 - 857ms/epoch - 34ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1108 - val_loss: 0.2511 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1107 - val_loss: 0.2512 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1104 - val_loss: 0.2510 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1104 - val_loss: 0.2511 - lr: 0.0100 - 876ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1104 - val_loss: 0.2513 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1104 - val_loss: 0.2513 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1104 - val_loss: 0.2513 - lr: 0.0100 - 858ms/epoch - 34ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1104 - val_loss: 0.2514 - lr: 0.0100 - 841ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1106 - val_loss: 0.2514 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0971 - val_loss: 0.2507 - lr: 0.0020 - 847ms/epoch - 34ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0964 - val_loss: 0.2497 - lr: 0.0020 - 812ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0958 - val_loss: 0.2491 - lr: 0.0020 - 769ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0955 - val_loss: 0.2488 - lr: 0.0020 - 762ms/epoch - 30ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0954 - val_loss: 0.2487 - lr: 0.0020 - 826ms/epoch - 33ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "25/25 - 1s - loss: 0.0954 - val_loss: 0.2486 - lr: 0.0020 - 802ms/epoch - 32ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  36.1s\n",
      "7/7 - 2s - 2s/epoch - 322ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  38.3s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.2915 - val_loss: 2.0725 - lr: 0.0100 - 17s/epoch - 669ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2155 - val_loss: 0.9782 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2900 - val_loss: 0.7630 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4161 - val_loss: 2.1320 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1086 - val_loss: 0.5979 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1356 - val_loss: 0.7929 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1217 - val_loss: 2.4539 - lr: 0.0100 - 768ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1344 - val_loss: 1.7328 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0993 - val_loss: 1.6133 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0886 - val_loss: 1.4813 - lr: 0.0020 - 789ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0782 - val_loss: 1.3552 - lr: 0.0020 - 792ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0691 - val_loss: 1.2350 - lr: 0.0020 - 794ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 1.1190 - lr: 0.0020 - 780ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 1.0099 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.9140 - lr: 0.0020 - 764ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0434 - val_loss: 0.8342 - lr: 0.0020 - 769ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.7713 - lr: 0.0020 - 755ms/epoch - 30ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.7236 - lr: 0.0020 - 815ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0351 - val_loss: 0.6867 - lr: 0.0020 - 821ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.6538 - lr: 0.0020 - 789ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0325 - val_loss: 0.6250 - lr: 0.0020 - 766ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0316 - val_loss: 0.5976 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.5742 - lr: 0.0020 - 790ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.5514 - lr: 0.0020 - 778ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0305 - val_loss: 0.5331 - lr: 0.0020 - 777ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0306 - val_loss: 0.5187 - lr: 0.0020 - 814ms/epoch - 33ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0304 - val_loss: 0.5053 - lr: 0.0020 - 798ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.4937 - lr: 0.0020 - 785ms/epoch - 31ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 0.4885 - lr: 0.0020 - 776ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.4781 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.4705 - lr: 0.0020 - 776ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.4662 - lr: 0.0020 - 764ms/epoch - 31ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.4612 - lr: 0.0020 - 778ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.4566 - lr: 0.0020 - 813ms/epoch - 33ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.4521 - lr: 0.0020 - 759ms/epoch - 30ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.4497 - lr: 0.0020 - 779ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0259 - val_loss: 0.4495 - lr: 0.0020 - 770ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.4454 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.4430 - lr: 0.0020 - 790ms/epoch - 32ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0254 - val_loss: 0.4428 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0244 - val_loss: 0.4437 - lr: 0.0020 - 789ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0240 - val_loss: 0.4413 - lr: 0.0020 - 810ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0238 - val_loss: 0.4415 - lr: 0.0020 - 769ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0235 - val_loss: 0.4367 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0242 - val_loss: 0.4367 - lr: 0.0020 - 786ms/epoch - 31ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0239 - val_loss: 0.4354 - lr: 0.0020 - 781ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0233 - val_loss: 0.4357 - lr: 0.0020 - 758ms/epoch - 30ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0230 - val_loss: 0.4322 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0228 - val_loss: 0.4298 - lr: 0.0020 - 822ms/epoch - 33ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0224 - val_loss: 0.4334 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0219 - val_loss: 0.4295 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0212 - val_loss: 0.4288 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0206 - val_loss: 0.4320 - lr: 0.0020 - 761ms/epoch - 30ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0206 - val_loss: 0.4250 - lr: 0.0020 - 792ms/epoch - 32ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0201 - val_loss: 0.4262 - lr: 0.0020 - 757ms/epoch - 30ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0211 - val_loss: 0.4333 - lr: 0.0020 - 798ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0202 - val_loss: 0.4230 - lr: 0.0020 - 812ms/epoch - 32ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0197 - val_loss: 0.4297 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0192 - val_loss: 0.4198 - lr: 0.0020 - 799ms/epoch - 32ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0187 - val_loss: 0.4255 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0184 - val_loss: 0.4188 - lr: 0.0020 - 778ms/epoch - 31ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0183 - val_loss: 0.4221 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0176 - val_loss: 0.4118 - lr: 0.0020 - 794ms/epoch - 32ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0170 - val_loss: 0.4311 - lr: 0.0020 - 786ms/epoch - 31ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0173 - val_loss: 0.3992 - lr: 0.0020 - 811ms/epoch - 32ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0164 - val_loss: 0.4577 - lr: 0.0020 - 776ms/epoch - 31ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0167 - val_loss: 0.3984 - lr: 0.0020 - 751ms/epoch - 30ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0165 - val_loss: 0.4830 - lr: 0.0020 - 785ms/epoch - 31ms/step\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0168 - val_loss: 0.3984 - lr: 0.0020 - 761ms/epoch - 30ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0164 - val_loss: 0.4202 - lr: 4.0000e-04 - 768ms/epoch - 31ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0141 - val_loss: 0.4404 - lr: 4.0000e-04 - 772ms/epoch - 31ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0145 - val_loss: 0.4410 - lr: 4.0000e-04 - 802ms/epoch - 32ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0138 - val_loss: 0.4455 - lr: 4.0000e-04 - 775ms/epoch - 31ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0141 - val_loss: 0.4471 - lr: 4.0000e-04 - 759ms/epoch - 30ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0141 - val_loss: 0.4480 - lr: 4.0000e-04 - 777ms/epoch - 31ms/step\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0140 - val_loss: 0.4490 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0142 - val_loss: 0.4499 - lr: 8.0000e-05 - 778ms/epoch - 31ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0138 - val_loss: 0.4505 - lr: 8.0000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0141 - val_loss: 0.4503 - lr: 8.0000e-05 - 791ms/epoch - 32ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0138 - val_loss: 0.4503 - lr: 1.6000e-05 - 804ms/epoch - 32ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0139 - val_loss: 0.4503 - lr: 1.6000e-05 - 768ms/epoch - 31ms/step\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0139 - val_loss: 0.4504 - lr: 1.6000e-05 - 769ms/epoch - 31ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0137 - val_loss: 0.4504 - lr: 3.2000e-06 - 780ms/epoch - 31ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0137 - val_loss: 0.4504 - lr: 3.2000e-06 - 793ms/epoch - 32ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0136 - val_loss: 0.4504 - lr: 3.2000e-06 - 770ms/epoch - 31ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0137 - val_loss: 0.4504 - lr: 3.2000e-06 - 764ms/epoch - 31ms/step\n",
      "Epoch 87/500\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "25/25 - 1s - loss: 0.0136 - val_loss: 0.4504 - lr: 3.2000e-06 - 830ms/epoch - 33ms/step\n",
      "Epoch 87: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.4min\n",
      "7/7 - 2s - 2s/epoch - 320ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time= 1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.2496 - val_loss: 0.3217 - lr: 0.0100 - 18s/epoch - 710ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7127 - val_loss: 0.3192 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0625 - val_loss: 1.0542 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9118 - val_loss: 1.5767 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7747 - val_loss: 1.6266 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7638 - val_loss: 1.6535 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.5088 - val_loss: 1.5310 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.4736 - val_loss: 1.5115 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.4255 - val_loss: 1.4853 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.4016 - val_loss: 1.4554 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3872 - val_loss: 1.4497 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.3841 - val_loss: 1.4440 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.3808 - val_loss: 1.4381 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.3771 - val_loss: 1.4369 - lr: 1.6000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.3769 - val_loss: 1.4357 - lr: 1.6000e-05 - 1s/epoch - 46ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.3766 - val_loss: 1.4345 - lr: 1.6000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.3752 - val_loss: 1.4343 - lr: 3.2000e-06 - 1s/epoch - 44ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.3761 - val_loss: 1.4340 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.3757 - val_loss: 1.4338 - lr: 3.2000e-06 - 1s/epoch - 44ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.3758 - val_loss: 1.4337 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.3755 - val_loss: 1.4336 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.3752 - val_loss: 1.4336 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  42.8s\n",
      "7/7 - 2s - 2s/epoch - 335ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  45.1s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.4350 - val_loss: 0.4402 - lr: 0.0100 - 18s/epoch - 727ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2769 - val_loss: 0.3530 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5421 - val_loss: 0.3822 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6755 - val_loss: 0.3836 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.6735 - val_loss: 0.2475 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.6398 - val_loss: 1.0324 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.9634 - val_loss: 1.4544 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8475 - val_loss: 1.6253 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8025 - val_loss: 1.6693 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7962 - val_loss: 1.7136 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7902 - val_loss: 1.7529 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7797 - val_loss: 1.7613 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7788 - val_loss: 1.7699 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7779 - val_loss: 1.7783 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7756 - val_loss: 1.7801 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7753 - val_loss: 1.7818 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7752 - val_loss: 1.7836 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7748 - val_loss: 1.7840 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7747 - val_loss: 1.7843 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7747 - val_loss: 1.7847 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.7746 - val_loss: 1.7848 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.7746 - val_loss: 1.7848 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.7745 - val_loss: 1.7849 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.7746 - val_loss: 1.7849 - lr: 1.2800e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 25/500\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "25/25 - 1s - loss: 0.7746 - val_loss: 1.7849 - lr: 1.2800e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 25: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  45.3s\n",
      "7/7 - 3s - 3s/epoch - 393ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  48.0s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.4949 - val_loss: 0.4028 - lr: 0.0100 - 18s/epoch - 735ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5914 - val_loss: 0.3225 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.1232 - val_loss: 0.4637 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.0759 - val_loss: 1.2309 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 1.0427 - val_loss: 1.5245 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9591 - val_loss: 1.6857 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.9232 - val_loss: 1.7972 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8939 - val_loss: 1.8189 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8904 - val_loss: 1.8409 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8869 - val_loss: 1.8626 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8812 - val_loss: 1.8671 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8805 - val_loss: 1.8717 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8799 - val_loss: 1.8763 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8786 - val_loss: 1.8773 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8785 - val_loss: 1.8782 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.8783 - val_loss: 1.8792 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8781 - val_loss: 1.8794 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8780 - val_loss: 1.8796 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8780 - val_loss: 1.8798 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8780 - val_loss: 1.8798 - lr: 6.4000e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.8780 - val_loss: 1.8799 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.8780 - val_loss: 1.8799 - lr: 6.4000e-07 - 1s/epoch - 44ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  42.4s\n",
      "7/7 - 2s - 2s/epoch - 319ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  44.6s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7869 - val_loss: 0.2496 - lr: 0.0100 - 20s/epoch - 789ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.3507 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1135 - val_loss: 0.3232 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1065 - val_loss: 0.3174 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1137 - val_loss: 0.3104 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1175 - val_loss: 0.3033 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1202 - val_loss: 0.2956 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1547 - val_loss: 0.2564 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1072 - val_loss: 0.2467 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1026 - val_loss: 0.2463 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1030 - val_loss: 0.2464 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1034 - val_loss: 0.2464 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1038 - val_loss: 0.2465 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1007 - val_loss: 0.2463 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0982 - val_loss: 0.2462 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0967 - val_loss: 0.2463 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0960 - val_loss: 0.2464 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0955 - val_loss: 0.2465 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0952 - val_loss: 0.2467 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0951 - val_loss: 0.2468 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0951 - val_loss: 0.2469 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0950 - val_loss: 0.2469 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0950 - val_loss: 0.2470 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0952 - val_loss: 0.2470 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0951 - val_loss: 0.2470 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0951 - val_loss: 0.2470 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2471 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2471 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2471 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2472 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2472 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0929 - val_loss: 0.2472 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0928 - val_loss: 0.2473 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0929 - val_loss: 0.2473 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 35/500\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "25/25 - 1s - loss: 0.0928 - val_loss: 0.2473 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 35: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  57.4s\n",
      "7/7 - 2s - 2s/epoch - 318ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  59.6s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.3131 - val_loss: 0.2604 - lr: 0.0100 - 18s/epoch - 711ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1965 - val_loss: 1.1489 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2229 - val_loss: 1.3939 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2333 - val_loss: 1.3535 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2564 - val_loss: 2.5490 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3121 - val_loss: 3.0058 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2466 - val_loss: 3.3352 - lr: 0.0020 - 1s/epoch - 44ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2199 - val_loss: 3.4580 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2105 - val_loss: 3.5002 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2084 - val_loss: 3.5431 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.2064 - val_loss: 3.5794 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2035 - val_loss: 3.5874 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2031 - val_loss: 3.5956 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.2028 - val_loss: 3.6037 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2022 - val_loss: 3.6053 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2021 - val_loss: 3.6071 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.2021 - val_loss: 3.6088 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 3.6092 - lr: 3.2000e-06 - 1s/epoch - 44ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 3.6095 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 3.6099 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2018 - val_loss: 3.6099 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.6s\n",
      "7/7 - 2s - 2s/epoch - 324ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  43.8s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.7637 - val_loss: 1.8311 - lr: 0.0100 - 17s/epoch - 689ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7560 - val_loss: 1.8451 - lr: 0.0100 - 819ms/epoch - 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7546 - val_loss: 1.8536 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7538 - val_loss: 1.8598 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7532 - val_loss: 1.8649 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7528 - val_loss: 1.8692 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7524 - val_loss: 1.8730 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7521 - val_loss: 1.8763 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7519 - val_loss: 1.8794 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7517 - val_loss: 1.8822 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7514 - val_loss: 1.8847 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7513 - val_loss: 1.8872 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7511 - val_loss: 1.8894 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7509 - val_loss: 1.8916 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7508 - val_loss: 1.8936 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7506 - val_loss: 1.8955 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7505 - val_loss: 1.8973 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7504 - val_loss: 1.8991 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7503 - val_loss: 1.9007 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7502 - val_loss: 1.9023 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7501 - val_loss: 1.9039 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.2s\n",
      "7/7 - 2s - 2s/epoch - 336ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.5s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.7758 - val_loss: 1.8308 - lr: 0.0100 - 16s/epoch - 641ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7709 - val_loss: 1.8459 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7693 - val_loss: 1.8552 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7683 - val_loss: 1.8622 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7677 - val_loss: 1.8679 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7672 - val_loss: 1.8727 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7668 - val_loss: 1.8770 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7664 - val_loss: 1.8808 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7661 - val_loss: 1.8843 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7658 - val_loss: 1.8875 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7655 - val_loss: 1.8904 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7653 - val_loss: 1.8932 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7651 - val_loss: 1.8958 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7649 - val_loss: 1.8982 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7647 - val_loss: 1.9006 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7646 - val_loss: 1.9028 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7644 - val_loss: 1.9049 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7642 - val_loss: 1.9069 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7641 - val_loss: 1.9089 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7640 - val_loss: 1.9108 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7638 - val_loss: 1.9126 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.5s\n",
      "7/7 - 3s - 3s/epoch - 372ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.1s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.8950 - val_loss: 1.8495 - lr: 0.0100 - 17s/epoch - 681ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8824 - val_loss: 1.8732 - lr: 0.0100 - 838ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8789 - val_loss: 1.8890 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8767 - val_loss: 1.9014 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8749 - val_loss: 1.9118 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8735 - val_loss: 1.9209 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8723 - val_loss: 1.9290 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8713 - val_loss: 1.9365 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8703 - val_loss: 1.9434 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8694 - val_loss: 1.9498 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8686 - val_loss: 1.9559 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8679 - val_loss: 1.9618 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8672 - val_loss: 1.9673 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8665 - val_loss: 1.9727 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8659 - val_loss: 1.9780 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8653 - val_loss: 1.9831 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8648 - val_loss: 1.9881 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8642 - val_loss: 1.9930 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8637 - val_loss: 1.9978 - lr: 0.0100 - 768ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8632 - val_loss: 2.0026 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8627 - val_loss: 2.0074 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.9s\n",
      "7/7 - 2s - 2s/epoch - 322ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.1s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.9517 - val_loss: 1.8688 - lr: 0.0100 - 17s/epoch - 678ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9318 - val_loss: 1.9035 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9248 - val_loss: 1.9278 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.9198 - val_loss: 1.9474 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9159 - val_loss: 1.9643 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9126 - val_loss: 1.9793 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.9097 - val_loss: 1.9932 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.9071 - val_loss: 2.0062 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9047 - val_loss: 2.0187 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.9025 - val_loss: 2.0308 - lr: 0.0100 - 771ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.9003 - val_loss: 2.0427 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8982 - val_loss: 2.0546 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8962 - val_loss: 2.0666 - lr: 0.0100 - 790ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8942 - val_loss: 2.0788 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8922 - val_loss: 2.0912 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8902 - val_loss: 2.1040 - lr: 0.0100 - 831ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8883 - val_loss: 2.1171 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8863 - val_loss: 2.1305 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8843 - val_loss: 2.1442 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8824 - val_loss: 2.1581 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8804 - val_loss: 2.1723 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.9s\n",
      "7/7 - 2s - 2s/epoch - 334ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.2s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.5993 - val_loss: 1.9229 - lr: 0.0100 - 17s/epoch - 676ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5472 - val_loss: 2.0066 - lr: 0.0100 - 833ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5147 - val_loss: 2.0734 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4889 - val_loss: 2.1340 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4657 - val_loss: 2.1946 - lr: 0.0100 - 831ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4428 - val_loss: 2.2612 - lr: 0.0100 - 848ms/epoch - 34ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.4184 - val_loss: 2.3397 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3912 - val_loss: 2.4357 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3611 - val_loss: 2.5521 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3293 - val_loss: 2.6873 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2984 - val_loss: 2.8343 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2712 - val_loss: 2.9827 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2493 - val_loss: 3.1213 - lr: 0.0100 - 839ms/epoch - 34ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2332 - val_loss: 3.2421 - lr: 0.0100 - 860ms/epoch - 34ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2221 - val_loss: 3.3412 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2149 - val_loss: 3.4188 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2102 - val_loss: 3.4778 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2072 - val_loss: 3.5219 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2053 - val_loss: 3.5545 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2040 - val_loss: 3.5787 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2032 - val_loss: 3.5969 - lr: 0.0100 - 868ms/epoch - 35ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.4s\n",
      "7/7 - 2s - 2s/epoch - 322ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.6s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.5893 - val_loss: 0.7467 - lr: 0.0100 - 17s/epoch - 661ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5623 - val_loss: 0.2974 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2213 - val_loss: 0.1827 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0851 - val_loss: 0.1801 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0684 - val_loss: 0.1767 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.1716 - lr: 0.0100 - 758ms/epoch - 30ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0650 - val_loss: 0.1691 - lr: 0.0100 - 757ms/epoch - 30ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0642 - val_loss: 0.1652 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1635 - lr: 0.0100 - 737ms/epoch - 29ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1616 - lr: 0.0100 - 749ms/epoch - 30ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1598 - lr: 0.0100 - 756ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.1581 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.1579 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1561 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0665 - val_loss: 0.1503 - lr: 0.0020 - 754ms/epoch - 30ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.1485 - lr: 0.0020 - 763ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0584 - val_loss: 0.1481 - lr: 0.0020 - 786ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.1477 - lr: 0.0020 - 759ms/epoch - 30ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.1474 - lr: 0.0020 - 763ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.1471 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1467 - lr: 0.0020 - 747ms/epoch - 30ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1465 - lr: 0.0020 - 752ms/epoch - 30ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1466 - lr: 0.0020 - 742ms/epoch - 30ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1467 - lr: 0.0020 - 746ms/epoch - 30ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1465 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1458 - lr: 0.0020 - 753ms/epoch - 30ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1456 - lr: 0.0020 - 786ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1456 - lr: 0.0020 - 761ms/epoch - 30ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1453 - lr: 0.0020 - 779ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1449 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1448 - lr: 0.0020 - 757ms/epoch - 30ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1444 - lr: 0.0020 - 761ms/epoch - 30ms/step\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1447 - lr: 0.0020 - 755ms/epoch - 30ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1445 - lr: 4.0000e-04 - 725ms/epoch - 29ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1443 - lr: 4.0000e-04 - 751ms/epoch - 30ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1443 - lr: 4.0000e-04 - 779ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.1443 - lr: 4.0000e-04 - 758ms/epoch - 30ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1443 - lr: 4.0000e-04 - 768ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1442 - lr: 4.0000e-04 - 776ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.1442 - lr: 4.0000e-04 - 733ms/epoch - 29ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1442 - lr: 4.0000e-04 - 731ms/epoch - 29ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.1442 - lr: 4.0000e-04 - 746ms/epoch - 30ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1443 - lr: 4.0000e-04 - 773ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.1443 - lr: 4.0000e-04 - 750ms/epoch - 30ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1444 - lr: 8.0000e-05 - 754ms/epoch - 30ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1443 - lr: 8.0000e-05 - 755ms/epoch - 30ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.1443 - lr: 8.0000e-05 - 756ms/epoch - 30ms/step\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1443 - lr: 8.0000e-05 - 762ms/epoch - 30ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1443 - lr: 1.6000e-05 - 747ms/epoch - 30ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1443 - lr: 1.6000e-05 - 772ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1443 - lr: 1.6000e-05 - 808ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1443 - lr: 1.6000e-05 - 742ms/epoch - 30ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1443 - lr: 1.6000e-05 - 738ms/epoch - 30ms/step\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1443 - lr: 1.6000e-05 - 744ms/epoch - 30ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1443 - lr: 3.2000e-06 - 740ms/epoch - 30ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1443 - lr: 3.2000e-06 - 756ms/epoch - 30ms/step\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1443 - lr: 3.2000e-06 - 771ms/epoch - 31ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1443 - lr: 6.4000e-07 - 771ms/epoch - 31ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1443 - lr: 6.4000e-07 - 771ms/epoch - 31ms/step\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1443 - lr: 6.4000e-07 - 721ms/epoch - 29ms/step\n",
      "Epoch 61/500\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1443 - lr: 1.2800e-07 - 757ms/epoch - 30ms/step\n",
      "Epoch 61: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 2s - 2s/epoch - 317ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.1min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.5906 - val_loss: 0.7093 - lr: 0.0100 - 16s/epoch - 653ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5906 - val_loss: 0.3080 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2432 - val_loss: 0.1863 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0858 - val_loss: 0.1812 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0651 - val_loss: 0.1768 - lr: 0.0100 - 741ms/epoch - 30ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0625 - val_loss: 0.1722 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1699 - lr: 0.0100 - 731ms/epoch - 29ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1665 - lr: 0.0100 - 763ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.1644 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1625 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1596 - lr: 0.0100 - 760ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.1591 - lr: 0.0100 - 745ms/epoch - 30ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1589 - lr: 0.0100 - 757ms/epoch - 30ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1550 - lr: 0.0100 - 751ms/epoch - 30ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1548 - lr: 0.0100 - 751ms/epoch - 30ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1543 - lr: 0.0100 - 754ms/epoch - 30ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1487 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.1476 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1469 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1472 - lr: 0.0020 - 741ms/epoch - 30ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1466 - lr: 0.0020 - 763ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1458 - lr: 0.0020 - 749ms/epoch - 30ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1457 - lr: 0.0020 - 753ms/epoch - 30ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.1456 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1454 - lr: 0.0020 - 740ms/epoch - 30ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1451 - lr: 0.0020 - 779ms/epoch - 31ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1455 - lr: 0.0020 - 772ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1452 - lr: 4.0000e-04 - 761ms/epoch - 30ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1452 - lr: 4.0000e-04 - 728ms/epoch - 29ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1452 - lr: 4.0000e-04 - 745ms/epoch - 30ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1451 - lr: 4.0000e-04 - 764ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1451 - lr: 4.0000e-04 - 756ms/epoch - 30ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.1452 - lr: 4.0000e-04 - 740ms/epoch - 30ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1452 - lr: 4.0000e-04 - 759ms/epoch - 30ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1452 - lr: 4.0000e-04 - 746ms/epoch - 30ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1452 - lr: 8.0000e-05 - 747ms/epoch - 30ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1452 - lr: 8.0000e-05 - 768ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1452 - lr: 8.0000e-05 - 744ms/epoch - 30ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1453 - lr: 8.0000e-05 - 755ms/epoch - 30ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1453 - lr: 8.0000e-05 - 743ms/epoch - 30ms/step\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.1453 - lr: 8.0000e-05 - 745ms/epoch - 30ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1453 - lr: 1.6000e-05 - 770ms/epoch - 31ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1453 - lr: 1.6000e-05 - 767ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1453 - lr: 1.6000e-05 - 762ms/epoch - 30ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1453 - lr: 3.2000e-06 - 750ms/epoch - 30ms/step\n",
      "Epoch 46/500\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1453 - lr: 3.2000e-06 - 766ms/epoch - 31ms/step\n",
      "Epoch 46: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  52.4s\n",
      "7/7 - 2s - 2s/epoch - 328ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  54.6s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.6454 - val_loss: 0.7268 - lr: 0.0100 - 16s/epoch - 637ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4423 - val_loss: 0.1949 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0806 - val_loss: 0.1838 - lr: 0.0100 - 741ms/epoch - 30ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.1762 - lr: 0.0100 - 749ms/epoch - 30ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0252 - val_loss: 0.1744 - lr: 0.0100 - 730ms/epoch - 29ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0252 - val_loss: 0.1714 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0249 - val_loss: 0.1687 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0248 - val_loss: 0.1667 - lr: 0.0100 - 770ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0248 - val_loss: 0.1635 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0250 - val_loss: 0.1626 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0250 - val_loss: 0.1598 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0316 - val_loss: 0.1537 - lr: 0.0020 - 751ms/epoch - 30ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0280 - val_loss: 0.1514 - lr: 0.0020 - 769ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1502 - lr: 0.0020 - 760ms/epoch - 30ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1501 - lr: 4.0000e-04 - 766ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1500 - lr: 4.0000e-04 - 778ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.1501 - lr: 4.0000e-04 - 791ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0264 - val_loss: 0.1501 - lr: 8.0000e-05 - 753ms/epoch - 30ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1501 - lr: 8.0000e-05 - 747ms/epoch - 30ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1501 - lr: 8.0000e-05 - 754ms/epoch - 30ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.1501 - lr: 1.6000e-05 - 734ms/epoch - 29ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0265 - val_loss: 0.1500 - lr: 1.6000e-05 - 738ms/epoch - 30ms/step\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0257 - val_loss: 0.1501 - lr: 1.6000e-05 - 730ms/epoch - 29ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.1501 - lr: 3.2000e-06 - 779ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0264 - val_loss: 0.1501 - lr: 3.2000e-06 - 764ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.1501 - lr: 3.2000e-06 - 763ms/epoch - 31ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.1501 - lr: 6.4000e-07 - 759ms/epoch - 30ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0258 - val_loss: 0.1501 - lr: 6.4000e-07 - 739ms/epoch - 30ms/step\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.1501 - lr: 6.4000e-07 - 770ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.1501 - lr: 1.2800e-07 - 770ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0260 - val_loss: 0.1501 - lr: 1.2800e-07 - 748ms/epoch - 30ms/step\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0263 - val_loss: 0.1501 - lr: 1.2800e-07 - 773ms/epoch - 31ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0264 - val_loss: 0.1501 - lr: 2.5600e-08 - 753ms/epoch - 30ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0260 - val_loss: 0.1501 - lr: 2.5600e-08 - 765ms/epoch - 31ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0264 - val_loss: 0.1501 - lr: 2.5600e-08 - 763ms/epoch - 31ms/step\n",
      "Epoch 36/500\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.1501 - lr: 5.1200e-09 - 757ms/epoch - 30ms/step\n",
      "Epoch 36: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  45.2s\n",
      "7/7 - 2s - 2s/epoch - 332ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  47.5s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.6396 - val_loss: 0.6198 - lr: 0.0100 - 17s/epoch - 676ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3630 - val_loss: 0.1779 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0729 - val_loss: 0.1750 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1718 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0507 - val_loss: 0.1671 - lr: 0.0100 - 755ms/epoch - 30ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1656 - lr: 0.0100 - 741ms/epoch - 30ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1641 - lr: 0.0100 - 751ms/epoch - 30ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1628 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1608 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1571 - lr: 0.0100 - 750ms/epoch - 30ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.1574 - lr: 0.0100 - 754ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1558 - lr: 0.0100 - 727ms/epoch - 29ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1555 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.1548 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1523 - lr: 0.0020 - 771ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1505 - lr: 0.0020 - 796ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1497 - lr: 0.0020 - 760ms/epoch - 30ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0468 - val_loss: 0.1498 - lr: 4.0000e-04 - 770ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.1497 - lr: 4.0000e-04 - 772ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1497 - lr: 4.0000e-04 - 784ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1498 - lr: 4.0000e-04 - 778ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0466 - val_loss: 0.1497 - lr: 4.0000e-04 - 769ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1497 - lr: 4.0000e-04 - 807ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.1496 - lr: 4.0000e-04 - 786ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1496 - lr: 8.0000e-05 - 744ms/epoch - 30ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.1496 - lr: 8.0000e-05 - 790ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1496 - lr: 8.0000e-05 - 762ms/epoch - 30ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0467 - val_loss: 0.1496 - lr: 8.0000e-05 - 772ms/epoch - 31ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1496 - lr: 1.6000e-05 - 745ms/epoch - 30ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1496 - lr: 1.6000e-05 - 746ms/epoch - 30ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1496 - lr: 1.6000e-05 - 756ms/epoch - 30ms/step\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1496 - lr: 1.6000e-05 - 784ms/epoch - 31ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1496 - lr: 3.2000e-06 - 754ms/epoch - 30ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1496 - lr: 3.2000e-06 - 743ms/epoch - 30ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1496 - lr: 3.2000e-06 - 731ms/epoch - 29ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1496 - lr: 6.4000e-07 - 755ms/epoch - 30ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0467 - val_loss: 0.1496 - lr: 6.4000e-07 - 766ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1496 - lr: 6.4000e-07 - 745ms/epoch - 30ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1496 - lr: 1.2800e-07 - 777ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1496 - lr: 1.2800e-07 - 756ms/epoch - 30ms/step\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1496 - lr: 1.2800e-07 - 751ms/epoch - 30ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1496 - lr: 2.5600e-08 - 771ms/epoch - 31ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1496 - lr: 2.5600e-08 - 753ms/epoch - 30ms/step\n",
      "Epoch 44/500\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1496 - lr: 2.5600e-08 - 758ms/epoch - 30ms/step\n",
      "Epoch 44: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  51.8s\n",
      "7/7 - 2s - 2s/epoch - 348ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  54.2s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.2902 - val_loss: 2.3215 - lr: 0.0100 - 17s/epoch - 666ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1908 - val_loss: 2.2157 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1789 - val_loss: 2.0457 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1661 - val_loss: 1.8184 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1494 - val_loss: 1.5283 - lr: 0.0100 - 745ms/epoch - 30ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1287 - val_loss: 1.1935 - lr: 0.0100 - 735ms/epoch - 29ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1055 - val_loss: 0.8724 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0841 - val_loss: 0.6319 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0702 - val_loss: 0.4941 - lr: 0.0100 - 736ms/epoch - 29ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.4298 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.4005 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.3844 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.3771 - lr: 0.0100 - 731ms/epoch - 29ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.3711 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.3649 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0580 - val_loss: 0.3615 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.3579 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.3549 - lr: 0.0100 - 746ms/epoch - 30ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.3513 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.3471 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.3457 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.3425 - lr: 0.0100 - 759ms/epoch - 30ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.3401 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.3383 - lr: 0.0100 - 768ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.3348 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.3319 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.3275 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.3249 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.3223 - lr: 0.0100 - 757ms/epoch - 30ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.3215 - lr: 0.0100 - 759ms/epoch - 30ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.3192 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3169 - lr: 0.0100 - 746ms/epoch - 30ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3138 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3117 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3105 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.3089 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.3065 - lr: 0.0100 - 719ms/epoch - 29ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.3049 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.3038 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.3021 - lr: 0.0100 - 742ms/epoch - 30ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.3005 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.2990 - lr: 0.0100 - 757ms/epoch - 30ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.2965 - lr: 0.0100 - 759ms/epoch - 30ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.2956 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.2933 - lr: 0.0100 - 721ms/epoch - 29ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.2918 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.2904 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.2887 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.2873 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.2866 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.2856 - lr: 0.0100 - 759ms/epoch - 30ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.2837 - lr: 0.0100 - 754ms/epoch - 30ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.2826 - lr: 0.0100 - 755ms/epoch - 30ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.2807 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.2794 - lr: 0.0100 - 754ms/epoch - 30ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.2782 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.2772 - lr: 0.0100 - 732ms/epoch - 29ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.2751 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.2748 - lr: 0.0100 - 746ms/epoch - 30ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.2733 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.2725 - lr: 0.0100 - 760ms/epoch - 30ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0508 - val_loss: 0.2723 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.2700 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.2698 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.2608 - lr: 0.0020 - 769ms/epoch - 31ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.2550 - lr: 0.0020 - 755ms/epoch - 30ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0467 - val_loss: 0.2509 - lr: 0.0020 - 727ms/epoch - 29ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.2479 - lr: 0.0020 - 734ms/epoch - 29ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.2455 - lr: 0.0020 - 747ms/epoch - 30ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.2435 - lr: 0.0020 - 781ms/epoch - 31ms/step\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.2420 - lr: 0.0020 - 743ms/epoch - 30ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0454 - val_loss: 0.2414 - lr: 4.0000e-04 - 766ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0451 - val_loss: 0.2409 - lr: 4.0000e-04 - 761ms/epoch - 30ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0453 - val_loss: 0.2404 - lr: 4.0000e-04 - 765ms/epoch - 31ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.2400 - lr: 4.0000e-04 - 757ms/epoch - 30ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2395 - lr: 4.0000e-04 - 760ms/epoch - 30ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0455 - val_loss: 0.2391 - lr: 4.0000e-04 - 760ms/epoch - 30ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.2387 - lr: 4.0000e-04 - 769ms/epoch - 31ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2383 - lr: 4.0000e-04 - 777ms/epoch - 31ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2379 - lr: 4.0000e-04 - 768ms/epoch - 31ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0450 - val_loss: 0.2376 - lr: 4.0000e-04 - 753ms/epoch - 30ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2373 - lr: 4.0000e-04 - 744ms/epoch - 30ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2370 - lr: 4.0000e-04 - 775ms/epoch - 31ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0451 - val_loss: 0.2368 - lr: 4.0000e-04 - 752ms/epoch - 30ms/step\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0451 - val_loss: 0.2366 - lr: 4.0000e-04 - 771ms/epoch - 31ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2365 - lr: 8.0000e-05 - 767ms/epoch - 31ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0450 - val_loss: 0.2365 - lr: 8.0000e-05 - 725ms/epoch - 29ms/step\n",
      "Epoch 88/500\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2364 - lr: 8.0000e-05 - 746ms/epoch - 30ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2364 - lr: 1.6000e-05 - 746ms/epoch - 30ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2364 - lr: 1.6000e-05 - 757ms/epoch - 30ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2364 - lr: 1.6000e-05 - 743ms/epoch - 30ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2364 - lr: 1.6000e-05 - 741ms/epoch - 30ms/step\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2364 - lr: 1.6000e-05 - 813ms/epoch - 33ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0445 - val_loss: 0.2364 - lr: 3.2000e-06 - 779ms/epoch - 31ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2364 - lr: 3.2000e-06 - 734ms/epoch - 29ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0445 - val_loss: 0.2364 - lr: 3.2000e-06 - 770ms/epoch - 31ms/step\n",
      "Epoch 97/500\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2364 - lr: 3.2000e-06 - 762ms/epoch - 30ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0444 - val_loss: 0.2364 - lr: 6.4000e-07 - 764ms/epoch - 31ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2364 - lr: 6.4000e-07 - 753ms/epoch - 30ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2364 - lr: 6.4000e-07 - 749ms/epoch - 30ms/step\n",
      "Epoch 101/500\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0444 - val_loss: 0.2363 - lr: 6.4000e-07 - 787ms/epoch - 31ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2364 - lr: 1.2800e-07 - 731ms/epoch - 29ms/step\n",
      "Epoch 103/500\n",
      "25/25 - 1s - loss: 0.0444 - val_loss: 0.2364 - lr: 1.2800e-07 - 758ms/epoch - 30ms/step\n",
      "Epoch 104/500\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2363 - lr: 1.2800e-07 - 753ms/epoch - 30ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2364 - lr: 2.5600e-08 - 735ms/epoch - 29ms/step\n",
      "Epoch 106/500\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2364 - lr: 2.5600e-08 - 750ms/epoch - 30ms/step\n",
      "Epoch 107/500\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2364 - lr: 2.5600e-08 - 740ms/epoch - 30ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2364 - lr: 5.1200e-09 - 728ms/epoch - 29ms/step\n",
      "Epoch 109/500\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2364 - lr: 5.1200e-09 - 788ms/epoch - 32ms/step\n",
      "Epoch 110/500\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2364 - lr: 5.1200e-09 - 755ms/epoch - 30ms/step\n",
      "Epoch 111/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2364 - lr: 1.0240e-09 - 743ms/epoch - 30ms/step\n",
      "Epoch 112/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2363 - lr: 1.0240e-09 - 773ms/epoch - 31ms/step\n",
      "Epoch 113/500\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2364 - lr: 1.0240e-09 - 763ms/epoch - 31ms/step\n",
      "Epoch 114/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2364 - lr: 2.0480e-10 - 781ms/epoch - 31ms/step\n",
      "Epoch 115/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2364 - lr: 2.0480e-10 - 738ms/epoch - 30ms/step\n",
      "Epoch 116/500\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2364 - lr: 2.0480e-10 - 762ms/epoch - 30ms/step\n",
      "Epoch 117/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2364 - lr: 4.0960e-11 - 781ms/epoch - 31ms/step\n",
      "Epoch 118/500\n",
      "25/25 - 1s - loss: 0.0450 - val_loss: 0.2364 - lr: 4.0960e-11 - 760ms/epoch - 30ms/step\n",
      "Epoch 119/500\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0445 - val_loss: 0.2364 - lr: 4.0960e-11 - 753ms/epoch - 30ms/step\n",
      "Epoch 120/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2364 - lr: 8.1920e-12 - 750ms/epoch - 30ms/step\n",
      "Epoch 121/500\n",
      "Restoring model weights from the end of the best epoch: 101.\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2364 - lr: 8.1920e-12 - 757ms/epoch - 30ms/step\n",
      "Epoch 121: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.8min\n",
      "7/7 - 2s - 2s/epoch - 325ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.9min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 2.6063 - val_loss: 0.3266 - lr: 0.0100 - 17s/epoch - 680ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4344 - val_loss: 0.3233 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4939 - val_loss: 0.3216 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4902 - val_loss: 0.3097 - lr: 0.0100 - 871ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.4798 - val_loss: 0.3592 - lr: 0.0100 - 843ms/epoch - 34ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.1813 - val_loss: 0.2467 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6395 - val_loss: 0.2469 - lr: 0.0020 - 886ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.5885 - val_loss: 0.2976 - lr: 0.0020 - 880ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 1.2391 - val_loss: 0.4811 - lr: 4.0000e-04 - 888ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.6118 - val_loss: 0.3245 - lr: 4.0000e-04 - 869ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3060 - val_loss: 0.3112 - lr: 4.0000e-04 - 876ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1775 - val_loss: 0.2755 - lr: 4.0000e-04 - 879ms/epoch - 35ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1369 - val_loss: 0.2882 - lr: 4.0000e-04 - 915ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1295 - val_loss: 0.2593 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1235 - val_loss: 0.2655 - lr: 4.0000e-04 - 861ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1153 - val_loss: 0.2313 - lr: 4.0000e-04 - 879ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1120 - val_loss: 0.2576 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1119 - val_loss: 0.2409 - lr: 4.0000e-04 - 864ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1047 - val_loss: 0.2229 - lr: 4.0000e-04 - 877ms/epoch - 35ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1038 - val_loss: 0.1830 - lr: 4.0000e-04 - 907ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1028 - val_loss: 0.1920 - lr: 4.0000e-04 - 887ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1045 - val_loss: 0.2501 - lr: 4.0000e-04 - 877ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0998 - val_loss: 0.2211 - lr: 4.0000e-04 - 857ms/epoch - 34ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0971 - val_loss: 0.2254 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0953 - val_loss: 0.2137 - lr: 4.0000e-04 - 883ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0943 - val_loss: 0.2183 - lr: 4.0000e-04 - 879ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0925 - val_loss: 0.2079 - lr: 4.0000e-04 - 896ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0896 - val_loss: 0.2253 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0900 - val_loss: 0.2143 - lr: 4.0000e-04 - 880ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0879 - val_loss: 0.2273 - lr: 4.0000e-04 - 875ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0866 - val_loss: 0.2180 - lr: 4.0000e-04 - 874ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0842 - val_loss: 0.2105 - lr: 4.0000e-04 - 865ms/epoch - 35ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0825 - val_loss: 0.2180 - lr: 4.0000e-04 - 880ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0826 - val_loss: 0.2160 - lr: 4.0000e-04 - 883ms/epoch - 35ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0795 - val_loss: 0.2137 - lr: 4.0000e-04 - 879ms/epoch - 35ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0792 - val_loss: 0.2012 - lr: 4.0000e-04 - 881ms/epoch - 35ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0777 - val_loss: 0.1840 - lr: 4.0000e-04 - 892ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0759 - val_loss: 0.2091 - lr: 4.0000e-04 - 871ms/epoch - 35ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0751 - val_loss: 0.1963 - lr: 4.0000e-04 - 884ms/epoch - 35ms/step\n",
      "Epoch 40/500\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "25/25 - 1s - loss: 0.0722 - val_loss: 0.2112 - lr: 4.0000e-04 - 907ms/epoch - 36ms/step\n",
      "Epoch 40: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  54.0s\n",
      "7/7 - 3s - 3s/epoch - 386ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  56.7s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 2.2776 - val_loss: 0.7782 - lr: 0.0100 - 17s/epoch - 697ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3138 - val_loss: 0.3222 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1993 - val_loss: 0.3877 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1689 - val_loss: 0.3619 - lr: 0.0100 - 869ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1800 - val_loss: 0.3789 - lr: 0.0100 - 858ms/epoch - 34ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1780 - val_loss: 0.3708 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1716 - val_loss: 0.3210 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3234 - val_loss: 0.2464 - lr: 0.0020 - 867ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2109 - val_loss: 0.2467 - lr: 0.0020 - 884ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1881 - val_loss: 0.2474 - lr: 0.0020 - 889ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2097 - val_loss: 0.2520 - lr: 4.0000e-04 - 861ms/epoch - 34ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1898 - val_loss: 0.2572 - lr: 4.0000e-04 - 860ms/epoch - 34ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1817 - val_loss: 0.2617 - lr: 4.0000e-04 - 864ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1785 - val_loss: 0.2631 - lr: 8.0000e-05 - 858ms/epoch - 34ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1777 - val_loss: 0.2644 - lr: 8.0000e-05 - 883ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.1774 - val_loss: 0.2657 - lr: 8.0000e-05 - 868ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1763 - val_loss: 0.2660 - lr: 1.6000e-05 - 882ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1761 - val_loss: 0.2663 - lr: 1.6000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.1759 - val_loss: 0.2666 - lr: 1.6000e-05 - 867ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1757 - val_loss: 0.2666 - lr: 3.2000e-06 - 835ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1757 - val_loss: 0.2667 - lr: 3.2000e-06 - 859ms/epoch - 34ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.2667 - lr: 3.2000e-06 - 861ms/epoch - 34ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.2667 - lr: 6.4000e-07 - 910ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.2668 - lr: 6.4000e-07 - 923ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.2668 - lr: 6.4000e-07 - 876ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.2668 - lr: 1.2800e-07 - 872ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.1752 - val_loss: 0.2668 - lr: 1.2800e-07 - 883ms/epoch - 35ms/step\n",
      "Epoch 28/500\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.1756 - val_loss: 0.2668 - lr: 1.2800e-07 - 890ms/epoch - 36ms/step\n",
      "Epoch 28: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.0s\n",
      "7/7 - 2s - 2s/epoch - 340ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  45.3s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 1.4561 - val_loss: 0.3273 - lr: 0.0100 - 17s/epoch - 696ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3728 - val_loss: 0.3611 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4653 - val_loss: 0.3547 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4650 - val_loss: 0.3157 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.4653 - val_loss: 0.5456 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.7531 - val_loss: 0.3701 - lr: 0.0020 - 888ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.3712 - val_loss: 0.4432 - lr: 0.0020 - 877ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2579 - val_loss: 0.3699 - lr: 0.0020 - 885ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.4313 - val_loss: 0.4352 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.4592 - val_loss: 0.3335 - lr: 0.0020 - 876ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.4409 - val_loss: 0.3281 - lr: 0.0020 - 856ms/epoch - 34ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 1.7609 - val_loss: 0.7437 - lr: 4.0000e-04 - 903ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7870 - val_loss: 0.3090 - lr: 4.0000e-04 - 885ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 1.0692 - val_loss: 0.5977 - lr: 4.0000e-04 - 866ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 1.3567 - val_loss: 0.9516 - lr: 8.0000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 1.1218 - val_loss: 1.2001 - lr: 8.0000e-05 - 858ms/epoch - 34ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 1.0083 - val_loss: 1.3890 - lr: 8.0000e-05 - 881ms/epoch - 35ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.9853 - val_loss: 1.4434 - lr: 1.6000e-05 - 872ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.9675 - val_loss: 1.4933 - lr: 1.6000e-05 - 923ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.9528 - val_loss: 1.5398 - lr: 1.6000e-05 - 845ms/epoch - 34ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.9452 - val_loss: 1.5498 - lr: 3.2000e-06 - 874ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.9440 - val_loss: 1.5596 - lr: 3.2000e-06 - 873ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.9387 - val_loss: 1.5692 - lr: 3.2000e-06 - 880ms/epoch - 35ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.9397 - val_loss: 1.5711 - lr: 6.4000e-07 - 875ms/epoch - 35ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.9386 - val_loss: 1.5731 - lr: 6.4000e-07 - 886ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.9381 - val_loss: 1.5751 - lr: 6.4000e-07 - 900ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.9377 - val_loss: 1.5756 - lr: 1.2800e-07 - 894ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.9372 - val_loss: 1.5760 - lr: 1.2800e-07 - 873ms/epoch - 35ms/step\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.9374 - val_loss: 1.5763 - lr: 1.2800e-07 - 882ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.9366 - val_loss: 1.5764 - lr: 2.5600e-08 - 859ms/epoch - 34ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.9369 - val_loss: 1.5765 - lr: 2.5600e-08 - 877ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.9390 - val_loss: 1.5765 - lr: 2.5600e-08 - 880ms/epoch - 35ms/step\n",
      "Epoch 33/500\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "25/25 - 1s - loss: 0.9379 - val_loss: 1.5765 - lr: 5.1200e-09 - 885ms/epoch - 35ms/step\n",
      "Epoch 33: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  47.5s\n",
      "7/7 - 2s - 2s/epoch - 335ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  49.8s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 1.5745 - val_loss: 0.3459 - lr: 0.0100 - 17s/epoch - 682ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4337 - val_loss: 0.3267 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4654 - val_loss: 0.3266 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5042 - val_loss: 0.3267 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.5429 - val_loss: 0.2569 - lr: 0.0100 - 873ms/epoch - 35ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.5484 - val_loss: 0.4132 - lr: 0.0020 - 878ms/epoch - 35ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7734 - val_loss: 0.3086 - lr: 0.0020 - 866ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8461 - val_loss: 0.3151 - lr: 0.0020 - 872ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 1.0931 - val_loss: 0.3697 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3680 - val_loss: 0.2679 - lr: 4.0000e-04 - 885ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1295 - val_loss: 0.2574 - lr: 4.0000e-04 - 843ms/epoch - 34ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0836 - val_loss: 0.2562 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0754 - val_loss: 0.2287 - lr: 4.0000e-04 - 893ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0722 - val_loss: 0.2433 - lr: 4.0000e-04 - 862ms/epoch - 34ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0731 - val_loss: 0.2317 - lr: 4.0000e-04 - 843ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.2656 - lr: 4.0000e-04 - 886ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.2460 - lr: 4.0000e-04 - 882ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0661 - val_loss: 0.2521 - lr: 4.0000e-04 - 864ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.2601 - lr: 4.0000e-04 - 874ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.2649 - lr: 4.0000e-04 - 858ms/epoch - 34ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.2603 - lr: 4.0000e-04 - 886ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.2594 - lr: 4.0000e-04 - 896ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.2592 - lr: 4.0000e-04 - 905ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.2485 - lr: 4.0000e-04 - 869ms/epoch - 35ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0665 - val_loss: 0.2197 - lr: 8.0000e-05 - 870ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.2121 - lr: 8.0000e-05 - 882ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.2101 - lr: 8.0000e-05 - 874ms/epoch - 35ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.2058 - lr: 8.0000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.2022 - lr: 8.0000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1997 - lr: 8.0000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0470 - val_loss: 0.1992 - lr: 8.0000e-05 - 871ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1979 - lr: 8.0000e-05 - 882ms/epoch - 35ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.1978 - lr: 8.0000e-05 - 876ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1964 - lr: 8.0000e-05 - 878ms/epoch - 35ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1974 - lr: 8.0000e-05 - 861ms/epoch - 34ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0458 - val_loss: 0.1961 - lr: 8.0000e-05 - 901ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1966 - lr: 8.0000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.1959 - lr: 8.0000e-05 - 875ms/epoch - 35ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.1964 - lr: 8.0000e-05 - 845ms/epoch - 34ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.1968 - lr: 8.0000e-05 - 880ms/epoch - 35ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.1959 - lr: 8.0000e-05 - 855ms/epoch - 34ms/step\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0450 - val_loss: 0.1960 - lr: 8.0000e-05 - 872ms/epoch - 35ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0444 - val_loss: 0.1941 - lr: 1.6000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0417 - val_loss: 0.1939 - lr: 1.6000e-05 - 849ms/epoch - 34ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0418 - val_loss: 0.1931 - lr: 1.6000e-05 - 871ms/epoch - 35ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0414 - val_loss: 0.1931 - lr: 1.6000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0410 - val_loss: 0.1928 - lr: 1.6000e-05 - 886ms/epoch - 35ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0411 - val_loss: 0.1926 - lr: 1.6000e-05 - 875ms/epoch - 35ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0422 - val_loss: 0.1922 - lr: 1.6000e-05 - 863ms/epoch - 35ms/step\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0419 - val_loss: 0.1920 - lr: 1.6000e-05 - 911ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1919 - lr: 3.2000e-06 - 887ms/epoch - 35ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1917 - lr: 3.2000e-06 - 877ms/epoch - 35ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0403 - val_loss: 0.1915 - lr: 3.2000e-06 - 873ms/epoch - 35ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0394 - val_loss: 0.1914 - lr: 3.2000e-06 - 884ms/epoch - 35ms/step\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1914 - lr: 3.2000e-06 - 888ms/epoch - 36ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1914 - lr: 3.2000e-06 - 856ms/epoch - 34ms/step\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0405 - val_loss: 0.1913 - lr: 3.2000e-06 - 905ms/epoch - 36ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1913 - lr: 6.4000e-07 - 903ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1912 - lr: 6.4000e-07 - 884ms/epoch - 35ms/step\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0403 - val_loss: 0.1912 - lr: 6.4000e-07 - 887ms/epoch - 35ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0398 - val_loss: 0.1912 - lr: 1.2800e-07 - 863ms/epoch - 35ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0404 - val_loss: 0.1912 - lr: 1.2800e-07 - 864ms/epoch - 35ms/step\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1912 - lr: 1.2800e-07 - 918ms/epoch - 37ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1912 - lr: 2.5600e-08 - 898ms/epoch - 36ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0403 - val_loss: 0.1912 - lr: 2.5600e-08 - 876ms/epoch - 35ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1912 - lr: 2.5600e-08 - 873ms/epoch - 35ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1912 - lr: 5.1200e-09 - 881ms/epoch - 35ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0405 - val_loss: 0.1912 - lr: 5.1200e-09 - 869ms/epoch - 35ms/step\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0402 - val_loss: 0.1912 - lr: 5.1200e-09 - 904ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1912 - lr: 1.0240e-09 - 888ms/epoch - 36ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1912 - lr: 1.0240e-09 - 858ms/epoch - 34ms/step\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1912 - lr: 1.0240e-09 - 846ms/epoch - 34ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0398 - val_loss: 0.1912 - lr: 2.0480e-10 - 871ms/epoch - 35ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1912 - lr: 2.0480e-10 - 873ms/epoch - 35ms/step\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0402 - val_loss: 0.1912 - lr: 2.0480e-10 - 890ms/epoch - 36ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1912 - lr: 4.0960e-11 - 847ms/epoch - 34ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0398 - val_loss: 0.1912 - lr: 4.0960e-11 - 874ms/epoch - 35ms/step\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1912 - lr: 4.0960e-11 - 873ms/epoch - 35ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1912 - lr: 8.1920e-12 - 874ms/epoch - 35ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0403 - val_loss: 0.1912 - lr: 8.1920e-12 - 890ms/epoch - 36ms/step\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0398 - val_loss: 0.1912 - lr: 8.1920e-12 - 868ms/epoch - 35ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0392 - val_loss: 0.1912 - lr: 1.6384e-12 - 879ms/epoch - 35ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1912 - lr: 1.6384e-12 - 858ms/epoch - 34ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0398 - val_loss: 0.1912 - lr: 1.6384e-12 - 900ms/epoch - 36ms/step\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0402 - val_loss: 0.1912 - lr: 1.6384e-12 - 891ms/epoch - 36ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0403 - val_loss: 0.1912 - lr: 3.2768e-13 - 894ms/epoch - 36ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0395 - val_loss: 0.1912 - lr: 3.2768e-13 - 850ms/epoch - 34ms/step\n",
      "Epoch 88/500\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 6.553599351567796e-14.\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1912 - lr: 3.2768e-13 - 881ms/epoch - 35ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1912 - lr: 6.5536e-14 - 868ms/epoch - 35ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1912 - lr: 6.5536e-14 - 860ms/epoch - 34ms/step\n",
      "Epoch 91/500\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.310719924523668e-14.\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1912 - lr: 6.5536e-14 - 897ms/epoch - 36ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0404 - val_loss: 0.1912 - lr: 1.3107e-14 - 868ms/epoch - 35ms/step\n",
      "Epoch 93/500\n",
      "Restoring model weights from the end of the best epoch: 73.\n",
      "25/25 - 1s - loss: 0.0405 - val_loss: 0.1912 - lr: 1.3107e-14 - 893ms/epoch - 36ms/step\n",
      "Epoch 93: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.7min\n",
      "7/7 - 2s - 2s/epoch - 327ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 3.0461 - val_loss: 1.8955 - lr: 0.0100 - 17s/epoch - 698ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1704 - val_loss: 1.1926 - lr: 0.0100 - 878ms/epoch - 35ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1960 - val_loss: 0.9850 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1897 - val_loss: 0.7360 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2227 - val_loss: 0.9560 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.5438 - val_loss: 2.1492 - lr: 0.0020 - 985ms/epoch - 39ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2092 - val_loss: 1.8747 - lr: 0.0020 - 979ms/epoch - 39ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1898 - val_loss: 2.0026 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3360 - val_loss: 2.3789 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2328 - val_loss: 2.5173 - lr: 4.0000e-04 - 903ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1743 - val_loss: 2.4713 - lr: 4.0000e-04 - 925ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1717 - val_loss: 2.4841 - lr: 8.0000e-05 - 935ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1497 - val_loss: 2.4913 - lr: 8.0000e-05 - 943ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1389 - val_loss: 2.4912 - lr: 8.0000e-05 - 972ms/epoch - 39ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1306 - val_loss: 2.4829 - lr: 8.0000e-05 - 917ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1237 - val_loss: 2.4688 - lr: 8.0000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1181 - val_loss: 2.4471 - lr: 8.0000e-05 - 899ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1135 - val_loss: 2.4220 - lr: 8.0000e-05 - 923ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1093 - val_loss: 2.3936 - lr: 8.0000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1055 - val_loss: 2.3634 - lr: 8.0000e-05 - 884ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1025 - val_loss: 2.3300 - lr: 8.0000e-05 - 872ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0997 - val_loss: 2.2960 - lr: 8.0000e-05 - 893ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0969 - val_loss: 2.2611 - lr: 8.0000e-05 - 884ms/epoch - 35ms/step\n",
      "Epoch 24/500\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "25/25 - 1s - loss: 0.0955 - val_loss: 2.2257 - lr: 8.0000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 24: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.6s\n",
      "7/7 - 2s - 2s/epoch - 333ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  42.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.8710 - val_loss: 0.3688 - lr: 0.0100 - 17s/epoch - 682ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.6431 - val_loss: 0.5166 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5446 - val_loss: 0.2737 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5554 - val_loss: 0.6461 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6833 - val_loss: 0.4563 - lr: 0.0100 - 762ms/epoch - 30ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.7369 - val_loss: 0.6204 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.3821 - val_loss: 0.5601 - lr: 0.0020 - 786ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3709 - val_loss: 0.6038 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3627 - val_loss: 0.6098 - lr: 0.0020 - 763ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3600 - val_loss: 0.6075 - lr: 0.0020 - 770ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3588 - val_loss: 0.6071 - lr: 0.0020 - 762ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.3579 - val_loss: 0.6073 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.3572 - val_loss: 0.6071 - lr: 0.0020 - 782ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.3566 - val_loss: 0.6069 - lr: 0.0020 - 754ms/epoch - 30ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.3560 - val_loss: 0.6065 - lr: 0.0020 - 808ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.3554 - val_loss: 0.6060 - lr: 0.0020 - 772ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.3548 - val_loss: 0.6054 - lr: 0.0020 - 732ms/epoch - 29ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.3542 - val_loss: 0.6047 - lr: 0.0020 - 762ms/epoch - 30ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.3536 - val_loss: 0.6040 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.3530 - val_loss: 0.6034 - lr: 0.0020 - 758ms/epoch - 30ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.3523 - val_loss: 0.6029 - lr: 0.0020 - 762ms/epoch - 30ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.3516 - val_loss: 0.6024 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "25/25 - 1s - loss: 0.3509 - val_loss: 0.6018 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.9s\n",
      "7/7 - 2s - 2s/epoch - 354ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  38.4s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.4694 - val_loss: 0.2581 - lr: 0.0100 - 16s/epoch - 658ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9747 - val_loss: 1.0492 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0931 - val_loss: 1.8150 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.8251 - val_loss: 1.9251 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7696 - val_loss: 1.9296 - lr: 0.0020 - 781ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7698 - val_loss: 1.9376 - lr: 0.0020 - 779ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7693 - val_loss: 1.9451 - lr: 0.0020 - 754ms/epoch - 30ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7628 - val_loss: 1.9468 - lr: 4.0000e-04 - 734ms/epoch - 29ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7628 - val_loss: 1.9488 - lr: 4.0000e-04 - 747ms/epoch - 30ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7626 - val_loss: 1.9508 - lr: 4.0000e-04 - 766ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7612 - val_loss: 1.9512 - lr: 8.0000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7612 - val_loss: 1.9517 - lr: 8.0000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7612 - val_loss: 1.9521 - lr: 8.0000e-05 - 789ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7609 - val_loss: 1.9522 - lr: 1.6000e-05 - 795ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7609 - val_loss: 1.9523 - lr: 1.6000e-05 - 771ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7609 - val_loss: 1.9523 - lr: 1.6000e-05 - 769ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7608 - val_loss: 1.9524 - lr: 3.2000e-06 - 792ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7608 - val_loss: 1.9524 - lr: 3.2000e-06 - 786ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7608 - val_loss: 1.9524 - lr: 3.2000e-06 - 795ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7608 - val_loss: 1.9524 - lr: 6.4000e-07 - 781ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7608 - val_loss: 1.9524 - lr: 6.4000e-07 - 800ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.7s\n",
      "7/7 - 2s - 2s/epoch - 320ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  36.9s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.5694 - val_loss: 0.3321 - lr: 0.0100 - 17s/epoch - 665ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.9632 - val_loss: 2.2957 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0369 - val_loss: 2.1309 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.0112 - val_loss: 2.2300 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8717 - val_loss: 2.2323 - lr: 0.0020 - 777ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8727 - val_loss: 2.2467 - lr: 0.0020 - 790ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8715 - val_loss: 2.2577 - lr: 0.0020 - 771ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8498 - val_loss: 2.2593 - lr: 4.0000e-04 - 761ms/epoch - 30ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8498 - val_loss: 2.2613 - lr: 4.0000e-04 - 780ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8497 - val_loss: 2.2631 - lr: 4.0000e-04 - 799ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8455 - val_loss: 2.2635 - lr: 8.0000e-05 - 764ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8455 - val_loss: 2.2638 - lr: 8.0000e-05 - 746ms/epoch - 30ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8455 - val_loss: 2.2642 - lr: 8.0000e-05 - 778ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8447 - val_loss: 2.2642 - lr: 1.6000e-05 - 774ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8447 - val_loss: 2.2643 - lr: 1.6000e-05 - 776ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8447 - val_loss: 2.2644 - lr: 1.6000e-05 - 783ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8445 - val_loss: 2.2644 - lr: 3.2000e-06 - 773ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8445 - val_loss: 2.2644 - lr: 3.2000e-06 - 799ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8445 - val_loss: 2.2644 - lr: 3.2000e-06 - 779ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8445 - val_loss: 2.2644 - lr: 6.4000e-07 - 743ms/epoch - 30ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8445 - val_loss: 2.2644 - lr: 6.4000e-07 - 785ms/epoch - 31ms/step\n",
      "Epoch 21: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.1s\n",
      "7/7 - 2s - 2s/epoch - 324ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  36.3s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.3227 - val_loss: 0.2706 - lr: 0.0100 - 16s/epoch - 647ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1001 - val_loss: 0.3392 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7029 - val_loss: 1.1051 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 1.5249 - val_loss: 0.3991 - lr: 0.0100 - 755ms/epoch - 30ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.6462 - val_loss: 0.6213 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1362 - val_loss: 0.2564 - lr: 0.0020 - 764ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1169 - val_loss: 0.2510 - lr: 0.0020 - 782ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1030 - val_loss: 0.2463 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2465 - lr: 4.0000e-04 - 798ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0942 - val_loss: 0.2467 - lr: 4.0000e-04 - 763ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2469 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0939 - val_loss: 0.2471 - lr: 4.0000e-04 - 772ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0939 - val_loss: 0.2472 - lr: 4.0000e-04 - 738ms/epoch - 30ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2474 - lr: 4.0000e-04 - 766ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2476 - lr: 4.0000e-04 - 776ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0937 - val_loss: 0.2477 - lr: 4.0000e-04 - 815ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2478 - lr: 4.0000e-04 - 782ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0937 - val_loss: 0.2479 - lr: 4.0000e-04 - 764ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2480 - lr: 4.0000e-04 - 766ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2481 - lr: 4.0000e-04 - 781ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2481 - lr: 4.0000e-04 - 777ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0923 - val_loss: 0.2481 - lr: 8.0000e-05 - 750ms/epoch - 30ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0923 - val_loss: 0.2481 - lr: 8.0000e-05 - 744ms/epoch - 30ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0924 - val_loss: 0.2481 - lr: 8.0000e-05 - 781ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0924 - val_loss: 0.2481 - lr: 8.0000e-05 - 773ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0920 - val_loss: 0.2481 - lr: 1.6000e-05 - 748ms/epoch - 30ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0920 - val_loss: 0.2481 - lr: 1.6000e-05 - 742ms/epoch - 30ms/step\n",
      "Epoch 28/500\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "25/25 - 1s - loss: 0.0921 - val_loss: 0.2481 - lr: 1.6000e-05 - 779ms/epoch - 31ms/step\n",
      "Epoch 28: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  39.6s\n",
      "7/7 - 2s - 2s/epoch - 332ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  41.8s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 1.1131 - val_loss: 3.2826 - lr: 0.0100 - 17s/epoch - 671ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4573 - val_loss: 4.4488 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2566 - val_loss: 4.2140 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2615 - val_loss: 4.2301 - lr: 0.0100 - 771ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2603 - val_loss: 4.1723 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2599 - val_loss: 4.1460 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2137 - val_loss: 4.1727 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2138 - val_loss: 4.2448 - lr: 0.0020 - 798ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2104 - val_loss: 4.2719 - lr: 0.0020 - 777ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2092 - val_loss: 4.2749 - lr: 0.0020 - 741ms/epoch - 30ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2093 - val_loss: 4.2719 - lr: 0.0020 - 755ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2096 - val_loss: 4.2685 - lr: 0.0020 - 778ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2100 - val_loss: 4.2652 - lr: 0.0020 - 777ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2005 - val_loss: 4.2655 - lr: 4.0000e-04 - 786ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2007 - val_loss: 4.2696 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2007 - val_loss: 4.2731 - lr: 4.0000e-04 - 788ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.2006 - val_loss: 4.2759 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 4.2764 - lr: 8.0000e-05 - 763ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 4.2770 - lr: 8.0000e-05 - 762ms/epoch - 30ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 4.2776 - lr: 8.0000e-05 - 801ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 4.2782 - lr: 8.0000e-05 - 792ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.2s\n",
      "7/7 - 2s - 2s/epoch - 339ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  36.5s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.7473 - val_loss: 1.6891 - lr: 0.0100 - 16s/epoch - 641ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7169 - val_loss: 1.6125 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6783 - val_loss: 1.5045 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6221 - val_loss: 1.3384 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.5344 - val_loss: 1.0796 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4022 - val_loss: 0.7214 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2404 - val_loss: 0.3762 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1196 - val_loss: 0.1976 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0716 - val_loss: 0.1503 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1433 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1439 - lr: 0.0100 - 790ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1453 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1464 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1470 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1477 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1483 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1486 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1491 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1487 - lr: 0.0100 - 782ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1492 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.1491 - lr: 0.0020 - 817ms/epoch - 33ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1491 - lr: 0.0020 - 805ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1490 - lr: 0.0020 - 781ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1489 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.1489 - lr: 4.0000e-04 - 794ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1489 - lr: 4.0000e-04 - 790ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1489 - lr: 4.0000e-04 - 762ms/epoch - 30ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1489 - lr: 4.0000e-04 - 818ms/epoch - 33ms/step\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1489 - lr: 4.0000e-04 - 824ms/epoch - 33ms/step\n",
      "Epoch 30/500\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.1489 - lr: 8.0000e-05 - 800ms/epoch - 32ms/step\n",
      "Epoch 30: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.8s\n",
      "7/7 - 3s - 3s/epoch - 381ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time=  44.4s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.7764 - val_loss: 1.7151 - lr: 0.0100 - 16s/epoch - 659ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7440 - val_loss: 1.6347 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7045 - val_loss: 1.5253 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6480 - val_loss: 1.3599 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.5593 - val_loss: 1.1010 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4202 - val_loss: 0.7313 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2440 - val_loss: 0.3649 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1112 - val_loss: 0.1852 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0657 - val_loss: 0.1440 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.1387 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1393 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1407 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.1421 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1432 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1440 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1447 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1449 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1455 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1460 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0508 - val_loss: 0.1457 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1460 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1467 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1465 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1464 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1465 - lr: 0.0100 - 864ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1463 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1457 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.1460 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1463 - lr: 0.0020 - 850ms/epoch - 34ms/step\n",
      "Epoch 30/500\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1464 - lr: 0.0020 - 895ms/epoch - 36ms/step\n",
      "Epoch 30: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  42.0s\n",
      "7/7 - 3s - 3s/epoch - 430ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time=  45.0s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.9139 - val_loss: 1.7425 - lr: 0.0100 - 17s/epoch - 672ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8736 - val_loss: 1.6643 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8255 - val_loss: 1.5555 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7522 - val_loss: 1.3740 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6232 - val_loss: 1.0487 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3977 - val_loss: 0.5612 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1468 - val_loss: 0.2264 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.1525 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0344 - val_loss: 0.1431 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1428 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.1443 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0254 - val_loss: 0.1461 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0252 - val_loss: 0.1477 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0243 - val_loss: 0.1487 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0249 - val_loss: 0.1497 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0245 - val_loss: 0.1506 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0245 - val_loss: 0.1511 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0243 - val_loss: 0.1512 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0241 - val_loss: 0.1514 - lr: 0.0020 - 813ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0239 - val_loss: 0.1514 - lr: 0.0020 - 800ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0242 - val_loss: 0.1515 - lr: 0.0020 - 799ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0241 - val_loss: 0.1514 - lr: 0.0020 - 772ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0244 - val_loss: 0.1513 - lr: 0.0020 - 782ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0239 - val_loss: 0.1513 - lr: 4.0000e-04 - 817ms/epoch - 33ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0239 - val_loss: 0.1513 - lr: 4.0000e-04 - 797ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0237 - val_loss: 0.1513 - lr: 4.0000e-04 - 798ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0241 - val_loss: 0.1513 - lr: 4.0000e-04 - 784ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0239 - val_loss: 0.1513 - lr: 4.0000e-04 - 808ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0240 - val_loss: 0.1513 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "25/25 - 1s - loss: 0.0238 - val_loss: 0.1513 - lr: 8.0000e-05 - 828ms/epoch - 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  42.0s\n",
      "7/7 - 2s - 2s/epoch - 322ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time=  44.3s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.9686 - val_loss: 1.7516 - lr: 0.0100 - 17s/epoch - 670ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9385 - val_loss: 1.7030 - lr: 0.0100 - 849ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9001 - val_loss: 1.6294 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8343 - val_loss: 1.4847 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6918 - val_loss: 1.1444 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3859 - val_loss: 0.5248 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1171 - val_loss: 0.2222 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0745 - val_loss: 0.1693 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0659 - val_loss: 0.1541 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1481 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1461 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1459 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0509 - val_loss: 0.1462 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1469 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1474 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1481 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1481 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1484 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1486 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1490 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.1490 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.1487 - lr: 0.0100 - 768ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.1489 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0468 - val_loss: 0.1490 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0466 - val_loss: 0.1488 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.1485 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1486 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0466 - val_loss: 0.1485 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1483 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.1479 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0455 - val_loss: 0.1476 - lr: 0.0020 - 807ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "25/25 - 1s - loss: 0.0457 - val_loss: 0.1473 - lr: 0.0020 - 849ms/epoch - 34ms/step\n",
      "Epoch 32: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.4s\n",
      "7/7 - 2s - 2s/epoch - 336ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time=  45.7s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.6437 - val_loss: 1.7488 - lr: 0.0100 - 16s/epoch - 644ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6149 - val_loss: 1.7177 - lr: 0.0100 - 827ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5766 - val_loss: 1.6663 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5165 - val_loss: 1.5661 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4104 - val_loss: 1.3584 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2469 - val_loss: 1.0170 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1178 - val_loss: 0.7624 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0821 - val_loss: 0.6610 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0750 - val_loss: 0.6031 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0722 - val_loss: 0.5575 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0698 - val_loss: 0.5163 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0682 - val_loss: 0.4789 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0662 - val_loss: 0.4456 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0650 - val_loss: 0.4152 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.3881 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.3649 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.3441 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.3237 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.3062 - lr: 0.0100 - 825ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.2907 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.2764 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.2641 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.2530 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2433 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2346 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.2267 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.2194 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.2135 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.2076 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.2030 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1985 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1944 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1910 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1875 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1842 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0537 - val_loss: 0.1823 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1794 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1769 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1744 - lr: 0.0020 - 809ms/epoch - 32ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1725 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 0.0020 - 820ms/epoch - 33ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1701 - lr: 0.0020 - 826ms/epoch - 33ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1691 - lr: 0.0020 - 809ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1684 - lr: 0.0020 - 770ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.1677 - lr: 0.0020 - 804ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1671 - lr: 0.0020 - 795ms/epoch - 32ms/step\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1666 - lr: 0.0020 - 815ms/epoch - 33ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1665 - lr: 4.0000e-04 - 815ms/epoch - 33ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1663 - lr: 4.0000e-04 - 818ms/epoch - 33ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1662 - lr: 4.0000e-04 - 804ms/epoch - 32ms/step\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1661 - lr: 4.0000e-04 - 808ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1660 - lr: 8.0000e-05 - 815ms/epoch - 33ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1660 - lr: 8.0000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1660 - lr: 8.0000e-05 - 789ms/epoch - 32ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1660 - lr: 1.6000e-05 - 785ms/epoch - 31ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1660 - lr: 1.6000e-05 - 829ms/epoch - 33ms/step\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1660 - lr: 1.6000e-05 - 817ms/epoch - 33ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1660 - lr: 3.2000e-06 - 797ms/epoch - 32ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1660 - lr: 3.2000e-06 - 808ms/epoch - 32ms/step\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1659 - lr: 3.2000e-06 - 789ms/epoch - 32ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1659 - lr: 6.4000e-07 - 793ms/epoch - 32ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 6.4000e-07 - 813ms/epoch - 33ms/step\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1659 - lr: 6.4000e-07 - 796ms/epoch - 32ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1659 - lr: 1.2800e-07 - 794ms/epoch - 32ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 1.2800e-07 - 802ms/epoch - 32ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 1.2800e-07 - 785ms/epoch - 31ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 2.5600e-08 - 807ms/epoch - 32ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 2.5600e-08 - 801ms/epoch - 32ms/step\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 2.5600e-08 - 787ms/epoch - 31ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 5.1200e-09 - 784ms/epoch - 31ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 5.1200e-09 - 816ms/epoch - 33ms/step\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1659 - lr: 5.1200e-09 - 799ms/epoch - 32ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1659 - lr: 1.0240e-09 - 793ms/epoch - 32ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1659 - lr: 1.0240e-09 - 808ms/epoch - 32ms/step\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1659 - lr: 1.0240e-09 - 815ms/epoch - 33ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 2.0480e-10 - 785ms/epoch - 31ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 2.0480e-10 - 796ms/epoch - 32ms/step\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1659 - lr: 2.0480e-10 - 795ms/epoch - 32ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 4.0960e-11 - 803ms/epoch - 32ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 4.0960e-11 - 789ms/epoch - 32ms/step\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1659 - lr: 4.0960e-11 - 810ms/epoch - 32ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 8.1920e-12 - 776ms/epoch - 31ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1659 - lr: 8.1920e-12 - 806ms/epoch - 32ms/step\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 8.1920e-12 - 798ms/epoch - 32ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 1.6384e-12 - 798ms/epoch - 32ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 1.6384e-12 - 826ms/epoch - 33ms/step\n",
      "Epoch 87/500\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 1.6384e-12 - 808ms/epoch - 32ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1659 - lr: 3.2768e-13 - 792ms/epoch - 32ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 3.2768e-13 - 774ms/epoch - 31ms/step\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 6.553599351567796e-14.\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1659 - lr: 3.2768e-13 - 796ms/epoch - 32ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 6.5536e-14 - 781ms/epoch - 31ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1659 - lr: 6.5536e-14 - 789ms/epoch - 32ms/step\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.310719924523668e-14.\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1659 - lr: 6.5536e-14 - 795ms/epoch - 32ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1659 - lr: 1.3107e-14 - 811ms/epoch - 32ms/step\n",
      "Epoch 95/500\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1659 - lr: 1.3107e-14 - 808ms/epoch - 32ms/step\n",
      "Epoch 95: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.6min\n",
      "7/7 - 3s - 3s/epoch - 377ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.6min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.3691 - val_loss: 0.2293 - lr: 0.0100 - 16s/epoch - 656ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1776 - val_loss: 0.1891 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1817 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1792 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1724 - lr: 0.0100 - 760ms/epoch - 30ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1685 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1674 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.1613 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0670 - val_loss: 0.1566 - lr: 0.0020 - 771ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1519 - lr: 0.0020 - 815ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1491 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1478 - lr: 0.0020 - 775ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1469 - lr: 0.0020 - 790ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1461 - lr: 0.0020 - 799ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1449 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1439 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1441 - lr: 0.0020 - 799ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1433 - lr: 0.0020 - 793ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1428 - lr: 0.0020 - 796ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1418 - lr: 0.0020 - 797ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.1423 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1411 - lr: 0.0020 - 796ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1408 - lr: 0.0020 - 776ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1415 - lr: 0.0020 - 781ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1407 - lr: 0.0020 - 790ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1401 - lr: 0.0020 - 805ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1393 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1383 - lr: 0.0020 - 785ms/epoch - 31ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.1373 - lr: 0.0020 - 792ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1371 - lr: 0.0020 - 794ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1375 - lr: 0.0020 - 771ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1372 - lr: 0.0020 - 760ms/epoch - 30ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1367 - lr: 0.0020 - 818ms/epoch - 33ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1362 - lr: 0.0020 - 814ms/epoch - 33ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1357 - lr: 0.0020 - 789ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0508 - val_loss: 0.1359 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1350 - lr: 0.0020 - 815ms/epoch - 33ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1346 - lr: 0.0020 - 825ms/epoch - 33ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1355 - lr: 4.0000e-04 - 787ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1362 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1365 - lr: 4.0000e-04 - 782ms/epoch - 31ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0491 - val_loss: 0.1364 - lr: 4.0000e-04 - 802ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1363 - lr: 4.0000e-04 - 787ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0491 - val_loss: 0.1361 - lr: 4.0000e-04 - 780ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1359 - lr: 4.0000e-04 - 778ms/epoch - 31ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1360 - lr: 8.0000e-05 - 768ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1360 - lr: 8.0000e-05 - 829ms/epoch - 33ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1360 - lr: 8.0000e-05 - 807ms/epoch - 32ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1360 - lr: 8.0000e-05 - 834ms/epoch - 33ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1360 - lr: 1.6000e-05 - 769ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1360 - lr: 1.6000e-05 - 785ms/epoch - 31ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1360 - lr: 1.6000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1360 - lr: 1.6000e-05 - 810ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1360 - lr: 1.6000e-05 - 800ms/epoch - 32ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1360 - lr: 1.6000e-05 - 796ms/epoch - 32ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1360 - lr: 1.6000e-05 - 800ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1360 - lr: 1.6000e-05 - 748ms/epoch - 30ms/step\n",
      "Epoch 58/500\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1360 - lr: 3.2000e-06 - 792ms/epoch - 32ms/step\n",
      "Epoch 58: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 2s - 2s/epoch - 334ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.1min\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.4010 - val_loss: 0.2167 - lr: 0.0100 - 16s/epoch - 642ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1388 - val_loss: 0.2073 - lr: 0.0100 - 844ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.1977 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1880 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1819 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1745 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1735 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1721 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0683 - val_loss: 0.1636 - lr: 0.0020 - 811ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.1575 - lr: 0.0020 - 812ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1535 - lr: 0.0020 - 765ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1557 - lr: 4.0000e-04 - 792ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1566 - lr: 4.0000e-04 - 771ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1568 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1566 - lr: 4.0000e-04 - 788ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1560 - lr: 4.0000e-04 - 814ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1555 - lr: 4.0000e-04 - 795ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1550 - lr: 4.0000e-04 - 794ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1545 - lr: 4.0000e-04 - 784ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1538 - lr: 4.0000e-04 - 816ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1532 - lr: 4.0000e-04 - 766ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.1528 - lr: 4.0000e-04 - 751ms/epoch - 30ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1522 - lr: 4.0000e-04 - 772ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1518 - lr: 4.0000e-04 - 811ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1513 - lr: 4.0000e-04 - 811ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1510 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1506 - lr: 4.0000e-04 - 804ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1504 - lr: 4.0000e-04 - 799ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0509 - val_loss: 0.1502 - lr: 4.0000e-04 - 765ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.1497 - lr: 4.0000e-04 - 787ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1494 - lr: 4.0000e-04 - 812ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0508 - val_loss: 0.1491 - lr: 4.0000e-04 - 794ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.1489 - lr: 4.0000e-04 - 801ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1487 - lr: 4.0000e-04 - 803ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1485 - lr: 4.0000e-04 - 790ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1484 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0509 - val_loss: 0.1482 - lr: 4.0000e-04 - 773ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0508 - val_loss: 0.1480 - lr: 4.0000e-04 - 780ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1481 - lr: 8.0000e-05 - 824ms/epoch - 33ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1481 - lr: 8.0000e-05 - 785ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1482 - lr: 8.0000e-05 - 799ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1482 - lr: 8.0000e-05 - 778ms/epoch - 31ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1483 - lr: 8.0000e-05 - 780ms/epoch - 31ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1483 - lr: 8.0000e-05 - 756ms/epoch - 30ms/step\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1484 - lr: 8.0000e-05 - 793ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1484 - lr: 1.6000e-05 - 810ms/epoch - 32ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.1484 - lr: 1.6000e-05 - 799ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1484 - lr: 1.6000e-05 - 770ms/epoch - 31ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1484 - lr: 1.6000e-05 - 796ms/epoch - 32ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1484 - lr: 3.2000e-06 - 784ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.1484 - lr: 3.2000e-06 - 785ms/epoch - 31ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1484 - lr: 3.2000e-06 - 765ms/epoch - 31ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1484 - lr: 6.4000e-07 - 767ms/epoch - 31ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1484 - lr: 6.4000e-07 - 816ms/epoch - 33ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1484 - lr: 6.4000e-07 - 796ms/epoch - 32ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1484 - lr: 1.2800e-07 - 784ms/epoch - 31ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.1484 - lr: 1.2800e-07 - 779ms/epoch - 31ms/step\n",
      "Epoch 58/500\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1484 - lr: 1.2800e-07 - 792ms/epoch - 32ms/step\n",
      "Epoch 58: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 2s - 2s/epoch - 328ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.1min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.4206 - val_loss: 0.1985 - lr: 0.0100 - 17s/epoch - 680ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1039 - val_loss: 0.2030 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0250 - val_loss: 0.1975 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0245 - val_loss: 0.1914 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0246 - val_loss: 0.1849 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0254 - val_loss: 0.1820 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0260 - val_loss: 0.1803 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0379 - val_loss: 0.1658 - lr: 0.0020 - 816ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.1607 - lr: 0.0020 - 806ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1579 - lr: 0.0020 - 785ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0324 - val_loss: 0.1598 - lr: 4.0000e-04 - 769ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0310 - val_loss: 0.1604 - lr: 4.0000e-04 - 778ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0303 - val_loss: 0.1605 - lr: 4.0000e-04 - 768ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1606 - lr: 8.0000e-05 - 765ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 0.1608 - lr: 8.0000e-05 - 806ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1609 - lr: 8.0000e-05 - 785ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1609 - lr: 1.6000e-05 - 741ms/epoch - 30ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1609 - lr: 1.6000e-05 - 778ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0294 - val_loss: 0.1609 - lr: 1.6000e-05 - 746ms/epoch - 30ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1609 - lr: 3.2000e-06 - 733ms/epoch - 29ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.1610 - lr: 3.2000e-06 - 750ms/epoch - 30ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0294 - val_loss: 0.1610 - lr: 3.2000e-06 - 774ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1610 - lr: 6.4000e-07 - 801ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1610 - lr: 6.4000e-07 - 777ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1610 - lr: 6.4000e-07 - 755ms/epoch - 30ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.1610 - lr: 1.2800e-07 - 767ms/epoch - 31ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 0.1610 - lr: 1.2800e-07 - 781ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1610 - lr: 1.2800e-07 - 766ms/epoch - 31ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1610 - lr: 2.5600e-08 - 745ms/epoch - 30ms/step\n",
      "Epoch 30/500\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1610 - lr: 2.5600e-08 - 774ms/epoch - 31ms/step\n",
      "Epoch 30: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.4s\n",
      "7/7 - 2s - 2s/epoch - 339ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time=  43.7s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.5349 - val_loss: 0.1844 - lr: 0.0100 - 17s/epoch - 667ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1141 - val_loss: 0.1687 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1765 - lr: 0.0100 - 781ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1721 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1733 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1738 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1749 - lr: 0.0100 - 774ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1697 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1606 - lr: 0.0020 - 781ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0491 - val_loss: 0.1582 - lr: 0.0020 - 785ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1570 - lr: 0.0020 - 786ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1575 - lr: 4.0000e-04 - 778ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.1573 - lr: 4.0000e-04 - 788ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0476 - val_loss: 0.1570 - lr: 4.0000e-04 - 758ms/epoch - 30ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.1566 - lr: 4.0000e-04 - 771ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1563 - lr: 4.0000e-04 - 784ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1558 - lr: 4.0000e-04 - 759ms/epoch - 30ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0473 - val_loss: 0.1552 - lr: 4.0000e-04 - 796ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0472 - val_loss: 0.1546 - lr: 4.0000e-04 - 773ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1542 - lr: 4.0000e-04 - 789ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0474 - val_loss: 0.1540 - lr: 4.0000e-04 - 765ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0470 - val_loss: 0.1535 - lr: 4.0000e-04 - 789ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.1530 - lr: 4.0000e-04 - 793ms/epoch - 32ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1528 - lr: 4.0000e-04 - 768ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.1524 - lr: 4.0000e-04 - 782ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1521 - lr: 4.0000e-04 - 784ms/epoch - 31ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1519 - lr: 4.0000e-04 - 795ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0468 - val_loss: 0.1515 - lr: 4.0000e-04 - 791ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1515 - lr: 4.0000e-04 - 749ms/epoch - 30ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.1514 - lr: 4.0000e-04 - 724ms/epoch - 29ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1515 - lr: 8.0000e-05 - 768ms/epoch - 31ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1515 - lr: 8.0000e-05 - 754ms/epoch - 30ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1514 - lr: 8.0000e-05 - 797ms/epoch - 32ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 8.0000e-05 - 794ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1514 - lr: 1.6000e-05 - 800ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0458 - val_loss: 0.1514 - lr: 1.6000e-05 - 811ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0458 - val_loss: 0.1514 - lr: 1.6000e-05 - 784ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 1.6000e-05 - 754ms/epoch - 30ms/step\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1514 - lr: 1.6000e-05 - 769ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0458 - val_loss: 0.1514 - lr: 3.2000e-06 - 747ms/epoch - 30ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 3.2000e-06 - 779ms/epoch - 31ms/step\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 3.2000e-06 - 782ms/epoch - 31ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1514 - lr: 6.4000e-07 - 797ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0458 - val_loss: 0.1514 - lr: 6.4000e-07 - 783ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.1514 - lr: 6.4000e-07 - 761ms/epoch - 30ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1514 - lr: 1.2800e-07 - 738ms/epoch - 30ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1514 - lr: 1.2800e-07 - 782ms/epoch - 31ms/step\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 1.2800e-07 - 780ms/epoch - 31ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1514 - lr: 2.5600e-08 - 734ms/epoch - 29ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0457 - val_loss: 0.1514 - lr: 2.5600e-08 - 770ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 2.5600e-08 - 804ms/epoch - 32ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.1514 - lr: 2.5600e-08 - 778ms/epoch - 31ms/step\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 2.5600e-08 - 783ms/epoch - 31ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1514 - lr: 5.1200e-09 - 750ms/epoch - 30ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 5.1200e-09 - 794ms/epoch - 32ms/step\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1514 - lr: 5.1200e-09 - 806ms/epoch - 32ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 1.0240e-09 - 794ms/epoch - 32ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 1.0240e-09 - 795ms/epoch - 32ms/step\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 1.0240e-09 - 807ms/epoch - 32ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0458 - val_loss: 0.1514 - lr: 2.0480e-10 - 788ms/epoch - 32ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.1514 - lr: 2.0480e-10 - 777ms/epoch - 31ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1514 - lr: 2.0480e-10 - 740ms/epoch - 30ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 4.0960e-11 - 757ms/epoch - 30ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 4.0960e-11 - 753ms/epoch - 30ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 4.0960e-11 - 766ms/epoch - 31ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1514 - lr: 8.1920e-12 - 810ms/epoch - 32ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 8.1920e-12 - 794ms/epoch - 32ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1514 - lr: 8.1920e-12 - 773ms/epoch - 31ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0457 - val_loss: 0.1514 - lr: 1.6384e-12 - 774ms/epoch - 31ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.1514 - lr: 1.6384e-12 - 777ms/epoch - 31ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 1.6384e-12 - 773ms/epoch - 31ms/step\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.1514 - lr: 1.6384e-12 - 789ms/epoch - 32ms/step\n",
      "Epoch 73/500\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.1514 - lr: 3.2768e-13 - 812ms/epoch - 32ms/step\n",
      "Epoch 73: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.2min\n",
      "7/7 - 2s - 2s/epoch - 325ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.1910 - val_loss: 1.8698 - lr: 0.0100 - 16s/epoch - 642ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1577 - val_loss: 1.5478 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1170 - val_loss: 0.9752 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0817 - val_loss: 0.5558 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.4266 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.3899 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.3722 - lr: 0.0100 - 756ms/epoch - 30ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.3612 - lr: 0.0100 - 740ms/epoch - 30ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0580 - val_loss: 0.3500 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.3427 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.3312 - lr: 0.0100 - 761ms/epoch - 30ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.3241 - lr: 0.0100 - 753ms/epoch - 30ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.3176 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.3119 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3041 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.2983 - lr: 0.0100 - 747ms/epoch - 30ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.2940 - lr: 0.0100 - 771ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.2862 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.2835 - lr: 0.0100 - 751ms/epoch - 30ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.2757 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.2707 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.2659 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.2622 - lr: 0.0100 - 748ms/epoch - 30ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.2575 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.2533 - lr: 0.0100 - 764ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.2483 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.2434 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.2408 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.2373 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.2338 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.2295 - lr: 0.0100 - 756ms/epoch - 30ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.2274 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0481 - val_loss: 0.2230 - lr: 0.0100 - 739ms/epoch - 30ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.2205 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.2171 - lr: 0.0100 - 767ms/epoch - 31ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.2133 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0466 - val_loss: 0.2102 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0461 - val_loss: 0.2078 - lr: 0.0100 - 746ms/epoch - 30ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0456 - val_loss: 0.2053 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0458 - val_loss: 0.2025 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0455 - val_loss: 0.1997 - lr: 0.0100 - 755ms/epoch - 30ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0451 - val_loss: 0.1968 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.1948 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.1914 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0443 - val_loss: 0.1882 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0437 - val_loss: 0.1854 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0435 - val_loss: 0.1844 - lr: 0.0100 - 746ms/epoch - 30ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0432 - val_loss: 0.1811 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0427 - val_loss: 0.1792 - lr: 0.0100 - 756ms/epoch - 30ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0423 - val_loss: 0.1775 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0421 - val_loss: 0.1765 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0417 - val_loss: 0.1733 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0417 - val_loss: 0.1701 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0414 - val_loss: 0.1685 - lr: 0.0100 - 746ms/epoch - 30ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0409 - val_loss: 0.1673 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0406 - val_loss: 0.1645 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1634 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1610 - lr: 0.0100 - 786ms/epoch - 31ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1587 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0394 - val_loss: 0.1573 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0389 - val_loss: 0.1565 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0386 - val_loss: 0.1548 - lr: 0.0100 - 757ms/epoch - 30ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0380 - val_loss: 0.1511 - lr: 0.0100 - 747ms/epoch - 30ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0381 - val_loss: 0.1498 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.1479 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0373 - val_loss: 0.1467 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0374 - val_loss: 0.1453 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0370 - val_loss: 0.1435 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0362 - val_loss: 0.1403 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0362 - val_loss: 0.1399 - lr: 0.0100 - 753ms/epoch - 30ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0357 - val_loss: 0.1366 - lr: 0.0100 - 776ms/epoch - 31ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0353 - val_loss: 0.1359 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0351 - val_loss: 0.1327 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0348 - val_loss: 0.1325 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1323 - lr: 0.0100 - 748ms/epoch - 30ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1286 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1271 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1250 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0331 - val_loss: 0.1239 - lr: 0.0100 - 768ms/epoch - 31ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.1217 - lr: 0.0100 - 752ms/epoch - 30ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.1223 - lr: 0.0100 - 771ms/epoch - 31ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.1200 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0321 - val_loss: 0.1167 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0315 - val_loss: 0.1147 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0317 - val_loss: 0.1138 - lr: 0.0100 - 779ms/epoch - 31ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1123 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1122 - lr: 0.0100 - 771ms/epoch - 31ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0307 - val_loss: 0.1102 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0302 - val_loss: 0.1095 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 0.1078 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1072 - lr: 0.0100 - 777ms/epoch - 31ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1046 - lr: 0.0100 - 765ms/epoch - 31ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1033 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.1016 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.1013 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.0992 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 97/500\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.0989 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0286 - val_loss: 0.0981 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.0971 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0282 - val_loss: 0.0973 - lr: 0.0100 - 773ms/epoch - 31ms/step\n",
      "Epoch 101/500\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.0955 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.0946 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 103/500\n",
      "25/25 - 1s - loss: 0.0275 - val_loss: 0.0945 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 104/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.0936 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.0928 - lr: 0.0100 - 796ms/epoch - 32ms/step\n",
      "Epoch 106/500\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.0919 - lr: 0.0100 - 782ms/epoch - 31ms/step\n",
      "Epoch 107/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.0916 - lr: 0.0100 - 754ms/epoch - 30ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.0916 - lr: 0.0100 - 740ms/epoch - 30ms/step\n",
      "Epoch 109/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.0911 - lr: 0.0100 - 751ms/epoch - 30ms/step\n",
      "Epoch 110/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.0903 - lr: 0.0100 - 788ms/epoch - 32ms/step\n",
      "Epoch 111/500\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.0898 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 112/500\n",
      "25/25 - 1s - loss: 0.0265 - val_loss: 0.0895 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 113/500\n",
      "25/25 - 1s - loss: 0.0265 - val_loss: 0.0891 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 114/500\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.0888 - lr: 0.0100 - 763ms/epoch - 31ms/step\n",
      "Epoch 115/500\n",
      "25/25 - 1s - loss: 0.0259 - val_loss: 0.0886 - lr: 0.0100 - 747ms/epoch - 30ms/step\n",
      "Epoch 116/500\n",
      "25/25 - 1s - loss: 0.0263 - val_loss: 0.0883 - lr: 0.0100 - 769ms/epoch - 31ms/step\n",
      "Epoch 117/500\n",
      "25/25 - 1s - loss: 0.0264 - val_loss: 0.0880 - lr: 0.0100 - 784ms/epoch - 31ms/step\n",
      "Epoch 118/500\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.0877 - lr: 0.0100 - 770ms/epoch - 31ms/step\n",
      "Epoch 119/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.0944 - lr: 0.0020 - 796ms/epoch - 32ms/step\n",
      "Epoch 120/500\n",
      "25/25 - 1s - loss: 0.0263 - val_loss: 0.0944 - lr: 0.0020 - 793ms/epoch - 32ms/step\n",
      "Epoch 121/500\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.0935 - lr: 0.0020 - 767ms/epoch - 31ms/step\n",
      "Epoch 122/500\n",
      "25/25 - 1s - loss: 0.0270 - val_loss: 0.0958 - lr: 4.0000e-04 - 782ms/epoch - 31ms/step\n",
      "Epoch 123/500\n",
      "25/25 - 1s - loss: 0.0264 - val_loss: 0.0974 - lr: 4.0000e-04 - 757ms/epoch - 30ms/step\n",
      "Epoch 124/500\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0264 - val_loss: 0.0981 - lr: 4.0000e-04 - 751ms/epoch - 30ms/step\n",
      "Epoch 125/500\n",
      "25/25 - 1s - loss: 0.0260 - val_loss: 0.0983 - lr: 8.0000e-05 - 745ms/epoch - 30ms/step\n",
      "Epoch 126/500\n",
      "25/25 - 1s - loss: 0.0256 - val_loss: 0.0986 - lr: 8.0000e-05 - 774ms/epoch - 31ms/step\n",
      "Epoch 127/500\n",
      "25/25 - 1s - loss: 0.0257 - val_loss: 0.0987 - lr: 8.0000e-05 - 790ms/epoch - 32ms/step\n",
      "Epoch 128/500\n",
      "25/25 - 1s - loss: 0.0257 - val_loss: 0.0989 - lr: 8.0000e-05 - 780ms/epoch - 31ms/step\n",
      "Epoch 129/500\n",
      "25/25 - 1s - loss: 0.0254 - val_loss: 0.0991 - lr: 8.0000e-05 - 768ms/epoch - 31ms/step\n",
      "Epoch 130/500\n",
      "25/25 - 1s - loss: 0.0253 - val_loss: 0.0992 - lr: 8.0000e-05 - 760ms/epoch - 30ms/step\n",
      "Epoch 131/500\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.0994 - lr: 8.0000e-05 - 795ms/epoch - 32ms/step\n",
      "Epoch 132/500\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.0995 - lr: 8.0000e-05 - 763ms/epoch - 31ms/step\n",
      "Epoch 133/500\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0257 - val_loss: 0.0996 - lr: 8.0000e-05 - 746ms/epoch - 30ms/step\n",
      "Epoch 134/500\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.0996 - lr: 1.6000e-05 - 761ms/epoch - 30ms/step\n",
      "Epoch 135/500\n",
      "25/25 - 1s - loss: 0.0258 - val_loss: 0.0996 - lr: 1.6000e-05 - 808ms/epoch - 32ms/step\n",
      "Epoch 136/500\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0257 - val_loss: 0.0996 - lr: 1.6000e-05 - 761ms/epoch - 30ms/step\n",
      "Epoch 137/500\n",
      "25/25 - 1s - loss: 0.0258 - val_loss: 0.0997 - lr: 3.2000e-06 - 771ms/epoch - 31ms/step\n",
      "Epoch 138/500\n",
      "Restoring model weights from the end of the best epoch: 118.\n",
      "25/25 - 1s - loss: 0.0259 - val_loss: 0.0997 - lr: 3.2000e-06 - 784ms/epoch - 31ms/step\n",
      "Epoch 138: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 2.1min\n",
      "7/7 - 2s - 2s/epoch - 322ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 2.1min\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.4850 - val_loss: 0.3111 - lr: 0.0100 - 17s/epoch - 669ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5380 - val_loss: 0.2500 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6280 - val_loss: 0.3343 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.5813 - val_loss: 0.2794 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6652 - val_loss: 0.2533 - lr: 0.0020 - 796ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2590 - val_loss: 0.2474 - lr: 0.0020 - 794ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1996 - val_loss: 0.2462 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1872 - val_loss: 0.2464 - lr: 0.0020 - 773ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1848 - val_loss: 0.2471 - lr: 0.0020 - 803ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1816 - val_loss: 0.2476 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1782 - val_loss: 0.2476 - lr: 0.0020 - 784ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1743 - val_loss: 0.2469 - lr: 0.0020 - 774ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1690 - val_loss: 0.2454 - lr: 0.0020 - 789ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1605 - val_loss: 0.2392 - lr: 0.0020 - 792ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1457 - val_loss: 0.2302 - lr: 0.0020 - 807ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1474 - val_loss: 0.2399 - lr: 0.0020 - 837ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1215 - val_loss: 0.2122 - lr: 0.0020 - 789ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2092 - lr: 0.0020 - 768ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0843 - val_loss: 0.2024 - lr: 0.0020 - 776ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0857 - val_loss: 0.1604 - lr: 0.0020 - 760ms/epoch - 30ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1495 - val_loss: 0.2413 - lr: 0.0020 - 763ms/epoch - 31ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.1149 - val_loss: 0.2627 - lr: 0.0020 - 770ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.1657 - val_loss: 0.2526 - lr: 4.0000e-04 - 755ms/epoch - 30ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0879 - val_loss: 0.2485 - lr: 4.0000e-04 - 807ms/epoch - 32ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0773 - val_loss: 0.2466 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0721 - val_loss: 0.2456 - lr: 4.0000e-04 - 746ms/epoch - 30ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0677 - val_loss: 0.2452 - lr: 4.0000e-04 - 780ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.2447 - lr: 4.0000e-04 - 808ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.2439 - lr: 4.0000e-04 - 804ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.2420 - lr: 4.0000e-04 - 784ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.2383 - lr: 4.0000e-04 - 800ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.2331 - lr: 4.0000e-04 - 815ms/epoch - 33ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.2281 - lr: 4.0000e-04 - 765ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.2251 - lr: 4.0000e-04 - 792ms/epoch - 32ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0457 - val_loss: 0.2234 - lr: 4.0000e-04 - 791ms/epoch - 32ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0447 - val_loss: 0.2213 - lr: 4.0000e-04 - 764ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0439 - val_loss: 0.2190 - lr: 4.0000e-04 - 772ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0428 - val_loss: 0.2171 - lr: 4.0000e-04 - 803ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0420 - val_loss: 0.2156 - lr: 4.0000e-04 - 808ms/epoch - 32ms/step\n",
      "Epoch 40/500\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "25/25 - 1s - loss: 0.0412 - val_loss: 0.2142 - lr: 4.0000e-04 - 838ms/epoch - 34ms/step\n",
      "Epoch 40: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  49.4s\n",
      "7/7 - 2s - 2s/epoch - 331ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  51.7s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.5465 - val_loss: 0.3490 - lr: 0.0100 - 16s/epoch - 638ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4428 - val_loss: 0.7786 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5981 - val_loss: 0.2453 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3544 - val_loss: 0.2410 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4133 - val_loss: 0.3885 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2529 - val_loss: 0.2725 - lr: 0.0100 - 783ms/epoch - 31ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2185 - val_loss: 0.2474 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2490 - val_loss: 0.2949 - lr: 0.0100 - 800ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1956 - val_loss: 0.2643 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2144 - val_loss: 0.2576 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2145 - val_loss: 0.2672 - lr: 0.0100 - 815ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1721 - val_loss: 0.2676 - lr: 0.0100 - 779ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2401 - val_loss: 0.2617 - lr: 0.0100 - 766ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1875 - val_loss: 0.2589 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1942 - val_loss: 0.2465 - lr: 0.0100 - 745ms/epoch - 30ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1174 - val_loss: 0.2438 - lr: 0.0020 - 780ms/epoch - 31ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0922 - val_loss: 0.2306 - lr: 0.0020 - 795ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0728 - val_loss: 0.2230 - lr: 0.0020 - 830ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0644 - val_loss: 0.2170 - lr: 0.0020 - 787ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.2124 - lr: 0.0020 - 791ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.2086 - lr: 0.0020 - 802ms/epoch - 32ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0416 - val_loss: 0.2038 - lr: 0.0020 - 788ms/epoch - 32ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0470 - val_loss: 0.1995 - lr: 0.0020 - 784ms/epoch - 31ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0363 - val_loss: 0.2030 - lr: 0.0020 - 786ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1908 - lr: 0.0020 - 805ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0404 - val_loss: 0.2009 - lr: 0.0020 - 798ms/epoch - 32ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0726 - val_loss: 0.1897 - lr: 0.0020 - 783ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.1825 - lr: 4.0000e-04 - 811ms/epoch - 32ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0323 - val_loss: 0.1923 - lr: 4.0000e-04 - 799ms/epoch - 32ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1962 - lr: 4.0000e-04 - 772ms/epoch - 31ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1963 - lr: 4.0000e-04 - 806ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0294 - val_loss: 0.1952 - lr: 4.0000e-04 - 794ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0286 - val_loss: 0.1942 - lr: 4.0000e-04 - 829ms/epoch - 33ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0280 - val_loss: 0.1935 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0278 - val_loss: 0.1930 - lr: 4.0000e-04 - 760ms/epoch - 30ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1922 - lr: 4.0000e-04 - 774ms/epoch - 31ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1912 - lr: 4.0000e-04 - 776ms/epoch - 31ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1909 - lr: 4.0000e-04 - 790ms/epoch - 32ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0265 - val_loss: 0.1903 - lr: 4.0000e-04 - 784ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0266 - val_loss: 0.1895 - lr: 4.0000e-04 - 816ms/epoch - 33ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0256 - val_loss: 0.1891 - lr: 4.0000e-04 - 789ms/epoch - 32ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0256 - val_loss: 0.1882 - lr: 4.0000e-04 - 776ms/epoch - 31ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0251 - val_loss: 0.1878 - lr: 4.0000e-04 - 789ms/epoch - 32ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0250 - val_loss: 0.1875 - lr: 4.0000e-04 - 777ms/epoch - 31ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0244 - val_loss: 0.1866 - lr: 4.0000e-04 - 808ms/epoch - 32ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0253 - val_loss: 0.1865 - lr: 4.0000e-04 - 787ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0244 - val_loss: 0.1858 - lr: 4.0000e-04 - 793ms/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0246 - val_loss: 0.1846 - lr: 4.0000e-04 - 824ms/epoch - 33ms/step\n",
      "Epoch 48: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  55.8s\n",
      "7/7 - 2s - 2s/epoch - 328ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  58.1s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.4250 - val_loss: 0.3477 - lr: 0.0100 - 16s/epoch - 659ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5871 - val_loss: 0.4974 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.3668 - val_loss: 0.9713 - lr: 0.0100 - 792ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9777 - val_loss: 1.8528 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9449 - val_loss: 1.9163 - lr: 0.0020 - 782ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9270 - val_loss: 2.0706 - lr: 0.0020 - 790ms/epoch - 32ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.9015 - val_loss: 2.1622 - lr: 0.0020 - 810ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8571 - val_loss: 2.1729 - lr: 4.0000e-04 - 767ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8565 - val_loss: 2.1830 - lr: 4.0000e-04 - 804ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8560 - val_loss: 2.1923 - lr: 4.0000e-04 - 781ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8491 - val_loss: 2.1941 - lr: 8.0000e-05 - 811ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8489 - val_loss: 2.1958 - lr: 8.0000e-05 - 771ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8489 - val_loss: 2.1976 - lr: 8.0000e-05 - 777ms/epoch - 31ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8474 - val_loss: 2.1980 - lr: 1.6000e-05 - 812ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8475 - val_loss: 2.1983 - lr: 1.6000e-05 - 813ms/epoch - 33ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8475 - val_loss: 2.1987 - lr: 1.6000e-05 - 802ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8473 - val_loss: 2.1988 - lr: 3.2000e-06 - 772ms/epoch - 31ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8471 - val_loss: 2.1989 - lr: 3.2000e-06 - 807ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8470 - val_loss: 2.1989 - lr: 3.2000e-06 - 803ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8472 - val_loss: 2.1990 - lr: 6.4000e-07 - 772ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8472 - val_loss: 2.1990 - lr: 6.4000e-07 - 794ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.9s\n",
      "7/7 - 2s - 2s/epoch - 323ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  37.1s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.3587 - val_loss: 0.4594 - lr: 0.0100 - 17s/epoch - 672ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.4800 - val_loss: 1.6353 - lr: 0.0100 - 835ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.3464 - val_loss: 2.3426 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9914 - val_loss: 2.6130 - lr: 0.0100 - 775ms/epoch - 31ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8527 - val_loss: 2.5864 - lr: 0.0020 - 784ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8520 - val_loss: 2.5684 - lr: 0.0020 - 809ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8485 - val_loss: 2.5447 - lr: 0.0020 - 791ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8303 - val_loss: 2.5382 - lr: 4.0000e-04 - 783ms/epoch - 31ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8282 - val_loss: 2.5311 - lr: 4.0000e-04 - 808ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8267 - val_loss: 2.5230 - lr: 4.0000e-04 - 809ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8223 - val_loss: 2.5211 - lr: 8.0000e-05 - 771ms/epoch - 31ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8219 - val_loss: 2.5193 - lr: 8.0000e-05 - 794ms/epoch - 32ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8212 - val_loss: 2.5173 - lr: 8.0000e-05 - 796ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8204 - val_loss: 2.5169 - lr: 1.6000e-05 - 777ms/epoch - 31ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8201 - val_loss: 2.5164 - lr: 1.6000e-05 - 804ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8198 - val_loss: 2.5160 - lr: 1.6000e-05 - 799ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8198 - val_loss: 2.5159 - lr: 3.2000e-06 - 846ms/epoch - 34ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8200 - val_loss: 2.5159 - lr: 3.2000e-06 - 780ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8196 - val_loss: 2.5158 - lr: 3.2000e-06 - 806ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8196 - val_loss: 2.5157 - lr: 6.4000e-07 - 800ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8196 - val_loss: 2.5157 - lr: 6.4000e-07 - 809ms/epoch - 32ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  34.7s\n",
      "7/7 - 2s - 2s/epoch - 330ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  37.0s\n",
      "Epoch 1/500\n",
      "25/25 - 16s - loss: 0.5953 - val_loss: 0.2452 - lr: 0.0100 - 16s/epoch - 653ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4640 - val_loss: 2.1190 - lr: 0.0100 - 873ms/epoch - 35ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2752 - val_loss: 0.7324 - lr: 0.0100 - 785ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2073 - val_loss: 0.6648 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2021 - val_loss: 1.1323 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1948 - val_loss: 0.5366 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1925 - val_loss: 1.1294 - lr: 0.0100 - 869ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1684 - val_loss: 0.7704 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1292 - val_loss: 1.1335 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1254 - val_loss: 2.0961 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0932 - val_loss: 1.5353 - lr: 0.0100 - 793ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1950 - val_loss: 1.0292 - lr: 0.0100 - 780ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1454 - val_loss: 0.8316 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1323 - val_loss: 1.3253 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1192 - val_loss: 1.2091 - lr: 0.0020 - 828ms/epoch - 33ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1096 - val_loss: 1.1497 - lr: 0.0020 - 803ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1069 - val_loss: 1.0950 - lr: 0.0020 - 805ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1033 - val_loss: 1.0896 - lr: 4.0000e-04 - 770ms/epoch - 31ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1032 - val_loss: 1.0856 - lr: 4.0000e-04 - 780ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1036 - val_loss: 1.0821 - lr: 4.0000e-04 - 800ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.1027 - val_loss: 1.0814 - lr: 8.0000e-05 - 827ms/epoch - 33ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.0s\n",
      "7/7 - 2s - 2s/epoch - 343ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  37.4s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.9153 - val_loss: 0.4102 - lr: 0.0100 - 18s/epoch - 737ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5027 - val_loss: 0.4216 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2630 - val_loss: 0.3563 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6431 - val_loss: 0.3942 - lr: 0.0100 - 1s/epoch - 41ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 1.0578 - val_loss: 1.1534 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.8315 - val_loss: 1.4555 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8188 - val_loss: 1.5372 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8021 - val_loss: 1.6068 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7898 - val_loss: 1.6628 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7758 - val_loss: 1.6736 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7742 - val_loss: 1.6840 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7728 - val_loss: 1.6940 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7702 - val_loss: 1.6960 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7700 - val_loss: 1.6980 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7697 - val_loss: 1.7000 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7692 - val_loss: 1.7004 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7692 - val_loss: 1.7008 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7691 - val_loss: 1.7012 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7690 - val_loss: 1.7013 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7690 - val_loss: 1.7014 - lr: 3.2000e-06 - 1s/epoch - 43ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7690 - val_loss: 1.7014 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.7690 - val_loss: 1.7015 - lr: 6.4000e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "25/25 - 1s - loss: 0.7690 - val_loss: 1.7015 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.3s\n",
      "7/7 - 2s - 2s/epoch - 340ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  45.7s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 1.1116 - val_loss: 0.6541 - lr: 0.0100 - 18s/epoch - 727ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6474 - val_loss: 0.3474 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3450 - val_loss: 0.3323 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4587 - val_loss: 0.3475 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2680 - val_loss: 0.3365 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3274 - val_loss: 0.2873 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.3285 - val_loss: 0.2527 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.3195 - val_loss: 0.2467 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3489 - val_loss: 0.2736 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2828 - val_loss: 0.3183 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2660 - val_loss: 0.3364 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2612 - val_loss: 0.3402 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2611 - val_loss: 0.3406 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2613 - val_loss: 0.3402 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2609 - val_loss: 0.3389 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2612 - val_loss: 0.3380 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2616 - val_loss: 0.3376 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2618 - val_loss: 0.3374 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2536 - val_loss: 0.3425 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2517 - val_loss: 0.3487 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.2502 - val_loss: 0.3540 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.2491 - val_loss: 0.3585 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.2484 - val_loss: 0.3621 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.2479 - val_loss: 0.3651 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.2475 - val_loss: 0.3675 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.2473 - val_loss: 0.3694 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.2471 - val_loss: 0.3710 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 28/500\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "25/25 - 1s - loss: 0.2470 - val_loss: 0.3723 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 28: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  49.0s\n",
      "7/7 - 2s - 2s/epoch - 327ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  51.3s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.7569 - val_loss: 0.3563 - lr: 0.0100 - 18s/epoch - 720ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4275 - val_loss: 0.3012 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7652 - val_loss: 0.2505 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 1.0238 - val_loss: 0.5406 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.1403 - val_loss: 1.0200 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.1477 - val_loss: 1.3541 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 1.0118 - val_loss: 1.6370 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.9410 - val_loss: 1.8162 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8930 - val_loss: 1.8538 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8873 - val_loss: 1.8908 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8820 - val_loss: 1.9252 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8728 - val_loss: 1.9323 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8719 - val_loss: 1.9395 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8711 - val_loss: 1.9466 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8691 - val_loss: 1.9480 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8689 - val_loss: 1.9495 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8688 - val_loss: 1.9509 - lr: 1.6000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8684 - val_loss: 1.9512 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8683 - val_loss: 1.9515 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8683 - val_loss: 1.9519 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.8682 - val_loss: 1.9519 - lr: 6.4000e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.8682 - val_loss: 1.9520 - lr: 6.4000e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.8682 - val_loss: 1.9520 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.5s\n",
      "7/7 - 2s - 2s/epoch - 350ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  45.9s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.6922 - val_loss: 0.3774 - lr: 0.0100 - 18s/epoch - 731ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5698 - val_loss: 0.3232 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5714 - val_loss: 0.3623 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6813 - val_loss: 0.3583 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.6734 - val_loss: 0.5070 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.7793 - val_loss: 1.4685 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 1.0118 - val_loss: 1.9913 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.9258 - val_loss: 2.1275 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8915 - val_loss: 2.1798 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8849 - val_loss: 2.2341 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8785 - val_loss: 2.2809 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8671 - val_loss: 2.2908 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8662 - val_loss: 2.3007 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8652 - val_loss: 2.3104 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8629 - val_loss: 2.3123 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8627 - val_loss: 2.3143 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8624 - val_loss: 2.3163 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8620 - val_loss: 2.3167 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8620 - val_loss: 2.3171 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8619 - val_loss: 2.3175 - lr: 3.2000e-06 - 1s/epoch - 44ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.8618 - val_loss: 2.3176 - lr: 6.4000e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "25/25 - 1s - loss: 0.8618 - val_loss: 2.3177 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  42.1s\n",
      "7/7 - 2s - 2s/epoch - 324ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  44.3s\n",
      "Epoch 1/500\n",
      "25/25 - 18s - loss: 0.4756 - val_loss: 0.6513 - lr: 0.0100 - 18s/epoch - 721ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1565 - val_loss: 2.3574 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3015 - val_loss: 1.8311 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2463 - val_loss: 1.7735 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2466 - val_loss: 1.6962 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3104 - val_loss: 1.7119 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2441 - val_loss: 1.8094 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2185 - val_loss: 1.9457 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2105 - val_loss: 1.9957 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2081 - val_loss: 2.0592 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.2059 - val_loss: 2.1360 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2027 - val_loss: 2.1544 - lr: 8.0000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2023 - val_loss: 2.1752 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 2.1982 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2012 - val_loss: 2.2031 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2011 - val_loss: 2.2084 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.2010 - val_loss: 2.2138 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2008 - val_loss: 2.2149 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2008 - val_loss: 2.2161 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.2008 - val_loss: 2.2173 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2008 - val_loss: 2.2176 - lr: 6.4000e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.6s\n",
      "7/7 - 2s - 2s/epoch - 353ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  44.0s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.7636 - val_loss: 1.8311 - lr: 0.0100 - 17s/epoch - 683ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7560 - val_loss: 1.8452 - lr: 0.0100 - 839ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7546 - val_loss: 1.8537 - lr: 0.0100 - 801ms/epoch - 32ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7538 - val_loss: 1.8600 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7532 - val_loss: 1.8652 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7528 - val_loss: 1.8695 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7524 - val_loss: 1.8733 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7521 - val_loss: 1.8768 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7519 - val_loss: 1.8799 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7516 - val_loss: 1.8827 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7514 - val_loss: 1.8853 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7512 - val_loss: 1.8878 - lr: 0.0100 - 787ms/epoch - 31ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7511 - val_loss: 1.8901 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7509 - val_loss: 1.8923 - lr: 0.0100 - 794ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7507 - val_loss: 1.8944 - lr: 0.0100 - 774ms/epoch - 31ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7506 - val_loss: 1.8963 - lr: 0.0100 - 799ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7505 - val_loss: 1.8982 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 18/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.7503 - val_loss: 1.9000 - lr: 0.0100 - 802ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7502 - val_loss: 1.9017 - lr: 0.0100 - 809ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7501 - val_loss: 1.9034 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7500 - val_loss: 1.9050 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.1s\n",
      "7/7 - 2s - 2s/epoch - 328ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.4s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.7778 - val_loss: 1.8303 - lr: 0.0100 - 17s/epoch - 683ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7709 - val_loss: 1.8455 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7693 - val_loss: 1.8549 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7684 - val_loss: 1.8619 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7677 - val_loss: 1.8677 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7672 - val_loss: 1.8726 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7668 - val_loss: 1.8769 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7664 - val_loss: 1.8808 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7661 - val_loss: 1.8843 - lr: 0.0100 - 778ms/epoch - 31ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7658 - val_loss: 1.8875 - lr: 0.0100 - 871ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7656 - val_loss: 1.8906 - lr: 0.0100 - 816ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7653 - val_loss: 1.8934 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7651 - val_loss: 1.8961 - lr: 0.0100 - 806ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7649 - val_loss: 1.8986 - lr: 0.0100 - 814ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7647 - val_loss: 1.9010 - lr: 0.0100 - 808ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7645 - val_loss: 1.9032 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7644 - val_loss: 1.9054 - lr: 0.0100 - 874ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7642 - val_loss: 1.9075 - lr: 0.0100 - 870ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7641 - val_loss: 1.9095 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7639 - val_loss: 1.9114 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7638 - val_loss: 1.9133 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.4s\n",
      "7/7 - 2s - 2s/epoch - 328ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.6s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.8935 - val_loss: 1.8498 - lr: 0.0100 - 17s/epoch - 684ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8824 - val_loss: 1.8737 - lr: 0.0100 - 847ms/epoch - 34ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8789 - val_loss: 1.8896 - lr: 0.0100 - 772ms/epoch - 31ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8766 - val_loss: 1.9021 - lr: 0.0100 - 819ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8748 - val_loss: 1.9127 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8734 - val_loss: 1.9219 - lr: 0.0100 - 789ms/epoch - 32ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8722 - val_loss: 1.9302 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8711 - val_loss: 1.9378 - lr: 0.0100 - 856ms/epoch - 34ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8701 - val_loss: 1.9449 - lr: 0.0100 - 813ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8693 - val_loss: 1.9516 - lr: 0.0100 - 843ms/epoch - 34ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8684 - val_loss: 1.9580 - lr: 0.0100 - 798ms/epoch - 32ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8677 - val_loss: 1.9641 - lr: 0.0100 - 831ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8669 - val_loss: 1.9701 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8663 - val_loss: 1.9759 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8656 - val_loss: 1.9815 - lr: 0.0100 - 840ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8650 - val_loss: 1.9871 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8644 - val_loss: 1.9926 - lr: 0.0100 - 807ms/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8638 - val_loss: 1.9981 - lr: 0.0100 - 812ms/epoch - 32ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8632 - val_loss: 2.0035 - lr: 0.0100 - 795ms/epoch - 32ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8626 - val_loss: 2.0090 - lr: 0.0100 - 805ms/epoch - 32ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8621 - val_loss: 2.0144 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.4s\n",
      "7/7 - 2s - 2s/epoch - 342ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  37.7s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.9484 - val_loss: 1.8697 - lr: 0.0100 - 17s/epoch - 691ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9316 - val_loss: 1.9048 - lr: 0.0100 - 836ms/epoch - 33ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9244 - val_loss: 1.9295 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.9195 - val_loss: 1.9495 - lr: 0.0100 - 866ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9155 - val_loss: 1.9669 - lr: 0.0100 - 810ms/epoch - 32ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9121 - val_loss: 1.9826 - lr: 0.0100 - 824ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.9091 - val_loss: 1.9974 - lr: 0.0100 - 803ms/epoch - 32ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.9063 - val_loss: 2.0115 - lr: 0.0100 - 791ms/epoch - 32ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9037 - val_loss: 2.0253 - lr: 0.0100 - 817ms/epoch - 33ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.9012 - val_loss: 2.0392 - lr: 0.0100 - 804ms/epoch - 32ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8988 - val_loss: 2.0532 - lr: 0.0100 - 826ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8964 - val_loss: 2.0676 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8940 - val_loss: 2.0825 - lr: 0.0100 - 811ms/epoch - 32ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8916 - val_loss: 2.0980 - lr: 0.0100 - 797ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8891 - val_loss: 2.1141 - lr: 0.0100 - 820ms/epoch - 33ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8867 - val_loss: 2.1309 - lr: 0.0100 - 829ms/epoch - 33ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8842 - val_loss: 2.1483 - lr: 0.0100 - 827ms/epoch - 33ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8818 - val_loss: 2.1661 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8794 - val_loss: 2.1844 - lr: 0.0100 - 862ms/epoch - 34ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8771 - val_loss: 2.2028 - lr: 0.0100 - 818ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8748 - val_loss: 2.2212 - lr: 0.0100 - 841ms/epoch - 34ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.7s\n",
      "7/7 - 3s - 3s/epoch - 386ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  38.4s\n",
      "Epoch 1/500\n",
      "25/25 - 17s - loss: 0.5978 - val_loss: 1.9235 - lr: 0.0100 - 17s/epoch - 672ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5467 - val_loss: 2.0080 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5139 - val_loss: 2.0764 - lr: 0.0100 - 824ms/epoch - 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4871 - val_loss: 2.1403 - lr: 0.0100 - 823ms/epoch - 33ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4623 - val_loss: 2.2078 - lr: 0.0100 - 821ms/epoch - 33ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4363 - val_loss: 2.2872 - lr: 0.0100 - 830ms/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.4069 - val_loss: 2.3882 - lr: 0.0100 - 834ms/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3725 - val_loss: 2.5188 - lr: 0.0100 - 854ms/epoch - 34ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3345 - val_loss: 2.6800 - lr: 0.0100 - 850ms/epoch - 34ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2966 - val_loss: 2.8610 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2641 - val_loss: 3.0407 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2401 - val_loss: 3.1982 - lr: 0.0100 - 846ms/epoch - 34ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2246 - val_loss: 3.3219 - lr: 0.0100 - 822ms/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2153 - val_loss: 3.4118 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2100 - val_loss: 3.4743 - lr: 0.0100 - 854ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2070 - val_loss: 3.5170 - lr: 0.0100 - 854ms/epoch - 34ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2052 - val_loss: 3.5463 - lr: 0.0100 - 849ms/epoch - 34ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2041 - val_loss: 3.5668 - lr: 0.0100 - 828ms/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2034 - val_loss: 3.5816 - lr: 0.0100 - 832ms/epoch - 33ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2029 - val_loss: 3.5925 - lr: 0.0100 - 837ms/epoch - 33ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2026 - val_loss: 3.6009 - lr: 0.0100 - 851ms/epoch - 34ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  35.6s\n",
      "7/7 - 3s - 3s/epoch - 399ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=4, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  38.3s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.6175 - val_loss: 0.8538 - lr: 0.0100 - 19s/epoch - 757ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7214 - val_loss: 0.7119 - lr: 0.0100 - 949ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6136 - val_loss: 0.4198 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3486 - val_loss: 0.2153 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1476 - val_loss: 0.2157 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1004 - val_loss: 0.2107 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0889 - val_loss: 0.2071 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0856 - val_loss: 0.2024 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0842 - val_loss: 0.1980 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0835 - val_loss: 0.1957 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0837 - val_loss: 0.1935 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0834 - val_loss: 0.1906 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0827 - val_loss: 0.1889 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0821 - val_loss: 0.1871 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0840 - val_loss: 0.1859 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0829 - val_loss: 0.1856 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0832 - val_loss: 0.1845 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0899 - val_loss: 0.1743 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0787 - val_loss: 0.1738 - lr: 0.0020 - 911ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0760 - val_loss: 0.1737 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0743 - val_loss: 0.1736 - lr: 0.0020 - 881ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0736 - val_loss: 0.1736 - lr: 0.0020 - 912ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0729 - val_loss: 0.1737 - lr: 0.0020 - 896ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0734 - val_loss: 0.1727 - lr: 0.0020 - 910ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0719 - val_loss: 0.1732 - lr: 0.0020 - 892ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0708 - val_loss: 0.1729 - lr: 0.0020 - 874ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0694 - val_loss: 0.1721 - lr: 0.0020 - 931ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0703 - val_loss: 0.1715 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0696 - val_loss: 0.1719 - lr: 0.0020 - 927ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0700 - val_loss: 0.1723 - lr: 0.0020 - 863ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0682 - val_loss: 0.1716 - lr: 4.0000e-04 - 897ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0672 - val_loss: 0.1713 - lr: 4.0000e-04 - 896ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0663 - val_loss: 0.1712 - lr: 4.0000e-04 - 866ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0663 - val_loss: 0.1711 - lr: 4.0000e-04 - 893ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.1713 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0663 - val_loss: 0.1714 - lr: 4.0000e-04 - 913ms/epoch - 37ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0656 - val_loss: 0.1715 - lr: 4.0000e-04 - 889ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0655 - val_loss: 0.1716 - lr: 4.0000e-04 - 912ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0662 - val_loss: 0.1718 - lr: 4.0000e-04 - 870ms/epoch - 35ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0657 - val_loss: 0.1718 - lr: 4.0000e-04 - 897ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0651 - val_loss: 0.1718 - lr: 4.0000e-04 - 932ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0657 - val_loss: 0.1720 - lr: 4.0000e-04 - 896ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.1719 - lr: 4.0000e-04 - 892ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0649 - val_loss: 0.1720 - lr: 4.0000e-04 - 882ms/epoch - 35ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1722 - lr: 4.0000e-04 - 907ms/epoch - 36ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1718 - lr: 4.0000e-04 - 877ms/epoch - 35ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1719 - lr: 4.0000e-04 - 893ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1718 - lr: 4.0000e-04 - 936ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.1717 - lr: 4.0000e-04 - 914ms/epoch - 37ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1716 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1715 - lr: 4.0000e-04 - 866ms/epoch - 35ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0645 - val_loss: 0.1714 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.1714 - lr: 8.0000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "25/25 - 1s - loss: 0.0642 - val_loss: 0.1713 - lr: 8.0000e-05 - 862ms/epoch - 34ms/step\n",
      "Epoch 54: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 3s - 3s/epoch - 384ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.2min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.5672 - val_loss: 0.6635 - lr: 0.0100 - 20s/epoch - 782ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7133 - val_loss: 0.5431 - lr: 0.0100 - 982ms/epoch - 39ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5878 - val_loss: 0.3036 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3019 - val_loss: 0.2035 - lr: 0.0100 - 869ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1231 - val_loss: 0.2064 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0863 - val_loss: 0.2017 - lr: 0.0100 - 955ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0799 - val_loss: 0.1991 - lr: 0.0100 - 973ms/epoch - 39ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0779 - val_loss: 0.1922 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0767 - val_loss: 0.1893 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0767 - val_loss: 0.1878 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0767 - val_loss: 0.1856 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0760 - val_loss: 0.1845 - lr: 0.0100 - 982ms/epoch - 39ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0761 - val_loss: 0.1835 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0765 - val_loss: 0.1809 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0754 - val_loss: 0.1792 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0763 - val_loss: 0.1782 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0756 - val_loss: 0.1765 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0753 - val_loss: 0.1736 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0745 - val_loss: 0.1750 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0746 - val_loss: 0.1736 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0738 - val_loss: 0.1738 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0739 - val_loss: 0.1722 - lr: 0.0100 - 874ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0745 - val_loss: 0.1721 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0741 - val_loss: 0.1701 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0799 - val_loss: 0.1612 - lr: 0.0020 - 896ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0692 - val_loss: 0.1615 - lr: 0.0020 - 868ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0667 - val_loss: 0.1620 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0653 - val_loss: 0.1634 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0650 - val_loss: 0.1635 - lr: 0.0020 - 876ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0643 - val_loss: 0.1629 - lr: 0.0020 - 954ms/epoch - 38ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.1627 - lr: 0.0020 - 907ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1615 - lr: 0.0020 - 945ms/epoch - 38ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1615 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1617 - lr: 0.0020 - 896ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1612 - lr: 0.0020 - 888ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1610 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1614 - lr: 0.0020 - 879ms/epoch - 35ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1609 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1607 - lr: 0.0020 - 918ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.1608 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1609 - lr: 0.0020 - 889ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1606 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.1597 - lr: 0.0020 - 931ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.1595 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1597 - lr: 0.0020 - 879ms/epoch - 35ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1593 - lr: 0.0020 - 923ms/epoch - 37ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1585 - lr: 0.0020 - 927ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1589 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0576 - val_loss: 0.1592 - lr: 0.0020 - 868ms/epoch - 35ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1590 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1580 - lr: 0.0020 - 915ms/epoch - 37ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.1580 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1573 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1568 - lr: 4.0000e-04 - 948ms/epoch - 38ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1565 - lr: 4.0000e-04 - 883ms/epoch - 35ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1566 - lr: 4.0000e-04 - 869ms/epoch - 35ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1567 - lr: 4.0000e-04 - 880ms/epoch - 35ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1567 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1566 - lr: 4.0000e-04 - 892ms/epoch - 36ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1569 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1569 - lr: 4.0000e-04 - 915ms/epoch - 37ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1569 - lr: 8.0000e-05 - 868ms/epoch - 35ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1569 - lr: 8.0000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1569 - lr: 8.0000e-05 - 886ms/epoch - 35ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1569 - lr: 8.0000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1569 - lr: 8.0000e-05 - 896ms/epoch - 36ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.1569 - lr: 1.6000e-05 - 880ms/epoch - 35ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0546 - val_loss: 0.1569 - lr: 1.6000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1569 - lr: 1.6000e-05 - 879ms/epoch - 35ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1569 - lr: 1.6000e-05 - 872ms/epoch - 35ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1569 - lr: 1.6000e-05 - 872ms/epoch - 35ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1569 - lr: 1.6000e-05 - 868ms/epoch - 35ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1569 - lr: 1.6000e-05 - 873ms/epoch - 35ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1569 - lr: 1.6000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 75/500\n",
      "Restoring model weights from the end of the best epoch: 55.\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1569 - lr: 1.6000e-05 - 896ms/epoch - 36ms/step\n",
      "Epoch 75: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.5min\n",
      "7/7 - 3s - 3s/epoch - 436ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.5min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.6663 - val_loss: 0.9045 - lr: 0.0100 - 20s/epoch - 784ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8106 - val_loss: 0.8203 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7380 - val_loss: 0.6133 - lr: 0.0100 - 920ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5031 - val_loss: 0.2855 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1829 - val_loss: 0.2105 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.2114 - lr: 0.0100 - 865ms/epoch - 35ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0358 - val_loss: 0.2075 - lr: 0.0100 - 859ms/epoch - 34ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0332 - val_loss: 0.2053 - lr: 0.0100 - 869ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0336 - val_loss: 0.2014 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0332 - val_loss: 0.1984 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1957 - lr: 0.0100 - 871ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1911 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0354 - val_loss: 0.1902 - lr: 0.0100 - 872ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.1790 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0429 - val_loss: 0.1775 - lr: 0.0020 - 878ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0413 - val_loss: 0.1768 - lr: 0.0020 - 881ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0425 - val_loss: 0.1763 - lr: 4.0000e-04 - 882ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0411 - val_loss: 0.1764 - lr: 4.0000e-04 - 884ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1767 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1767 - lr: 8.0000e-05 - 863ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1767 - lr: 8.0000e-05 - 896ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1767 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0389 - val_loss: 0.1767 - lr: 1.6000e-05 - 870ms/epoch - 35ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0394 - val_loss: 0.1767 - lr: 1.6000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0390 - val_loss: 0.1768 - lr: 1.6000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0391 - val_loss: 0.1768 - lr: 3.2000e-06 - 898ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0388 - val_loss: 0.1768 - lr: 3.2000e-06 - 863ms/epoch - 35ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0390 - val_loss: 0.1768 - lr: 3.2000e-06 - 851ms/epoch - 34ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0393 - val_loss: 0.1768 - lr: 6.4000e-07 - 858ms/epoch - 34ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0393 - val_loss: 0.1768 - lr: 6.4000e-07 - 886ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0392 - val_loss: 0.1768 - lr: 6.4000e-07 - 877ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0388 - val_loss: 0.1768 - lr: 1.2800e-07 - 882ms/epoch - 35ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0387 - val_loss: 0.1768 - lr: 1.2800e-07 - 880ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0391 - val_loss: 0.1768 - lr: 1.2800e-07 - 857ms/epoch - 34ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0391 - val_loss: 0.1768 - lr: 2.5600e-08 - 876ms/epoch - 35ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0394 - val_loss: 0.1768 - lr: 2.5600e-08 - 886ms/epoch - 35ms/step\n",
      "Epoch 37/500\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1768 - lr: 2.5600e-08 - 889ms/epoch - 36ms/step\n",
      "Epoch 37: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  53.7s\n",
      "7/7 - 3s - 3s/epoch - 390ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  56.4s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.8230 - val_loss: 1.4673 - lr: 0.0100 - 19s/epoch - 757ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8299 - val_loss: 1.3069 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7311 - val_loss: 0.9266 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4370 - val_loss: 0.3695 - lr: 0.0100 - 875ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1414 - val_loss: 0.2178 - lr: 0.0100 - 876ms/epoch - 35ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0674 - val_loss: 0.2100 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.2073 - lr: 0.0100 - 862ms/epoch - 34ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.2026 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.1998 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.1966 - lr: 0.0100 - 883ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1954 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.1937 - lr: 0.0100 - 871ms/epoch - 35ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1934 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0627 - val_loss: 0.1867 - lr: 0.0020 - 868ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1845 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1835 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.1835 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1835 - lr: 4.0000e-04 - 889ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1836 - lr: 4.0000e-04 - 893ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.1836 - lr: 8.0000e-05 - 874ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1836 - lr: 8.0000e-05 - 901ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1836 - lr: 8.0000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1836 - lr: 1.6000e-05 - 872ms/epoch - 35ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1836 - lr: 1.6000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1836 - lr: 1.6000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1836 - lr: 3.2000e-06 - 863ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1836 - lr: 3.2000e-06 - 903ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1836 - lr: 3.2000e-06 - 891ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.1836 - lr: 6.4000e-07 - 916ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.1836 - lr: 6.4000e-07 - 880ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1836 - lr: 6.4000e-07 - 891ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1836 - lr: 1.2800e-07 - 894ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1836 - lr: 1.2800e-07 - 879ms/epoch - 35ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.1836 - lr: 1.2800e-07 - 868ms/epoch - 35ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.1836 - lr: 2.5600e-08 - 883ms/epoch - 35ms/step\n",
      "Epoch 36/500\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1836 - lr: 2.5600e-08 - 899ms/epoch - 36ms/step\n",
      "Epoch 36: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  52.9s\n",
      "7/7 - 3s - 3s/epoch - 374ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  55.5s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.3138 - val_loss: 2.5660 - lr: 0.0100 - 19s/epoch - 771ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2113 - val_loss: 2.5672 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2049 - val_loss: 2.5224 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2010 - val_loss: 2.4615 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1966 - val_loss: 2.3785 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1909 - val_loss: 2.2648 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1837 - val_loss: 2.1120 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1739 - val_loss: 1.9104 - lr: 0.0100 - 864ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1601 - val_loss: 1.6536 - lr: 0.0100 - 879ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1417 - val_loss: 1.3615 - lr: 0.0100 - 879ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1196 - val_loss: 1.0831 - lr: 0.0100 - 883ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0983 - val_loss: 0.8740 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0829 - val_loss: 0.7463 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0749 - val_loss: 0.6794 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0722 - val_loss: 0.6468 - lr: 0.0100 - 859ms/epoch - 34ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0703 - val_loss: 0.6269 - lr: 0.0100 - 868ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0697 - val_loss: 0.6180 - lr: 0.0100 - 875ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0695 - val_loss: 0.6128 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0694 - val_loss: 0.6035 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0691 - val_loss: 0.5968 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0684 - val_loss: 0.5916 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0687 - val_loss: 0.5858 - lr: 0.0100 - 870ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0684 - val_loss: 0.5802 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0683 - val_loss: 0.5748 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0680 - val_loss: 0.5702 - lr: 0.0100 - 874ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0677 - val_loss: 0.5673 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0682 - val_loss: 0.5623 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0680 - val_loss: 0.5595 - lr: 0.0100 - 852ms/epoch - 34ms/step\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0680 - val_loss: 0.5553 - lr: 0.0100 - 883ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0729 - val_loss: 0.5668 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.5608 - lr: 0.0020 - 881ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.5524 - lr: 0.0020 - 885ms/epoch - 35ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.5448 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.5389 - lr: 0.0020 - 879ms/epoch - 35ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.5344 - lr: 0.0020 - 895ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.5304 - lr: 0.0020 - 874ms/epoch - 35ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.5276 - lr: 0.0020 - 919ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.5250 - lr: 0.0020 - 876ms/epoch - 35ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.5232 - lr: 0.0020 - 875ms/epoch - 35ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.5210 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.5208 - lr: 0.0020 - 883ms/epoch - 35ms/step\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.5195 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.5205 - lr: 4.0000e-04 - 851ms/epoch - 34ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.5209 - lr: 4.0000e-04 - 867ms/epoch - 35ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.5212 - lr: 4.0000e-04 - 897ms/epoch - 36ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.5211 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.5209 - lr: 4.0000e-04 - 889ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.5206 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.5202 - lr: 4.0000e-04 - 885ms/epoch - 35ms/step\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.5199 - lr: 4.0000e-04 - 866ms/epoch - 35ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.5199 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.5199 - lr: 8.0000e-05 - 873ms/epoch - 35ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.5198 - lr: 8.0000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.5198 - lr: 8.0000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.5197 - lr: 8.0000e-05 - 859ms/epoch - 34ms/step\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.5197 - lr: 8.0000e-05 - 871ms/epoch - 35ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.5196 - lr: 1.6000e-05 - 920ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.5196 - lr: 1.6000e-05 - 882ms/epoch - 35ms/step\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.5196 - lr: 1.6000e-05 - 894ms/epoch - 36ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.5196 - lr: 3.2000e-06 - 904ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.5196 - lr: 3.2000e-06 - 864ms/epoch - 35ms/step\n",
      "Epoch 62/500\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.5196 - lr: 3.2000e-06 - 915ms/epoch - 37ms/step\n",
      "Epoch 62: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.3min\n",
      "7/7 - 3s - 3s/epoch - 373ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.3min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.9386 - val_loss: 0.3633 - lr: 0.0100 - 20s/epoch - 798ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3667 - val_loss: 0.3634 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4115 - val_loss: 0.4826 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2543 - val_loss: 0.4336 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.5309 - val_loss: 0.4956 - lr: 0.0100 - 997ms/epoch - 40ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.5493 - val_loss: 0.3169 - lr: 0.0100 - 1s/epoch - 41ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.5149 - val_loss: 0.3274 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 1.7001 - val_loss: 0.5985 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8696 - val_loss: 0.6751 - lr: 0.0020 - 996ms/epoch - 40ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7787 - val_loss: 0.3881 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 1.1736 - val_loss: 1.1483 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7424 - val_loss: 0.6455 - lr: 4.0000e-04 - 998ms/epoch - 40ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8152 - val_loss: 0.7070 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 1.0614 - val_loss: 0.9173 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8627 - val_loss: 1.0648 - lr: 8.0000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7520 - val_loss: 1.0324 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7919 - val_loss: 1.0886 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7440 - val_loss: 1.1255 - lr: 1.6000e-05 - 983ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7364 - val_loss: 1.1600 - lr: 1.6000e-05 - 984ms/epoch - 39ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7144 - val_loss: 1.1658 - lr: 3.2000e-06 - 997ms/epoch - 40ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.7195 - val_loss: 1.1723 - lr: 3.2000e-06 - 1s/epoch - 40ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7140 - val_loss: 1.1795 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.7178 - val_loss: 1.1816 - lr: 6.4000e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.7077 - val_loss: 1.1829 - lr: 6.4000e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.7137 - val_loss: 1.1840 - lr: 6.4000e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 26/500\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "25/25 - 1s - loss: 0.7163 - val_loss: 1.1842 - lr: 1.2800e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 26: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  47.6s\n",
      "7/7 - 3s - 3s/epoch - 448ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  50.7s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.9870 - val_loss: 0.3043 - lr: 0.0100 - 20s/epoch - 815ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4113 - val_loss: 0.4749 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4805 - val_loss: 0.3375 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5139 - val_loss: 0.3054 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.5280 - val_loss: 0.2494 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.4897 - val_loss: 0.5237 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6138 - val_loss: 0.3378 - lr: 0.0020 - 995ms/epoch - 40ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3380 - val_loss: 0.2530 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1729 - val_loss: 0.2870 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2055 - val_loss: 0.2577 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1646 - val_loss: 0.3150 - lr: 0.0020 - 991ms/epoch - 40ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2334 - val_loss: 0.2608 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1307 - val_loss: 0.2942 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1875 - val_loss: 0.2287 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1195 - val_loss: 0.2502 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1508 - val_loss: 0.2473 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2068 - val_loss: 0.3345 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.5913 - val_loss: 0.3446 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 1.6683 - val_loss: 0.5067 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8365 - val_loss: 0.4004 - lr: 4.0000e-04 - 997ms/epoch - 40ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.6903 - val_loss: 0.3145 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.9478 - val_loss: 0.5565 - lr: 8.0000e-05 - 987ms/epoch - 39ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.4258 - val_loss: 0.4614 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.3403 - val_loss: 0.4105 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.3085 - val_loss: 0.4120 - lr: 1.6000e-05 - 994ms/epoch - 40ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.2949 - val_loss: 0.4228 - lr: 1.6000e-05 - 996ms/epoch - 40ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.2780 - val_loss: 0.4215 - lr: 1.6000e-05 - 988ms/epoch - 40ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.2695 - val_loss: 0.4245 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.2667 - val_loss: 0.4242 - lr: 3.2000e-06 - 990ms/epoch - 40ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.2650 - val_loss: 0.4207 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.2630 - val_loss: 0.4208 - lr: 6.4000e-07 - 997ms/epoch - 40ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.2595 - val_loss: 0.4211 - lr: 6.4000e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.2574 - val_loss: 0.4214 - lr: 6.4000e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 34/500\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "25/25 - 1s - loss: 0.2587 - val_loss: 0.4215 - lr: 1.2800e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 34: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  56.0s\n",
      "7/7 - 3s - 3s/epoch - 393ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  58.7s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.5818 - val_loss: 0.4657 - lr: 0.0100 - 20s/epoch - 790ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4155 - val_loss: 0.3242 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4100 - val_loss: 0.3248 - lr: 0.0100 - 996ms/epoch - 40ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4526 - val_loss: 0.4297 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4769 - val_loss: 0.3991 - lr: 0.0100 - 985ms/epoch - 39ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.4953 - val_loss: 0.3840 - lr: 0.0100 - 977ms/epoch - 39ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 1.6853 - val_loss: 0.6623 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8701 - val_loss: 0.5698 - lr: 0.0020 - 995ms/epoch - 40ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8562 - val_loss: 0.5847 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 1.3974 - val_loss: 0.8878 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 1.1419 - val_loss: 1.1402 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.9007 - val_loss: 1.1515 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7399 - val_loss: 1.1400 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.5034 - val_loss: 1.0607 - lr: 8.0000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2973 - val_loss: 0.9282 - lr: 8.0000e-05 - 983ms/epoch - 39ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2334 - val_loss: 0.8002 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1980 - val_loss: 0.6922 - lr: 8.0000e-05 - 996ms/epoch - 40ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1754 - val_loss: 0.6106 - lr: 8.0000e-05 - 983ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1582 - val_loss: 0.5518 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1452 - val_loss: 0.5091 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1338 - val_loss: 0.4752 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "25/25 - 1s - loss: 0.1249 - val_loss: 0.4485 - lr: 8.0000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  44.0s\n",
      "7/7 - 3s - 3s/epoch - 389ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  46.7s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 1.1250 - val_loss: 0.4134 - lr: 0.0100 - 20s/epoch - 799ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4789 - val_loss: 0.3215 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4971 - val_loss: 0.3117 - lr: 0.0100 - 992ms/epoch - 40ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5436 - val_loss: 0.4065 - lr: 0.0100 - 996ms/epoch - 40ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.5834 - val_loss: 0.3442 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.5304 - val_loss: 0.2505 - lr: 0.0020 - 1s/epoch - 44ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0759 - val_loss: 0.2645 - lr: 0.0020 - 986ms/epoch - 39ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1367 - val_loss: 0.2532 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3913 - val_loss: 0.2647 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0818 - val_loss: 0.2750 - lr: 0.0020 - 999ms/epoch - 40ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1016 - val_loss: 0.2565 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0991 - val_loss: 0.2497 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0974 - val_loss: 0.2472 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1047 - val_loss: 0.2469 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0993 - val_loss: 0.2466 - lr: 8.0000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0970 - val_loss: 0.2464 - lr: 8.0000e-05 - 978ms/epoch - 39ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0952 - val_loss: 0.2464 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0948 - val_loss: 0.2463 - lr: 1.6000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0948 - val_loss: 0.2463 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0945 - val_loss: 0.2463 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0943 - val_loss: 0.2463 - lr: 3.2000e-06 - 1s/epoch - 40ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0942 - val_loss: 0.2463 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 6.4000e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 6.4000e-07 - 999ms/epoch - 40ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0943 - val_loss: 0.2463 - lr: 6.4000e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2463 - lr: 1.2800e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0939 - val_loss: 0.2463 - lr: 1.2800e-07 - 1s/epoch - 40ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 1.2800e-07 - 1s/epoch - 41ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0942 - val_loss: 0.2463 - lr: 2.5600e-08 - 1s/epoch - 41ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0947 - val_loss: 0.2463 - lr: 2.5600e-08 - 1s/epoch - 40ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0937 - val_loss: 0.2463 - lr: 2.5600e-08 - 1s/epoch - 41ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 5.1200e-09 - 1s/epoch - 40ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0942 - val_loss: 0.2463 - lr: 5.1200e-09 - 1s/epoch - 41ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0936 - val_loss: 0.2463 - lr: 5.1200e-09 - 1s/epoch - 41ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.0240e-09 - 1s/epoch - 40ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0945 - val_loss: 0.2463 - lr: 1.0240e-09 - 1s/epoch - 41ms/step\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 1.0240e-09 - 1s/epoch - 41ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0941 - val_loss: 0.2463 - lr: 2.0480e-10 - 1s/epoch - 40ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0945 - val_loss: 0.2463 - lr: 2.0480e-10 - 993ms/epoch - 40ms/step\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0942 - val_loss: 0.2463 - lr: 2.0480e-10 - 988ms/epoch - 40ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0942 - val_loss: 0.2463 - lr: 4.0960e-11 - 988ms/epoch - 40ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0941 - val_loss: 0.2463 - lr: 4.0960e-11 - 983ms/epoch - 39ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 4.0960e-11 - 1s/epoch - 41ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 8.1920e-12 - 1s/epoch - 41ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0939 - val_loss: 0.2463 - lr: 8.1920e-12 - 999ms/epoch - 40ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0941 - val_loss: 0.2463 - lr: 8.1920e-12 - 983ms/epoch - 39ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0939 - val_loss: 0.2463 - lr: 1.6384e-12 - 996ms/epoch - 40ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0938 - val_loss: 0.2463 - lr: 1.6384e-12 - 999ms/epoch - 40ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 1.6384e-12 - 1s/epoch - 41ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2768e-13 - 998ms/epoch - 40ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0935 - val_loss: 0.2463 - lr: 3.2768e-13 - 1s/epoch - 40ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.553599351567796e-14.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2768e-13 - 1s/epoch - 41ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0940 - val_loss: 0.2463 - lr: 6.5536e-14 - 1s/epoch - 40ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0935 - val_loss: 0.2463 - lr: 6.5536e-14 - 1s/epoch - 41ms/step\n",
      "Epoch 55/500\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.310719924523668e-14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0941 - val_loss: 0.2463 - lr: 6.5536e-14 - 1s/epoch - 40ms/step\n",
      "Epoch 55: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.3min\n",
      "7/7 - 3s - 3s/epoch - 374ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.3min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.4930 - val_loss: 0.7745 - lr: 0.0100 - 20s/epoch - 781ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1750 - val_loss: 0.7046 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1967 - val_loss: 1.1477 - lr: 0.0100 - 974ms/epoch - 39ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2202 - val_loss: 1.4534 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2111 - val_loss: 1.3921 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.5578 - val_loss: 2.4155 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 2.3016 - lr: 0.0020 - 985ms/epoch - 39ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1647 - val_loss: 2.0411 - lr: 0.0020 - 993ms/epoch - 40ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1319 - val_loss: 1.7594 - lr: 0.0020 - 975ms/epoch - 39ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1104 - val_loss: 1.6683 - lr: 0.0020 - 995ms/epoch - 40ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1077 - val_loss: 1.4746 - lr: 0.0020 - 986ms/epoch - 39ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1102 - val_loss: 1.3710 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0993 - val_loss: 1.3361 - lr: 0.0020 - 991ms/epoch - 40ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0675 - val_loss: 1.1351 - lr: 0.0020 - 984ms/epoch - 39ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0903 - val_loss: 1.0956 - lr: 0.0020 - 989ms/epoch - 40ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0710 - val_loss: 1.0125 - lr: 0.0020 - 993ms/epoch - 40ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0673 - val_loss: 0.8650 - lr: 0.0020 - 999ms/epoch - 40ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0649 - val_loss: 0.6257 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0803 - val_loss: 0.4923 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0786 - val_loss: 0.9233 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.7216 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0625 - val_loss: 0.7287 - lr: 0.0020 - 987ms/epoch - 39ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.8813 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.7488 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.8978 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0422 - val_loss: 0.8457 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0409 - val_loss: 0.7712 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0410 - val_loss: 0.9594 - lr: 0.0020 - 973ms/epoch - 39ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.7268 - lr: 0.0020 - 997ms/epoch - 40ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.9270 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.6804 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.9542 - lr: 0.0020 - 1000ms/epoch - 40ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.6821 - lr: 0.0020 - 969ms/epoch - 39ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0366 - val_loss: 0.9720 - lr: 0.0020 - 997ms/epoch - 40ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0323 - val_loss: 0.6954 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0323 - val_loss: 0.7943 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 1.1684 - lr: 0.0020 - 992ms/epoch - 40ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.6148 - lr: 0.0020 - 984ms/epoch - 39ms/step\n",
      "Epoch 39/500\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "25/25 - 1s - loss: 0.0301 - val_loss: 0.9470 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 39: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.0min\n",
      "7/7 - 3s - 3s/epoch - 447ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.1min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.4524 - val_loss: 0.4792 - lr: 0.0100 - 19s/epoch - 771ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 2.1687 - val_loss: 2.3288 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7468 - val_loss: 1.8907 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.7932 - val_loss: 1.9034 - lr: 0.0100 - 872ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7579 - val_loss: 1.9101 - lr: 0.0020 - 909ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7583 - val_loss: 1.9211 - lr: 0.0020 - 878ms/epoch - 35ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7579 - val_loss: 1.9312 - lr: 0.0020 - 858ms/epoch - 34ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7500 - val_loss: 1.9333 - lr: 4.0000e-04 - 911ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7500 - val_loss: 1.9355 - lr: 4.0000e-04 - 915ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7499 - val_loss: 1.9377 - lr: 4.0000e-04 - 881ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7482 - val_loss: 1.9382 - lr: 8.0000e-05 - 869ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7482 - val_loss: 1.9386 - lr: 8.0000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7482 - val_loss: 1.9391 - lr: 8.0000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7478 - val_loss: 1.9392 - lr: 1.6000e-05 - 872ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7478 - val_loss: 1.9393 - lr: 1.6000e-05 - 913ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7478 - val_loss: 1.9394 - lr: 1.6000e-05 - 918ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7477 - val_loss: 1.9394 - lr: 3.2000e-06 - 878ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7477 - val_loss: 1.9394 - lr: 3.2000e-06 - 888ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7477 - val_loss: 1.9395 - lr: 3.2000e-06 - 900ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7477 - val_loss: 1.9395 - lr: 6.4000e-07 - 889ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7477 - val_loss: 1.9395 - lr: 6.4000e-07 - 903ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  39.3s\n",
      "7/7 - 3s - 3s/epoch - 384ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  41.9s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.3667 - val_loss: 0.4505 - lr: 0.0100 - 19s/epoch - 773ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.3308 - val_loss: 0.8159 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.2832 - val_loss: 1.9493 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.7985 - val_loss: 1.9925 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7646 - val_loss: 1.9940 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7649 - val_loss: 1.9974 - lr: 0.0020 - 917ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.7649 - val_loss: 2.0007 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7600 - val_loss: 2.0014 - lr: 4.0000e-04 - 920ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7600 - val_loss: 2.0022 - lr: 4.0000e-04 - 895ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7600 - val_loss: 2.0030 - lr: 4.0000e-04 - 880ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7589 - val_loss: 2.0032 - lr: 8.0000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7589 - val_loss: 2.0034 - lr: 8.0000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7589 - val_loss: 2.0035 - lr: 8.0000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0036 - lr: 1.6000e-05 - 899ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0036 - lr: 1.6000e-05 - 932ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0037 - lr: 1.6000e-05 - 892ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0037 - lr: 3.2000e-06 - 891ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0037 - lr: 3.2000e-06 - 888ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0037 - lr: 3.2000e-06 - 877ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0037 - lr: 6.4000e-07 - 896ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0037 - lr: 6.4000e-07 - 903ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  39.4s\n",
      "7/7 - 3s - 3s/epoch - 443ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  42.5s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.2555 - val_loss: 0.6525 - lr: 0.0100 - 21s/epoch - 821ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 2.2811 - val_loss: 1.7299 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0952 - val_loss: 1.9226 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9448 - val_loss: 2.1083 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8635 - val_loss: 2.1225 - lr: 0.0020 - 927ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8632 - val_loss: 2.1389 - lr: 0.0020 - 908ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8623 - val_loss: 2.1541 - lr: 0.0020 - 945ms/epoch - 38ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8521 - val_loss: 2.1572 - lr: 4.0000e-04 - 912ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8519 - val_loss: 2.1605 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8518 - val_loss: 2.1639 - lr: 4.0000e-04 - 905ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8496 - val_loss: 2.1646 - lr: 8.0000e-05 - 870ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8496 - val_loss: 2.1653 - lr: 8.0000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8495 - val_loss: 2.1660 - lr: 8.0000e-05 - 914ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8491 - val_loss: 2.1661 - lr: 1.6000e-05 - 886ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8490 - val_loss: 2.1663 - lr: 1.6000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8490 - val_loss: 2.1664 - lr: 1.6000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8489 - val_loss: 2.1665 - lr: 3.2000e-06 - 901ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8490 - val_loss: 2.1665 - lr: 3.2000e-06 - 894ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8489 - val_loss: 2.1665 - lr: 3.2000e-06 - 910ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8489 - val_loss: 2.1665 - lr: 6.4000e-07 - 918ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8489 - val_loss: 2.1665 - lr: 6.4000e-07 - 944ms/epoch - 38ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.8s\n",
      "7/7 - 3s - 3s/epoch - 395ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  43.5s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.3511 - val_loss: 1.0490 - lr: 0.0100 - 19s/epoch - 752ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 2.4431 - val_loss: 2.3479 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9468 - val_loss: 2.0689 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9547 - val_loss: 2.2726 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8787 - val_loss: 2.3075 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8757 - val_loss: 2.3442 - lr: 0.0020 - 912ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8725 - val_loss: 2.3779 - lr: 0.0020 - 936ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8592 - val_loss: 2.3842 - lr: 4.0000e-04 - 917ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8587 - val_loss: 2.3907 - lr: 4.0000e-04 - 876ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8583 - val_loss: 2.3971 - lr: 4.0000e-04 - 881ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8556 - val_loss: 2.3983 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8556 - val_loss: 2.3996 - lr: 8.0000e-05 - 917ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8555 - val_loss: 2.4009 - lr: 8.0000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8549 - val_loss: 2.4012 - lr: 1.6000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8549 - val_loss: 2.4014 - lr: 1.6000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8549 - val_loss: 2.4017 - lr: 1.6000e-05 - 918ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8548 - val_loss: 2.4017 - lr: 3.2000e-06 - 891ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8548 - val_loss: 2.4018 - lr: 3.2000e-06 - 984ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8548 - val_loss: 2.4018 - lr: 3.2000e-06 - 950ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8548 - val_loss: 2.4018 - lr: 6.4000e-07 - 913ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8547 - val_loss: 2.4019 - lr: 6.4000e-07 - 911ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.2s\n",
      "7/7 - 3s - 3s/epoch - 398ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  43.0s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.5260 - val_loss: 1.7299 - lr: 0.0100 - 19s/epoch - 760ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4536 - val_loss: 3.7304 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.2592 - val_loss: 3.5823 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2571 - val_loss: 3.6343 - lr: 0.0100 - 980ms/epoch - 39ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2545 - val_loss: 3.6197 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2533 - val_loss: 3.6330 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2511 - val_loss: 3.6346 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2492 - val_loss: 3.6418 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2473 - val_loss: 3.6463 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2455 - val_loss: 3.6515 - lr: 0.0100 - 974ms/epoch - 39ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2437 - val_loss: 3.6561 - lr: 0.0100 - 955ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2420 - val_loss: 3.6605 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2406 - val_loss: 3.6649 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2392 - val_loss: 3.6692 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2377 - val_loss: 3.6726 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2364 - val_loss: 3.6759 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2352 - val_loss: 3.6795 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2340 - val_loss: 3.6825 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2331 - val_loss: 3.6850 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2320 - val_loss: 3.6872 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2311 - val_loss: 3.6900 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.6s\n",
      "7/7 - 3s - 3s/epoch - 417ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  43.5s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7811 - val_loss: 1.7854 - lr: 0.0100 - 20s/epoch - 789ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7718 - val_loss: 1.7677 - lr: 0.0100 - 986ms/epoch - 39ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7634 - val_loss: 1.7506 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7549 - val_loss: 1.7326 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7461 - val_loss: 1.7123 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7355 - val_loss: 1.6879 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7231 - val_loss: 1.6571 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7069 - val_loss: 1.6162 - lr: 0.0100 - 966ms/epoch - 39ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.6850 - val_loss: 1.5599 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.6558 - val_loss: 1.4810 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.6131 - val_loss: 1.3674 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.5526 - val_loss: 1.2052 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.4682 - val_loss: 0.9857 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.3587 - val_loss: 0.7271 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2452 - val_loss: 0.4892 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1577 - val_loss: 0.3272 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1082 - val_loss: 0.2420 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0852 - val_loss: 0.2029 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0759 - val_loss: 0.1859 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0715 - val_loss: 0.1783 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0692 - val_loss: 0.1750 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0681 - val_loss: 0.1733 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0675 - val_loss: 0.1724 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0670 - val_loss: 0.1719 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0666 - val_loss: 0.1715 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0671 - val_loss: 0.1713 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.1710 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0661 - val_loss: 0.1709 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0657 - val_loss: 0.1707 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0657 - val_loss: 0.1706 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0653 - val_loss: 0.1703 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.1703 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0647 - val_loss: 0.1701 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.1700 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.1700 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1698 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0647 - val_loss: 0.1696 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1695 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0642 - val_loss: 0.1694 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1693 - lr: 0.0020 - 956ms/epoch - 38ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1693 - lr: 0.0020 - 926ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0637 - val_loss: 0.1693 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1692 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1692 - lr: 0.0020 - 910ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0634 - val_loss: 0.1692 - lr: 0.0020 - 919ms/epoch - 37ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0634 - val_loss: 0.1691 - lr: 0.0020 - 960ms/epoch - 38ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.1691 - lr: 4.0000e-04 - 924ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.1691 - lr: 4.0000e-04 - 920ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0638 - val_loss: 0.1691 - lr: 4.0000e-04 - 922ms/epoch - 37ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.1691 - lr: 8.0000e-05 - 924ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 8.0000e-05 - 937ms/epoch - 37ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.1691 - lr: 8.0000e-05 - 944ms/epoch - 38ms/step\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0637 - val_loss: 0.1691 - lr: 8.0000e-05 - 971ms/epoch - 39ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1691 - lr: 1.6000e-05 - 908ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1691 - lr: 1.6000e-05 - 918ms/epoch - 37ms/step\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1691 - lr: 1.6000e-05 - 914ms/epoch - 37ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 3.2000e-06 - 922ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0634 - val_loss: 0.1691 - lr: 3.2000e-06 - 913ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.1691 - lr: 3.2000e-06 - 914ms/epoch - 37ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 6.4000e-07 - 915ms/epoch - 37ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.1691 - lr: 6.4000e-07 - 947ms/epoch - 38ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.1691 - lr: 6.4000e-07 - 923ms/epoch - 37ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1691 - lr: 6.4000e-07 - 925ms/epoch - 37ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.1691 - lr: 6.4000e-07 - 907ms/epoch - 36ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.1691 - lr: 6.4000e-07 - 943ms/epoch - 38ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1691 - lr: 1.2800e-07 - 945ms/epoch - 38ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 1.2800e-07 - 910ms/epoch - 36ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1691 - lr: 1.2800e-07 - 934ms/epoch - 37ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0634 - val_loss: 0.1691 - lr: 2.5600e-08 - 906ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1691 - lr: 2.5600e-08 - 920ms/epoch - 37ms/step\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 2.5600e-08 - 935ms/epoch - 37ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.1691 - lr: 5.1200e-09 - 941ms/epoch - 38ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1691 - lr: 5.1200e-09 - 922ms/epoch - 37ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.1691 - lr: 5.1200e-09 - 931ms/epoch - 37ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 5.1200e-09 - 921ms/epoch - 37ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 5.1200e-09 - 923ms/epoch - 37ms/step\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1691 - lr: 5.1200e-09 - 942ms/epoch - 38ms/step\n",
      "Epoch 78/500\n",
      "Restoring model weights from the end of the best epoch: 58.\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1691 - lr: 1.0240e-09 - 924ms/epoch - 37ms/step\n",
      "Epoch 78: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.6min\n",
      "7/7 - 3s - 3s/epoch - 388ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.6min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7801 - val_loss: 1.7575 - lr: 0.0100 - 20s/epoch - 791ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7711 - val_loss: 1.7398 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7614 - val_loss: 1.7197 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7497 - val_loss: 1.6946 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7355 - val_loss: 1.6614 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7155 - val_loss: 1.6145 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6866 - val_loss: 1.5449 - lr: 0.0100 - 968ms/epoch - 39ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.6449 - val_loss: 1.4390 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.5790 - val_loss: 1.2741 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.4805 - val_loss: 1.0306 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3483 - val_loss: 0.7337 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2162 - val_loss: 0.4805 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1340 - val_loss: 0.3323 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0972 - val_loss: 0.2610 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0827 - val_loss: 0.2266 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0759 - val_loss: 0.2089 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0723 - val_loss: 0.1988 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0707 - val_loss: 0.1926 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0688 - val_loss: 0.1887 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0678 - val_loss: 0.1858 - lr: 0.0100 - 964ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0672 - val_loss: 0.1836 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0663 - val_loss: 0.1818 - lr: 0.0100 - 970ms/epoch - 39ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0659 - val_loss: 0.1804 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0654 - val_loss: 0.1792 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0647 - val_loss: 0.1782 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0637 - val_loss: 0.1774 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1766 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1760 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1754 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.1749 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0625 - val_loss: 0.1745 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0625 - val_loss: 0.1741 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1740 - lr: 0.0020 - 953ms/epoch - 38ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1739 - lr: 0.0020 - 929ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1738 - lr: 0.0020 - 968ms/epoch - 39ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1738 - lr: 0.0020 - 924ms/epoch - 37ms/step\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1737 - lr: 0.0020 - 932ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1737 - lr: 4.0000e-04 - 950ms/epoch - 38ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1737 - lr: 4.0000e-04 - 941ms/epoch - 38ms/step\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1736 - lr: 4.0000e-04 - 940ms/epoch - 38ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1736 - lr: 8.0000e-05 - 930ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1736 - lr: 8.0000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1736 - lr: 8.0000e-05 - 925ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1736 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1736 - lr: 8.0000e-05 - 951ms/epoch - 38ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1736 - lr: 1.6000e-05 - 918ms/epoch - 37ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1736 - lr: 1.6000e-05 - 929ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1736 - lr: 1.6000e-05 - 937ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1736 - lr: 3.2000e-06 - 941ms/epoch - 38ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1736 - lr: 3.2000e-06 - 928ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1736 - lr: 3.2000e-06 - 930ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1736 - lr: 6.4000e-07 - 923ms/epoch - 37ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1736 - lr: 6.4000e-07 - 893ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1736 - lr: 6.4000e-07 - 935ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1736 - lr: 1.2800e-07 - 921ms/epoch - 37ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1736 - lr: 1.2800e-07 - 922ms/epoch - 37ms/step\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1736 - lr: 1.2800e-07 - 910ms/epoch - 36ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1736 - lr: 2.5600e-08 - 935ms/epoch - 37ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1736 - lr: 2.5600e-08 - 903ms/epoch - 36ms/step\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1736 - lr: 2.5600e-08 - 916ms/epoch - 37ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1736 - lr: 5.1200e-09 - 909ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1736 - lr: 5.1200e-09 - 881ms/epoch - 35ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1736 - lr: 5.1200e-09 - 932ms/epoch - 37ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1736 - lr: 5.1200e-09 - 929ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1736 - lr: 5.1200e-09 - 960ms/epoch - 38ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1736 - lr: 1.0240e-09 - 924ms/epoch - 37ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1736 - lr: 1.0240e-09 - 919ms/epoch - 37ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1736 - lr: 1.0240e-09 - 919ms/epoch - 37ms/step\n",
      "Epoch 69/500\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1736 - lr: 2.0480e-10 - 909ms/epoch - 36ms/step\n",
      "Epoch 69: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.4min\n",
      "7/7 - 3s - 3s/epoch - 397ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.5min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.8774 - val_loss: 1.7143 - lr: 0.0100 - 19s/epoch - 771ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8632 - val_loss: 1.6907 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8440 - val_loss: 1.6564 - lr: 0.0100 - 952ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8158 - val_loss: 1.6028 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7700 - val_loss: 1.5126 - lr: 0.0100 - 971ms/epoch - 39ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.6918 - val_loss: 1.3532 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.5524 - val_loss: 1.0799 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3510 - val_loss: 0.7285 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1765 - val_loss: 0.4700 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0999 - val_loss: 0.3432 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0727 - val_loss: 0.2820 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.2496 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.2305 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.2186 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0426 - val_loss: 0.2106 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0410 - val_loss: 0.2052 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0395 - val_loss: 0.2014 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0387 - val_loss: 0.1984 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0374 - val_loss: 0.1962 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0376 - val_loss: 0.1944 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0365 - val_loss: 0.1929 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1916 - lr: 0.0100 - 962ms/epoch - 38ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0372 - val_loss: 0.1905 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0352 - val_loss: 0.1896 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0353 - val_loss: 0.1887 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0355 - val_loss: 0.1879 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0352 - val_loss: 0.1872 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1871 - lr: 0.0020 - 936ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1869 - lr: 0.0020 - 936ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1868 - lr: 0.0020 - 941ms/epoch - 38ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1867 - lr: 0.0020 - 943ms/epoch - 38ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1866 - lr: 4.0000e-04 - 943ms/epoch - 38ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1866 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1866 - lr: 4.0000e-04 - 921ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1866 - lr: 4.0000e-04 - 935ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1865 - lr: 4.0000e-04 - 927ms/epoch - 37ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1865 - lr: 8.0000e-05 - 915ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1865 - lr: 8.0000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1865 - lr: 8.0000e-05 - 911ms/epoch - 36ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1865 - lr: 1.6000e-05 - 935ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0342 - val_loss: 0.1865 - lr: 1.6000e-05 - 951ms/epoch - 38ms/step\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1865 - lr: 1.6000e-05 - 915ms/epoch - 37ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1865 - lr: 3.2000e-06 - 909ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.1865 - lr: 3.2000e-06 - 908ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1865 - lr: 3.2000e-06 - 914ms/epoch - 37ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1865 - lr: 6.4000e-07 - 930ms/epoch - 37ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1865 - lr: 6.4000e-07 - 920ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.1865 - lr: 6.4000e-07 - 918ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0335 - val_loss: 0.1865 - lr: 6.4000e-07 - 909ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0335 - val_loss: 0.1865 - lr: 6.4000e-07 - 877ms/epoch - 35ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1865 - lr: 6.4000e-07 - 911ms/epoch - 36ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0344 - val_loss: 0.1865 - lr: 6.4000e-07 - 920ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1865 - lr: 1.2800e-07 - 905ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1865 - lr: 1.2800e-07 - 927ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1865 - lr: 1.2800e-07 - 910ms/epoch - 36ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0336 - val_loss: 0.1865 - lr: 2.5600e-08 - 909ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0336 - val_loss: 0.1865 - lr: 2.5600e-08 - 917ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1865 - lr: 2.5600e-08 - 898ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0336 - val_loss: 0.1865 - lr: 5.1200e-09 - 892ms/epoch - 36ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1865 - lr: 5.1200e-09 - 909ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1865 - lr: 5.1200e-09 - 940ms/epoch - 38ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1865 - lr: 1.0240e-09 - 894ms/epoch - 36ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1865 - lr: 1.0240e-09 - 923ms/epoch - 37ms/step\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1865 - lr: 1.0240e-09 - 935ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1865 - lr: 2.0480e-10 - 910ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1865 - lr: 2.0480e-10 - 909ms/epoch - 36ms/step\n",
      "Epoch 66: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.4min\n",
      "7/7 - 3s - 3s/epoch - 443ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.4min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.9392 - val_loss: 1.7103 - lr: 0.0100 - 19s/epoch - 771ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9197 - val_loss: 1.6798 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8954 - val_loss: 1.6376 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8614 - val_loss: 1.5741 - lr: 0.0100 - 962ms/epoch - 38ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8102 - val_loss: 1.4730 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7281 - val_loss: 1.3059 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.5923 - val_loss: 1.0362 - lr: 0.0100 - 958ms/epoch - 38ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3923 - val_loss: 0.6859 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1978 - val_loss: 0.4118 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1066 - val_loss: 0.2868 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0837 - val_loss: 0.2373 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0755 - val_loss: 0.2147 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0695 - val_loss: 0.2023 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.1943 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0638 - val_loss: 0.1888 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1853 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.1829 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.1815 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1803 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1796 - lr: 0.0100 - 966ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1790 - lr: 0.0100 - 955ms/epoch - 38ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.1786 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1784 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.1781 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1780 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1777 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1776 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1774 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1771 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1770 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1769 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1767 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1767 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1765 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1764 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0538 - val_loss: 0.1761 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1757 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.1753 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1749 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1744 - lr: 0.0100 - 941ms/epoch - 38ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.1743 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1743 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1743 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1739 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1735 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1733 - lr: 0.0100 - 956ms/epoch - 38ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1728 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1726 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1720 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1718 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1718 - lr: 0.0020 - 942ms/epoch - 38ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1716 - lr: 0.0020 - 962ms/epoch - 38ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1715 - lr: 0.0020 - 947ms/epoch - 38ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1715 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.1714 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1713 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1713 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.1712 - lr: 0.0020 - 918ms/epoch - 37ms/step\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1712 - lr: 0.0020 - 953ms/epoch - 38ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1711 - lr: 4.0000e-04 - 913ms/epoch - 37ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1711 - lr: 4.0000e-04 - 929ms/epoch - 37ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1711 - lr: 4.0000e-04 - 908ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1711 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.1711 - lr: 4.0000e-04 - 913ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1711 - lr: 4.0000e-04 - 922ms/epoch - 37ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.1711 - lr: 4.0000e-04 - 932ms/epoch - 37ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1710 - lr: 4.0000e-04 - 897ms/epoch - 36ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1710 - lr: 4.0000e-04 - 943ms/epoch - 38ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1710 - lr: 8.0000e-05 - 932ms/epoch - 37ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1710 - lr: 8.0000e-05 - 917ms/epoch - 37ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0511 - val_loss: 0.1710 - lr: 8.0000e-05 - 894ms/epoch - 36ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.1710 - lr: 8.0000e-05 - 941ms/epoch - 38ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1710 - lr: 8.0000e-05 - 935ms/epoch - 37ms/step\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1710 - lr: 8.0000e-05 - 909ms/epoch - 36ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1710 - lr: 1.6000e-05 - 882ms/epoch - 35ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1710 - lr: 1.6000e-05 - 921ms/epoch - 37ms/step\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1710 - lr: 1.6000e-05 - 913ms/epoch - 37ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.1710 - lr: 3.2000e-06 - 930ms/epoch - 37ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.1710 - lr: 3.2000e-06 - 930ms/epoch - 37ms/step\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1710 - lr: 3.2000e-06 - 897ms/epoch - 36ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1710 - lr: 6.4000e-07 - 909ms/epoch - 36ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1710 - lr: 6.4000e-07 - 914ms/epoch - 37ms/step\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1710 - lr: 6.4000e-07 - 915ms/epoch - 37ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1710 - lr: 1.2800e-07 - 932ms/epoch - 37ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.1710 - lr: 1.2800e-07 - 929ms/epoch - 37ms/step\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.1710 - lr: 1.2800e-07 - 883ms/epoch - 35ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1710 - lr: 2.5600e-08 - 905ms/epoch - 36ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1710 - lr: 2.5600e-08 - 925ms/epoch - 37ms/step\n",
      "Epoch 89/500\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1710 - lr: 2.5600e-08 - 926ms/epoch - 37ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0513 - val_loss: 0.1710 - lr: 5.1200e-09 - 928ms/epoch - 37ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0509 - val_loss: 0.1710 - lr: 5.1200e-09 - 945ms/epoch - 38ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1710 - lr: 5.1200e-09 - 897ms/epoch - 36ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1710 - lr: 5.1200e-09 - 915ms/epoch - 37ms/step\n",
      "Epoch 94/500\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1710 - lr: 5.1200e-09 - 909ms/epoch - 36ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1710 - lr: 1.0240e-09 - 929ms/epoch - 37ms/step\n",
      "Epoch 96/500\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1710 - lr: 1.0240e-09 - 913ms/epoch - 37ms/step\n",
      "Epoch 96: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.8min\n",
      "7/7 - 3s - 3s/epoch - 388ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.9min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.6424 - val_loss: 1.7613 - lr: 0.0100 - 19s/epoch - 744ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6264 - val_loss: 1.7612 - lr: 0.0100 - 989ms/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6075 - val_loss: 1.7589 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5819 - val_loss: 1.7519 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.5445 - val_loss: 1.7353 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4852 - val_loss: 1.6983 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.3864 - val_loss: 1.6189 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2531 - val_loss: 1.4823 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1495 - val_loss: 1.3412 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1086 - val_loss: 1.2423 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0979 - val_loss: 1.1714 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0929 - val_loss: 1.1133 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0898 - val_loss: 1.0625 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0866 - val_loss: 1.0136 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0839 - val_loss: 0.9711 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0812 - val_loss: 0.9308 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0788 - val_loss: 0.8929 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0771 - val_loss: 0.8600 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0748 - val_loss: 0.8269 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0732 - val_loss: 0.7986 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0719 - val_loss: 0.7714 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0700 - val_loss: 0.7461 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0694 - val_loss: 0.7227 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0680 - val_loss: 0.7006 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0671 - val_loss: 0.6797 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0659 - val_loss: 0.6611 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0649 - val_loss: 0.6435 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0647 - val_loss: 0.6279 - lr: 0.0100 - 968ms/epoch - 39ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.6130 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.5989 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0626 - val_loss: 0.5847 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.5731 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.5621 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.5515 - lr: 0.0100 - 963ms/epoch - 39ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.5425 - lr: 0.0100 - 932ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.5339 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.5253 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.5175 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 39/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.5142 - lr: 0.0020 - 943ms/epoch - 38ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.5111 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.5086 - lr: 0.0020 - 953ms/epoch - 38ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.5062 - lr: 0.0020 - 920ms/epoch - 37ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.5041 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.5037 - lr: 4.0000e-04 - 929ms/epoch - 37ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.5032 - lr: 4.0000e-04 - 925ms/epoch - 37ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.5028 - lr: 4.0000e-04 - 898ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.5024 - lr: 4.0000e-04 - 933ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.5019 - lr: 4.0000e-04 - 928ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.5015 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.5015 - lr: 8.0000e-05 - 915ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.5014 - lr: 8.0000e-05 - 920ms/epoch - 37ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.5013 - lr: 8.0000e-05 - 918ms/epoch - 37ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.5013 - lr: 1.6000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.5013 - lr: 1.6000e-05 - 930ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.5012 - lr: 1.6000e-05 - 923ms/epoch - 37ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.5012 - lr: 1.6000e-05 - 918ms/epoch - 37ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.5012 - lr: 1.6000e-05 - 933ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.5012 - lr: 1.6000e-05 - 929ms/epoch - 37ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.5012 - lr: 3.2000e-06 - 916ms/epoch - 37ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.5012 - lr: 3.2000e-06 - 904ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.5012 - lr: 3.2000e-06 - 957ms/epoch - 38ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.5012 - lr: 3.2000e-06 - 934ms/epoch - 37ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.5012 - lr: 6.4000e-07 - 897ms/epoch - 36ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.5012 - lr: 6.4000e-07 - 908ms/epoch - 36ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.5012 - lr: 6.4000e-07 - 900ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.5012 - lr: 1.2800e-07 - 946ms/epoch - 38ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.5012 - lr: 1.2800e-07 - 937ms/epoch - 37ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.5012 - lr: 1.2800e-07 - 910ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.5012 - lr: 2.5600e-08 - 923ms/epoch - 37ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.5012 - lr: 2.5600e-08 - 927ms/epoch - 37ms/step\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.5012 - lr: 2.5600e-08 - 936ms/epoch - 37ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.5012 - lr: 5.1200e-09 - 898ms/epoch - 36ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.5012 - lr: 5.1200e-09 - 936ms/epoch - 37ms/step\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.5012 - lr: 5.1200e-09 - 956ms/epoch - 38ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.5012 - lr: 1.0240e-09 - 915ms/epoch - 37ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.5012 - lr: 1.0240e-09 - 917ms/epoch - 37ms/step\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.5012 - lr: 1.0240e-09 - 919ms/epoch - 37ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.5012 - lr: 2.0480e-10 - 919ms/epoch - 37ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.5012 - lr: 2.0480e-10 - 925ms/epoch - 37ms/step\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.5012 - lr: 2.0480e-10 - 926ms/epoch - 37ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.5012 - lr: 4.0960e-11 - 869ms/epoch - 35ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.5012 - lr: 4.0960e-11 - 919ms/epoch - 37ms/step\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.5012 - lr: 4.0960e-11 - 912ms/epoch - 36ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.5012 - lr: 8.1920e-12 - 908ms/epoch - 36ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.5012 - lr: 8.1920e-12 - 914ms/epoch - 37ms/step\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.5012 - lr: 8.1920e-12 - 954ms/epoch - 38ms/step\n",
      "Epoch 87/500\n",
      "Restoring model weights from the end of the best epoch: 67.\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.5012 - lr: 1.6384e-12 - 951ms/epoch - 38ms/step\n",
      "Epoch 87: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.7min\n",
      "7/7 - 3s - 3s/epoch - 388ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.5995 - val_loss: 0.7821 - lr: 0.0100 - 19s/epoch - 748ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3730 - val_loss: 0.2397 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0945 - val_loss: 0.2445 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0767 - val_loss: 0.2350 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0729 - val_loss: 0.2285 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0722 - val_loss: 0.2169 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0722 - val_loss: 0.2134 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0725 - val_loss: 0.2059 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0730 - val_loss: 0.1997 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0831 - val_loss: 0.1873 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0749 - val_loss: 0.1801 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0720 - val_loss: 0.1761 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0707 - val_loss: 0.1743 - lr: 0.0020 - 920ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0695 - val_loss: 0.1716 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0686 - val_loss: 0.1708 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0673 - val_loss: 0.1694 - lr: 0.0020 - 901ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0677 - val_loss: 0.1689 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0661 - val_loss: 0.1683 - lr: 0.0020 - 887ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0661 - val_loss: 0.1674 - lr: 0.0020 - 974ms/epoch - 39ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0655 - val_loss: 0.1671 - lr: 0.0020 - 902ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0647 - val_loss: 0.1660 - lr: 0.0020 - 911ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0647 - val_loss: 0.1661 - lr: 0.0020 - 879ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0637 - val_loss: 0.1653 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0635 - val_loss: 0.1640 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1633 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1631 - lr: 0.0020 - 950ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0622 - val_loss: 0.1625 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1623 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1618 - lr: 0.0020 - 883ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.1610 - lr: 0.0020 - 911ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1613 - lr: 0.0020 - 902ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1602 - lr: 0.0020 - 915ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1596 - lr: 0.0020 - 908ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1597 - lr: 0.0020 - 873ms/epoch - 35ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1581 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.1572 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.1573 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1573 - lr: 0.0020 - 889ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.1576 - lr: 0.0020 - 896ms/epoch - 36ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.1570 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1561 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.1553 - lr: 0.0020 - 909ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1549 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.1551 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1543 - lr: 0.0020 - 957ms/epoch - 38ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0580 - val_loss: 0.1540 - lr: 0.0020 - 920ms/epoch - 37ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.1540 - lr: 0.0020 - 902ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.1536 - lr: 0.0020 - 885ms/epoch - 35ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.1530 - lr: 0.0020 - 881ms/epoch - 35ms/step\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.1527 - lr: 0.0020 - 896ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.1535 - lr: 4.0000e-04 - 877ms/epoch - 35ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.1539 - lr: 4.0000e-04 - 914ms/epoch - 37ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1538 - lr: 4.0000e-04 - 902ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.1538 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1539 - lr: 4.0000e-04 - 885ms/epoch - 35ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1538 - lr: 4.0000e-04 - 888ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1537 - lr: 4.0000e-04 - 914ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1536 - lr: 4.0000e-04 - 911ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1536 - lr: 4.0000e-04 - 949ms/epoch - 38ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1534 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1534 - lr: 4.0000e-04 - 905ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1532 - lr: 4.0000e-04 - 886ms/epoch - 35ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1532 - lr: 8.0000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1532 - lr: 8.0000e-05 - 916ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1532 - lr: 8.0000e-05 - 916ms/epoch - 37ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1531 - lr: 8.0000e-05 - 916ms/epoch - 37ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1531 - lr: 8.0000e-05 - 880ms/epoch - 35ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.1532 - lr: 8.0000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1532 - lr: 8.0000e-05 - 880ms/epoch - 35ms/step\n",
      "Epoch 70/500\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1531 - lr: 8.0000e-05 - 915ms/epoch - 37ms/step\n",
      "Epoch 70: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.4min\n",
      "7/7 - 3s - 3s/epoch - 395ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.4min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.5645 - val_loss: 0.7466 - lr: 0.0100 - 19s/epoch - 743ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2737 - val_loss: 0.2509 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0825 - val_loss: 0.2440 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0698 - val_loss: 0.2375 - lr: 0.0100 - 878ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0663 - val_loss: 0.2255 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.2178 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0669 - val_loss: 0.2137 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0676 - val_loss: 0.2030 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0683 - val_loss: 0.2018 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0812 - val_loss: 0.1892 - lr: 0.0020 - 901ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0723 - val_loss: 0.1807 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0697 - val_loss: 0.1759 - lr: 0.0020 - 884ms/epoch - 35ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0681 - val_loss: 0.1772 - lr: 4.0000e-04 - 895ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0666 - val_loss: 0.1780 - lr: 4.0000e-04 - 873ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.1780 - lr: 4.0000e-04 - 908ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0653 - val_loss: 0.1777 - lr: 4.0000e-04 - 921ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0641 - val_loss: 0.1773 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1770 - lr: 4.0000e-04 - 885ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0637 - val_loss: 0.1764 - lr: 4.0000e-04 - 917ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.1756 - lr: 4.0000e-04 - 901ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.1753 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0631 - val_loss: 0.1747 - lr: 4.0000e-04 - 921ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.1741 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1736 - lr: 4.0000e-04 - 884ms/epoch - 35ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0627 - val_loss: 0.1731 - lr: 4.0000e-04 - 867ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0628 - val_loss: 0.1727 - lr: 4.0000e-04 - 901ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.1722 - lr: 4.0000e-04 - 896ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.1722 - lr: 8.0000e-05 - 938ms/epoch - 38ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1722 - lr: 8.0000e-05 - 889ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1722 - lr: 8.0000e-05 - 880ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1721 - lr: 8.0000e-05 - 932ms/epoch - 37ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1721 - lr: 8.0000e-05 - 893ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1721 - lr: 8.0000e-05 - 887ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1721 - lr: 1.6000e-05 - 893ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1721 - lr: 1.6000e-05 - 921ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1721 - lr: 1.6000e-05 - 917ms/epoch - 37ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1720 - lr: 1.6000e-05 - 928ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1720 - lr: 1.6000e-05 - 953ms/epoch - 38ms/step\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1720 - lr: 1.6000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1720 - lr: 3.2000e-06 - 883ms/epoch - 35ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1720 - lr: 3.2000e-06 - 892ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.1720 - lr: 3.2000e-06 - 954ms/epoch - 38ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1720 - lr: 6.4000e-07 - 892ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1720 - lr: 6.4000e-07 - 949ms/epoch - 38ms/step\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1720 - lr: 6.4000e-07 - 886ms/epoch - 35ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1720 - lr: 1.2800e-07 - 887ms/epoch - 35ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1720 - lr: 1.2800e-07 - 892ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1720 - lr: 1.2800e-07 - 911ms/epoch - 36ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1720 - lr: 2.5600e-08 - 903ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.1720 - lr: 2.5600e-08 - 910ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1720 - lr: 2.5600e-08 - 870ms/epoch - 35ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1720 - lr: 5.1200e-09 - 902ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1720 - lr: 5.1200e-09 - 919ms/epoch - 37ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1720 - lr: 5.1200e-09 - 911ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1720 - lr: 5.1200e-09 - 951ms/epoch - 38ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1720 - lr: 1.0240e-09 - 931ms/epoch - 37ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0608 - val_loss: 0.1720 - lr: 1.0240e-09 - 906ms/epoch - 36ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.1720 - lr: 1.0240e-09 - 894ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1720 - lr: 2.0480e-10 - 905ms/epoch - 36ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1720 - lr: 2.0480e-10 - 900ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1720 - lr: 2.0480e-10 - 904ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1720 - lr: 4.0960e-11 - 923ms/epoch - 37ms/step\n",
      "Epoch 62: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.3min\n",
      "7/7 - 3s - 3s/epoch - 373ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.3min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.5212 - val_loss: 0.3795 - lr: 0.0100 - 19s/epoch - 779ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2401 - val_loss: 0.2587 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0361 - val_loss: 0.2382 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0310 - val_loss: 0.2341 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0329 - val_loss: 0.2231 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.2186 - lr: 0.0100 - 966ms/epoch - 39ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.2094 - lr: 0.0100 - 875ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1889 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0453 - val_loss: 0.1820 - lr: 0.0020 - 917ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0425 - val_loss: 0.1794 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0432 - val_loss: 0.1811 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0425 - val_loss: 0.1819 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0415 - val_loss: 0.1824 - lr: 4.0000e-04 - 917ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0409 - val_loss: 0.1825 - lr: 8.0000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0404 - val_loss: 0.1827 - lr: 8.0000e-05 - 914ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1828 - lr: 8.0000e-05 - 902ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1828 - lr: 1.6000e-05 - 879ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1828 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1829 - lr: 1.6000e-05 - 930ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0405 - val_loss: 0.1829 - lr: 3.2000e-06 - 901ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1829 - lr: 3.2000e-06 - 873ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1829 - lr: 3.2000e-06 - 882ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0398 - val_loss: 0.1829 - lr: 6.4000e-07 - 866ms/epoch - 35ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0394 - val_loss: 0.1829 - lr: 6.4000e-07 - 894ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0395 - val_loss: 0.1829 - lr: 6.4000e-07 - 919ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1829 - lr: 1.2800e-07 - 924ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1829 - lr: 1.2800e-07 - 913ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0395 - val_loss: 0.1829 - lr: 1.2800e-07 - 904ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0399 - val_loss: 0.1829 - lr: 2.5600e-08 - 881ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "25/25 - 1s - loss: 0.0398 - val_loss: 0.1829 - lr: 2.5600e-08 - 891ms/epoch - 36ms/step\n",
      "Epoch 30: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  47.9s\n",
      "7/7 - 3s - 3s/epoch - 386ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time=  50.6s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.6593 - val_loss: 0.6061 - lr: 0.0100 - 19s/epoch - 766ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1391 - val_loss: 0.2519 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.2365 - lr: 0.0100 - 963ms/epoch - 39ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.2346 - lr: 0.0100 - 878ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.2247 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.2184 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.2113 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.2045 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.2037 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1921 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.1806 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.1747 - lr: 0.0020 - 877ms/epoch - 35ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1716 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1717 - lr: 4.0000e-04 - 873ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1716 - lr: 4.0000e-04 - 892ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1715 - lr: 4.0000e-04 - 944ms/epoch - 38ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1715 - lr: 8.0000e-05 - 1s/epoch - 52ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.1715 - lr: 8.0000e-05 - 883ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1715 - lr: 8.0000e-05 - 859ms/epoch - 34ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0537 - val_loss: 0.1714 - lr: 8.0000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1714 - lr: 8.0000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1714 - lr: 8.0000e-05 - 953ms/epoch - 38ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1713 - lr: 8.0000e-05 - 920ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1713 - lr: 8.0000e-05 - 908ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1713 - lr: 8.0000e-05 - 876ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1712 - lr: 1.6000e-05 - 883ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1712 - lr: 1.6000e-05 - 934ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 1.6000e-05 - 921ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1712 - lr: 3.2000e-06 - 917ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1712 - lr: 3.2000e-06 - 879ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 3.2000e-06 - 883ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1712 - lr: 3.2000e-06 - 900ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1712 - lr: 3.2000e-06 - 903ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1712 - lr: 3.2000e-06 - 887ms/epoch - 35ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1712 - lr: 3.2000e-06 - 910ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 6.4000e-07 - 927ms/epoch - 37ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1712 - lr: 6.4000e-07 - 897ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1712 - lr: 6.4000e-07 - 903ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1712 - lr: 6.4000e-07 - 901ms/epoch - 36ms/step\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 6.4000e-07 - 896ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1712 - lr: 1.2800e-07 - 895ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1712 - lr: 1.2800e-07 - 932ms/epoch - 37ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1712 - lr: 1.2800e-07 - 924ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1712 - lr: 2.5600e-08 - 901ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1712 - lr: 2.5600e-08 - 911ms/epoch - 36ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1712 - lr: 2.5600e-08 - 869ms/epoch - 35ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 2.5600e-08 - 894ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 2.5600e-08 - 912ms/epoch - 36ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 2.5600e-08 - 908ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1712 - lr: 5.1200e-09 - 866ms/epoch - 35ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.1712 - lr: 5.1200e-09 - 881ms/epoch - 35ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1712 - lr: 5.1200e-09 - 854ms/epoch - 34ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1712 - lr: 1.0240e-09 - 883ms/epoch - 35ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.1712 - lr: 1.0240e-09 - 906ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 1.0240e-09 - 948ms/epoch - 38ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1712 - lr: 2.0480e-10 - 905ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1712 - lr: 2.0480e-10 - 886ms/epoch - 35ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0537 - val_loss: 0.1712 - lr: 2.0480e-10 - 892ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1712 - lr: 4.0960e-11 - 859ms/epoch - 34ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1712 - lr: 4.0960e-11 - 895ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 4.0960e-11 - 901ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 8.1920e-12 - 915ms/epoch - 37ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1712 - lr: 8.1920e-12 - 891ms/epoch - 36ms/step\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 8.1920e-12 - 878ms/epoch - 35ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 1.6384e-12 - 900ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1712 - lr: 1.6384e-12 - 907ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1712 - lr: 1.6384e-12 - 899ms/epoch - 36ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 1.6384e-12 - 917ms/epoch - 37ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1712 - lr: 1.6384e-12 - 923ms/epoch - 37ms/step\n",
      "Epoch 70/500\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1712 - lr: 1.6384e-12 - 884ms/epoch - 35ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1712 - lr: 3.2768e-13 - 917ms/epoch - 37ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1712 - lr: 3.2768e-13 - 910ms/epoch - 36ms/step\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.553599351567796e-14.\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1712 - lr: 3.2768e-13 - 912ms/epoch - 36ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1712 - lr: 6.5536e-14 - 911ms/epoch - 36ms/step\n",
      "Epoch 75/500\n",
      "Restoring model weights from the end of the best epoch: 55.\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1712 - lr: 6.5536e-14 - 956ms/epoch - 38ms/step\n",
      "Epoch 75: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.5min\n",
      "7/7 - 3s - 3s/epoch - 377ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.5min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.2310 - val_loss: 2.2830 - lr: 0.0100 - 19s/epoch - 773ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2031 - val_loss: 2.3372 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1871 - val_loss: 2.2316 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1652 - val_loss: 1.9435 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1319 - val_loss: 1.4452 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0941 - val_loss: 0.9593 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0739 - val_loss: 0.7413 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0678 - val_loss: 0.6767 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.6444 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0648 - val_loss: 0.6272 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.6120 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.5973 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.5839 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.5685 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.5551 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.5427 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.5299 - lr: 0.0100 - 932ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.5192 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.5101 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.5007 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.4917 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.4795 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.4692 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.4587 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.4500 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.4434 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.4366 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.4266 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.4222 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.4150 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.4043 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.4010 - lr: 0.0100 - 953ms/epoch - 38ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.3953 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3870 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.3802 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.3753 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.3656 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.3613 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.3569 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.3519 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.3460 - lr: 0.0100 - 870ms/epoch - 35ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.3430 - lr: 0.0100 - 859ms/epoch - 34ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.3376 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.3313 - lr: 0.0100 - 868ms/epoch - 35ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.3249 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.3214 - lr: 0.0100 - 976ms/epoch - 39ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0509 - val_loss: 0.3154 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0510 - val_loss: 0.3120 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.3073 - lr: 0.0100 - 873ms/epoch - 35ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0496 - val_loss: 0.3012 - lr: 0.0100 - 876ms/epoch - 35ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.2999 - lr: 0.0100 - 876ms/epoch - 35ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0498 - val_loss: 0.2942 - lr: 0.0100 - 963ms/epoch - 39ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.2911 - lr: 0.0100 - 966ms/epoch - 39ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.2895 - lr: 0.0100 - 931ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.2855 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.2818 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.2765 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0477 - val_loss: 0.2698 - lr: 0.0100 - 853ms/epoch - 34ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.2681 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.2665 - lr: 0.0100 - 866ms/epoch - 35ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0470 - val_loss: 0.2635 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.2580 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0468 - val_loss: 0.2575 - lr: 0.0100 - 876ms/epoch - 35ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0464 - val_loss: 0.2514 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.2539 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.2466 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0462 - val_loss: 0.2437 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0449 - val_loss: 0.2388 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0451 - val_loss: 0.2352 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2340 - lr: 0.0100 - 907ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0446 - val_loss: 0.2313 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0444 - val_loss: 0.2283 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0445 - val_loss: 0.2264 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0438 - val_loss: 0.2224 - lr: 0.0100 - 931ms/epoch - 37ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0435 - val_loss: 0.2190 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0427 - val_loss: 0.2154 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0429 - val_loss: 0.2165 - lr: 0.0100 - 882ms/epoch - 35ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0425 - val_loss: 0.2079 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0415 - val_loss: 0.2060 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0425 - val_loss: 0.2044 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0418 - val_loss: 0.2024 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0414 - val_loss: 0.1999 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0413 - val_loss: 0.1982 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0405 - val_loss: 0.1950 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0404 - val_loss: 0.1940 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1897 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0401 - val_loss: 0.1874 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0393 - val_loss: 0.1837 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0389 - val_loss: 0.1797 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0393 - val_loss: 0.1788 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0388 - val_loss: 0.1784 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0384 - val_loss: 0.1734 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0380 - val_loss: 0.1736 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0381 - val_loss: 0.1690 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0377 - val_loss: 0.1704 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1618 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 97/500\n",
      "25/25 - 1s - loss: 0.0369 - val_loss: 0.1599 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.1607 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0367 - val_loss: 0.1560 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0362 - val_loss: 0.1556 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 101/500\n",
      "25/25 - 1s - loss: 0.0362 - val_loss: 0.1513 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0354 - val_loss: 0.1499 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 103/500\n",
      "25/25 - 1s - loss: 0.0356 - val_loss: 0.1505 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 104/500\n",
      "25/25 - 1s - loss: 0.0349 - val_loss: 0.1437 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0350 - val_loss: 0.1422 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 106/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1438 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 107/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1386 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0344 - val_loss: 0.1378 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 109/500\n",
      "25/25 - 1s - loss: 0.0346 - val_loss: 0.1351 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 110/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1340 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 111/500\n",
      "25/25 - 1s - loss: 0.0340 - val_loss: 0.1318 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 112/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1310 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 113/500\n",
      "25/25 - 1s - loss: 0.0331 - val_loss: 0.1278 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 114/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.1279 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 115/500\n",
      "25/25 - 1s - loss: 0.0332 - val_loss: 0.1263 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 116/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.1238 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 117/500\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.1232 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 118/500\n",
      "25/25 - 1s - loss: 0.0353 - val_loss: 0.1165 - lr: 0.0020 - 933ms/epoch - 37ms/step\n",
      "Epoch 119/500\n",
      "25/25 - 1s - loss: 0.0322 - val_loss: 0.1161 - lr: 0.0020 - 895ms/epoch - 36ms/step\n",
      "Epoch 120/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.1159 - lr: 0.0020 - 892ms/epoch - 36ms/step\n",
      "Epoch 121/500\n",
      "25/25 - 1s - loss: 0.0320 - val_loss: 0.1157 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 122/500\n",
      "25/25 - 1s - loss: 0.0328 - val_loss: 0.1155 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 123/500\n",
      "25/25 - 1s - loss: 0.0315 - val_loss: 0.1154 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 124/500\n",
      "25/25 - 1s - loss: 0.0319 - val_loss: 0.1152 - lr: 0.0020 - 878ms/epoch - 35ms/step\n",
      "Epoch 125/500\n",
      "25/25 - 1s - loss: 0.0316 - val_loss: 0.1151 - lr: 0.0020 - 934ms/epoch - 37ms/step\n",
      "Epoch 126/500\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0316 - val_loss: 0.1148 - lr: 0.0020 - 888ms/epoch - 36ms/step\n",
      "Epoch 127/500\n",
      "25/25 - 1s - loss: 0.0324 - val_loss: 0.1143 - lr: 4.0000e-04 - 915ms/epoch - 37ms/step\n",
      "Epoch 128/500\n",
      "25/25 - 1s - loss: 0.0315 - val_loss: 0.1140 - lr: 4.0000e-04 - 908ms/epoch - 36ms/step\n",
      "Epoch 129/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1139 - lr: 4.0000e-04 - 898ms/epoch - 36ms/step\n",
      "Epoch 130/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1138 - lr: 4.0000e-04 - 887ms/epoch - 35ms/step\n",
      "Epoch 131/500\n",
      "25/25 - 1s - loss: 0.0318 - val_loss: 0.1137 - lr: 4.0000e-04 - 909ms/epoch - 36ms/step\n",
      "Epoch 132/500\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1136 - lr: 4.0000e-04 - 914ms/epoch - 37ms/step\n",
      "Epoch 133/500\n",
      "25/25 - 1s - loss: 0.0308 - val_loss: 0.1136 - lr: 8.0000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 134/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1136 - lr: 8.0000e-05 - 892ms/epoch - 36ms/step\n",
      "Epoch 135/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1136 - lr: 8.0000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 136/500\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1136 - lr: 8.0000e-05 - 917ms/epoch - 37ms/step\n",
      "Epoch 137/500\n",
      "25/25 - 1s - loss: 0.0318 - val_loss: 0.1136 - lr: 1.6000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 138/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1136 - lr: 1.6000e-05 - 930ms/epoch - 37ms/step\n",
      "Epoch 139/500\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1135 - lr: 1.6000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 140/500\n",
      "25/25 - 1s - loss: 0.0319 - val_loss: 0.1135 - lr: 3.2000e-06 - 900ms/epoch - 36ms/step\n",
      "Epoch 141/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1135 - lr: 3.2000e-06 - 906ms/epoch - 36ms/step\n",
      "Epoch 142/500\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0310 - val_loss: 0.1135 - lr: 3.2000e-06 - 933ms/epoch - 37ms/step\n",
      "Epoch 143/500\n",
      "25/25 - 1s - loss: 0.0314 - val_loss: 0.1135 - lr: 6.4000e-07 - 893ms/epoch - 36ms/step\n",
      "Epoch 144/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1135 - lr: 6.4000e-07 - 926ms/epoch - 37ms/step\n",
      "Epoch 145/500\n",
      "25/25 - 1s - loss: 0.0304 - val_loss: 0.1135 - lr: 6.4000e-07 - 919ms/epoch - 37ms/step\n",
      "Epoch 146/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1135 - lr: 6.4000e-07 - 902ms/epoch - 36ms/step\n",
      "Epoch 147/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1135 - lr: 6.4000e-07 - 918ms/epoch - 37ms/step\n",
      "Epoch 148/500\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0308 - val_loss: 0.1135 - lr: 6.4000e-07 - 896ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1135 - lr: 1.2800e-07 - 878ms/epoch - 35ms/step\n",
      "Epoch 150/500\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1135 - lr: 1.2800e-07 - 897ms/epoch - 36ms/step\n",
      "Epoch 151/500\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1135 - lr: 1.2800e-07 - 937ms/epoch - 37ms/step\n",
      "Epoch 152/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1135 - lr: 2.5600e-08 - 911ms/epoch - 36ms/step\n",
      "Epoch 153/500\n",
      "25/25 - 1s - loss: 0.0314 - val_loss: 0.1135 - lr: 2.5600e-08 - 907ms/epoch - 36ms/step\n",
      "Epoch 154/500\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1135 - lr: 2.5600e-08 - 895ms/epoch - 36ms/step\n",
      "Epoch 155/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1135 - lr: 5.1200e-09 - 893ms/epoch - 36ms/step\n",
      "Epoch 156/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1135 - lr: 5.1200e-09 - 886ms/epoch - 35ms/step\n",
      "Epoch 157/500\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1135 - lr: 5.1200e-09 - 892ms/epoch - 36ms/step\n",
      "Epoch 158/500\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1135 - lr: 1.0240e-09 - 935ms/epoch - 37ms/step\n",
      "Epoch 159/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1135 - lr: 1.0240e-09 - 874ms/epoch - 35ms/step\n",
      "Epoch 160/500\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1135 - lr: 1.0240e-09 - 907ms/epoch - 36ms/step\n",
      "Epoch 161/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1135 - lr: 2.0480e-10 - 880ms/epoch - 35ms/step\n",
      "Epoch 162/500\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1135 - lr: 2.0480e-10 - 930ms/epoch - 37ms/step\n",
      "Epoch 163/500\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1135 - lr: 2.0480e-10 - 882ms/epoch - 35ms/step\n",
      "Epoch 164/500\n",
      "25/25 - 1s - loss: 0.0310 - val_loss: 0.1135 - lr: 4.0960e-11 - 956ms/epoch - 38ms/step\n",
      "Epoch 165/500\n",
      "25/25 - 1s - loss: 0.0321 - val_loss: 0.1135 - lr: 4.0960e-11 - 892ms/epoch - 36ms/step\n",
      "Epoch 166/500\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1135 - lr: 4.0960e-11 - 905ms/epoch - 36ms/step\n",
      "Epoch 167/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1135 - lr: 8.1920e-12 - 902ms/epoch - 36ms/step\n",
      "Epoch 168/500\n",
      "25/25 - 1s - loss: 0.0307 - val_loss: 0.1135 - lr: 8.1920e-12 - 903ms/epoch - 36ms/step\n",
      "Epoch 169/500\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0308 - val_loss: 0.1135 - lr: 8.1920e-12 - 868ms/epoch - 35ms/step\n",
      "Epoch 170/500\n",
      "25/25 - 1s - loss: 0.0308 - val_loss: 0.1135 - lr: 1.6384e-12 - 898ms/epoch - 36ms/step\n",
      "Epoch 171/500\n",
      "Restoring model weights from the end of the best epoch: 151.\n",
      "25/25 - 1s - loss: 0.0317 - val_loss: 0.1135 - lr: 1.6384e-12 - 936ms/epoch - 37ms/step\n",
      "Epoch 171: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 2.9min\n",
      "7/7 - 3s - 3s/epoch - 444ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 3.0min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.6530 - val_loss: 0.4263 - lr: 0.0100 - 20s/epoch - 795ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4556 - val_loss: 0.2962 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4216 - val_loss: 0.2479 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3463 - val_loss: 0.2716 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.3106 - val_loss: 0.2601 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2235 - val_loss: 0.3416 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2071 - val_loss: 0.2520 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1931 - val_loss: 0.2546 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1976 - val_loss: 0.2540 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1905 - val_loss: 0.2565 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1823 - val_loss: 0.2523 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1670 - val_loss: 0.2534 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2203 - val_loss: 0.2464 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1776 - val_loss: 0.2671 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1901 - val_loss: 0.2491 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1579 - val_loss: 0.2498 - lr: 0.0020 - 901ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1546 - val_loss: 0.2484 - lr: 0.0020 - 887ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1501 - val_loss: 0.2468 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1433 - val_loss: 0.2476 - lr: 0.0020 - 943ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1384 - val_loss: 0.2379 - lr: 0.0020 - 929ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1342 - val_loss: 0.2243 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1260 - val_loss: 0.2265 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.1062 - val_loss: 0.2255 - lr: 0.0020 - 920ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.1000 - val_loss: 0.2298 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0953 - val_loss: 0.2320 - lr: 0.0020 - 907ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0966 - val_loss: 0.2286 - lr: 0.0020 - 922ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0890 - val_loss: 0.2256 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0853 - val_loss: 0.1977 - lr: 0.0020 - 927ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0775 - val_loss: 0.2218 - lr: 0.0020 - 883ms/epoch - 35ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0904 - val_loss: 0.2394 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0654 - val_loss: 0.2183 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0742 - val_loss: 0.2254 - lr: 0.0020 - 947ms/epoch - 38ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.2177 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.2239 - lr: 0.0020 - 937ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.2148 - lr: 0.0020 - 907ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0778 - val_loss: 0.2249 - lr: 0.0020 - 885ms/epoch - 35ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.2208 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0666 - val_loss: 0.2357 - lr: 0.0020 - 939ms/epoch - 38ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.2256 - lr: 4.0000e-04 - 915ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0434 - val_loss: 0.2238 - lr: 4.0000e-04 - 908ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0427 - val_loss: 0.2219 - lr: 4.0000e-04 - 926ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0426 - val_loss: 0.2188 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0418 - val_loss: 0.2133 - lr: 4.0000e-04 - 939ms/epoch - 38ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0408 - val_loss: 0.2074 - lr: 4.0000e-04 - 917ms/epoch - 37ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0403 - val_loss: 0.2000 - lr: 4.0000e-04 - 963ms/epoch - 39ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0402 - val_loss: 0.1920 - lr: 4.0000e-04 - 912ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1866 - lr: 4.0000e-04 - 930ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0404 - val_loss: 0.1854 - lr: 4.0000e-04 - 935ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0393 - val_loss: 0.1902 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1873 - lr: 4.0000e-04 - 930ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0384 - val_loss: 0.1925 - lr: 4.0000e-04 - 941ms/epoch - 38ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0397 - val_loss: 0.1938 - lr: 4.0000e-04 - 897ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0377 - val_loss: 0.1893 - lr: 4.0000e-04 - 903ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0387 - val_loss: 0.1859 - lr: 4.0000e-04 - 913ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0379 - val_loss: 0.1894 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0387 - val_loss: 0.1844 - lr: 4.0000e-04 - 883ms/epoch - 35ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0374 - val_loss: 0.1840 - lr: 8.0000e-05 - 935ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0368 - val_loss: 0.1861 - lr: 8.0000e-05 - 921ms/epoch - 37ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0364 - val_loss: 0.1874 - lr: 8.0000e-05 - 924ms/epoch - 37ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0368 - val_loss: 0.1880 - lr: 8.0000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0368 - val_loss: 0.1894 - lr: 8.0000e-05 - 911ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0362 - val_loss: 0.1896 - lr: 8.0000e-05 - 906ms/epoch - 36ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0358 - val_loss: 0.1881 - lr: 8.0000e-05 - 922ms/epoch - 37ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0359 - val_loss: 0.1875 - lr: 8.0000e-05 - 961ms/epoch - 38ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0363 - val_loss: 0.1879 - lr: 8.0000e-05 - 928ms/epoch - 37ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0359 - val_loss: 0.1895 - lr: 8.0000e-05 - 906ms/epoch - 36ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0358 - val_loss: 0.1894 - lr: 1.6000e-05 - 913ms/epoch - 37ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0359 - val_loss: 0.1894 - lr: 1.6000e-05 - 909ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0355 - val_loss: 0.1894 - lr: 1.6000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0359 - val_loss: 0.1893 - lr: 1.6000e-05 - 943ms/epoch - 38ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0362 - val_loss: 0.1892 - lr: 1.6000e-05 - 898ms/epoch - 36ms/step\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0359 - val_loss: 0.1890 - lr: 1.6000e-05 - 906ms/epoch - 36ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0359 - val_loss: 0.1890 - lr: 3.2000e-06 - 909ms/epoch - 36ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0350 - val_loss: 0.1889 - lr: 3.2000e-06 - 923ms/epoch - 37ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0355 - val_loss: 0.1890 - lr: 3.2000e-06 - 920ms/epoch - 37ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0359 - val_loss: 0.1891 - lr: 3.2000e-06 - 907ms/epoch - 36ms/step\n",
      "Epoch 77/500\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0356 - val_loss: 0.1891 - lr: 3.2000e-06 - 957ms/epoch - 38ms/step\n",
      "Epoch 77: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.5min\n",
      "7/7 - 3s - 3s/epoch - 379ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time= 1.6min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.3327 - val_loss: 0.3939 - lr: 0.0100 - 19s/epoch - 750ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5243 - val_loss: 0.3951 - lr: 0.0100 - 957ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7536 - val_loss: 0.2764 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.3592 - val_loss: 0.2552 - lr: 0.0100 - 931ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2119 - val_loss: 0.2532 - lr: 0.0020 - 909ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1945 - val_loss: 0.2457 - lr: 0.0020 - 920ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1850 - val_loss: 0.2492 - lr: 0.0020 - 935ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1812 - val_loss: 0.2509 - lr: 0.0020 - 941ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1802 - val_loss: 0.2511 - lr: 0.0020 - 929ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1790 - val_loss: 0.2510 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1783 - val_loss: 0.2505 - lr: 0.0020 - 923ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1779 - val_loss: 0.2495 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1774 - val_loss: 0.2489 - lr: 0.0020 - 955ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1767 - val_loss: 0.2483 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1766 - val_loss: 0.2464 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1750 - val_loss: 0.2448 - lr: 0.0020 - 939ms/epoch - 38ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1744 - val_loss: 0.2412 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1731 - val_loss: 0.2369 - lr: 0.0020 - 915ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1753 - val_loss: 0.2387 - lr: 0.0020 - 901ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1675 - val_loss: 0.2319 - lr: 0.0020 - 937ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1772 - val_loss: 0.2390 - lr: 0.0020 - 911ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1637 - val_loss: 0.2286 - lr: 0.0020 - 911ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.1662 - val_loss: 0.2178 - lr: 0.0020 - 940ms/epoch - 38ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.1571 - val_loss: 0.2188 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.1600 - val_loss: 0.2013 - lr: 0.0020 - 980ms/epoch - 39ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.1539 - val_loss: 0.2046 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.1498 - val_loss: 0.2343 - lr: 0.0020 - 934ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.1629 - val_loss: 0.1884 - lr: 0.0020 - 933ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.1593 - val_loss: 0.1886 - lr: 0.0020 - 922ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1880 - val_loss: 0.2687 - lr: 0.0020 - 926ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.1720 - val_loss: 0.2747 - lr: 4.0000e-04 - 893ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.1601 - val_loss: 0.2547 - lr: 4.0000e-04 - 937ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.1491 - val_loss: 0.2374 - lr: 4.0000e-04 - 930ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.1432 - val_loss: 0.2239 - lr: 4.0000e-04 - 898ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.1410 - val_loss: 0.2168 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.1406 - val_loss: 0.2153 - lr: 4.0000e-04 - 893ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.1403 - val_loss: 0.2152 - lr: 4.0000e-04 - 904ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.1400 - val_loss: 0.2144 - lr: 4.0000e-04 - 898ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.1396 - val_loss: 0.2128 - lr: 4.0000e-04 - 934ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.1392 - val_loss: 0.2122 - lr: 4.0000e-04 - 931ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.1388 - val_loss: 0.2122 - lr: 4.0000e-04 - 932ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.1393 - val_loss: 0.2111 - lr: 4.0000e-04 - 920ms/epoch - 37ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.1389 - val_loss: 0.2100 - lr: 4.0000e-04 - 929ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.1382 - val_loss: 0.2098 - lr: 4.0000e-04 - 936ms/epoch - 37ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.1384 - val_loss: 0.2087 - lr: 4.0000e-04 - 951ms/epoch - 38ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.1377 - val_loss: 0.2088 - lr: 4.0000e-04 - 931ms/epoch - 37ms/step\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.1379 - val_loss: 0.2083 - lr: 4.0000e-04 - 913ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "25/25 - 1s - loss: 0.1372 - val_loss: 0.2079 - lr: 4.0000e-04 - 948ms/epoch - 38ms/step\n",
      "Epoch 48: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 3s - 3s/epoch - 379ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time= 1.1min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.4319 - val_loss: 0.4879 - lr: 0.0100 - 20s/epoch - 785ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3074 - val_loss: 0.4670 - lr: 0.0100 - 993ms/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8478 - val_loss: 0.4995 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3368 - val_loss: 0.2530 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.4940 - val_loss: 0.3078 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2051 - val_loss: 0.3725 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1837 - val_loss: 0.2838 - lr: 0.0020 - 909ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1809 - val_loss: 0.2607 - lr: 0.0020 - 931ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1812 - val_loss: 0.2571 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1805 - val_loss: 0.2572 - lr: 0.0020 - 959ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1804 - val_loss: 0.2576 - lr: 0.0020 - 910ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1803 - val_loss: 0.2578 - lr: 0.0020 - 938ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1803 - val_loss: 0.2577 - lr: 0.0020 - 960ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1807 - val_loss: 0.2577 - lr: 0.0020 - 911ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1809 - val_loss: 0.2578 - lr: 0.0020 - 918ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1720 - val_loss: 0.2576 - lr: 4.0000e-04 - 936ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1721 - val_loss: 0.2572 - lr: 4.0000e-04 - 934ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1719 - val_loss: 0.2569 - lr: 4.0000e-04 - 914ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1720 - val_loss: 0.2566 - lr: 4.0000e-04 - 933ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1723 - val_loss: 0.2564 - lr: 4.0000e-04 - 939ms/epoch - 38ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1721 - val_loss: 0.2562 - lr: 4.0000e-04 - 926ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1702 - val_loss: 0.2562 - lr: 8.0000e-05 - 920ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.1701 - val_loss: 0.2561 - lr: 8.0000e-05 - 949ms/epoch - 38ms/step\n",
      "Epoch 24/500\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "25/25 - 1s - loss: 0.1703 - val_loss: 0.2561 - lr: 8.0000e-05 - 919ms/epoch - 37ms/step\n",
      "Epoch 24: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.2s\n",
      "7/7 - 3s - 3s/epoch - 377ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  45.8s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.2147 - val_loss: 0.3765 - lr: 0.0100 - 19s/epoch - 775ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3680 - val_loss: 0.2759 - lr: 0.0100 - 972ms/epoch - 39ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4437 - val_loss: 0.2931 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.3006 - val_loss: 0.3531 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1013 - val_loss: 0.2471 - lr: 0.0020 - 956ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1002 - val_loss: 0.2467 - lr: 0.0020 - 933ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0980 - val_loss: 0.2477 - lr: 0.0020 - 922ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0971 - val_loss: 0.2485 - lr: 0.0020 - 907ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0968 - val_loss: 0.2488 - lr: 0.0020 - 912ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0968 - val_loss: 0.2489 - lr: 0.0020 - 910ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0969 - val_loss: 0.2490 - lr: 0.0020 - 946ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0971 - val_loss: 0.2490 - lr: 0.0020 - 962ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0970 - val_loss: 0.2490 - lr: 0.0020 - 924ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2489 - lr: 4.0000e-04 - 914ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2488 - lr: 4.0000e-04 - 929ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2487 - lr: 4.0000e-04 - 940ms/epoch - 38ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2486 - lr: 4.0000e-04 - 930ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0922 - val_loss: 0.2486 - lr: 8.0000e-05 - 949ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0921 - val_loss: 0.2485 - lr: 8.0000e-05 - 951ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0922 - val_loss: 0.2485 - lr: 8.0000e-05 - 950ms/epoch - 38ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0922 - val_loss: 0.2485 - lr: 8.0000e-05 - 920ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0921 - val_loss: 0.2485 - lr: 8.0000e-05 - 936ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0920 - val_loss: 0.2485 - lr: 1.6000e-05 - 940ms/epoch - 38ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0919 - val_loss: 0.2485 - lr: 1.6000e-05 - 954ms/epoch - 38ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0920 - val_loss: 0.2485 - lr: 1.6000e-05 - 947ms/epoch - 38ms/step\n",
      "Epoch 26/500\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "25/25 - 1s - loss: 0.0920 - val_loss: 0.2485 - lr: 1.6000e-05 - 951ms/epoch - 38ms/step\n",
      "Epoch 26: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  44.9s\n",
      "7/7 - 3s - 3s/epoch - 434ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  47.9s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.5684 - val_loss: 2.6150 - lr: 0.0100 - 20s/epoch - 787ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4171 - val_loss: 4.0645 - lr: 0.0100 - 952ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2189 - val_loss: 3.6393 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2457 - val_loss: 3.7443 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2346 - val_loss: 3.7290 - lr: 0.0100 - 957ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2343 - val_loss: 3.7291 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2046 - val_loss: 3.7285 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2054 - val_loss: 3.7476 - lr: 0.0020 - 924ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2048 - val_loss: 3.7593 - lr: 0.0020 - 896ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2045 - val_loss: 3.7649 - lr: 0.0020 - 935ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2045 - val_loss: 3.7672 - lr: 0.0020 - 951ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2044 - val_loss: 3.7680 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2046 - val_loss: 3.7681 - lr: 0.0020 - 917ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2046 - val_loss: 3.7678 - lr: 0.0020 - 925ms/epoch - 37ms/step\n",
      "Epoch 15/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2048 - val_loss: 3.7675 - lr: 0.0020 - 935ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1989 - val_loss: 3.7669 - lr: 4.0000e-04 - 930ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1991 - val_loss: 3.7675 - lr: 4.0000e-04 - 965ms/epoch - 39ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1991 - val_loss: 3.7680 - lr: 4.0000e-04 - 951ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1991 - val_loss: 3.7686 - lr: 4.0000e-04 - 936ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1978 - val_loss: 3.7686 - lr: 8.0000e-05 - 908ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.1978 - val_loss: 3.7687 - lr: 8.0000e-05 - 945ms/epoch - 38ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.5s\n",
      "7/7 - 3s - 3s/epoch - 393ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  43.3s\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 0.3888 - val_loss: 0.4352 - lr: 0.0100 - 22s/epoch - 867ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5758 - val_loss: 0.4442 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6564 - val_loss: 0.3577 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9275 - val_loss: 0.5569 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 1.2134 - val_loss: 1.0639 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9364 - val_loss: 1.3920 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8407 - val_loss: 1.5795 - lr: 0.0020 - 1s/epoch - 47ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7922 - val_loss: 1.6178 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7861 - val_loss: 1.6557 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7805 - val_loss: 1.6909 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7716 - val_loss: 1.6983 - lr: 8.0000e-05 - 1s/epoch - 50ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7705 - val_loss: 1.7058 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7696 - val_loss: 1.7134 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7677 - val_loss: 1.7149 - lr: 1.6000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7673 - val_loss: 1.7165 - lr: 1.6000e-05 - 1s/epoch - 50ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7674 - val_loss: 1.7181 - lr: 1.6000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7668 - val_loss: 1.7184 - lr: 3.2000e-06 - 1s/epoch - 47ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7668 - val_loss: 1.7188 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7668 - val_loss: 1.7191 - lr: 3.2000e-06 - 1s/epoch - 50ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7667 - val_loss: 1.7192 - lr: 6.4000e-07 - 1s/epoch - 50ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.7666 - val_loss: 1.7192 - lr: 6.4000e-07 - 1s/epoch - 49ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.7666 - val_loss: 1.7193 - lr: 6.4000e-07 - 1s/epoch - 49ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "25/25 - 1s - loss: 0.7666 - val_loss: 1.7193 - lr: 1.2800e-07 - 1s/epoch - 49ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  50.7s\n",
      "7/7 - 3s - 3s/epoch - 397ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  53.4s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.3414 - val_loss: 0.4529 - lr: 0.0100 - 21s/epoch - 860ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6460 - val_loss: 0.3973 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0180 - val_loss: 0.5347 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.0078 - val_loss: 1.2548 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8923 - val_loss: 1.4105 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8502 - val_loss: 1.5480 - lr: 0.0020 - 1s/epoch - 51ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8217 - val_loss: 1.6506 - lr: 0.0020 - 1s/epoch - 48ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7954 - val_loss: 1.6721 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7922 - val_loss: 1.6938 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7891 - val_loss: 1.7148 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7838 - val_loss: 1.7191 - lr: 8.0000e-05 - 1s/epoch - 51ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7833 - val_loss: 1.7235 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7827 - val_loss: 1.7280 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7815 - val_loss: 1.7289 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7815 - val_loss: 1.7298 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7813 - val_loss: 1.7308 - lr: 1.6000e-05 - 1s/epoch - 51ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7810 - val_loss: 1.7309 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7811 - val_loss: 1.7311 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7810 - val_loss: 1.7314 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7810 - val_loss: 1.7314 - lr: 6.4000e-07 - 1s/epoch - 48ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.7809 - val_loss: 1.7314 - lr: 6.4000e-07 - 1s/epoch - 51ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.7810 - val_loss: 1.7315 - lr: 6.4000e-07 - 1s/epoch - 49ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  49.4s\n",
      "7/7 - 3s - 3s/epoch - 378ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  52.0s\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 0.5202 - val_loss: 0.4503 - lr: 0.0100 - 22s/epoch - 863ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6040 - val_loss: 0.3763 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0661 - val_loss: 0.9484 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.0958 - val_loss: 1.4158 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9905 - val_loss: 1.5323 - lr: 0.0020 - 1s/epoch - 48ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9583 - val_loss: 1.6495 - lr: 0.0020 - 1s/epoch - 48ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.9315 - val_loss: 1.7564 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 8/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.9014 - val_loss: 1.7800 - lr: 4.0000e-04 - 1s/epoch - 50ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8972 - val_loss: 1.8043 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8931 - val_loss: 1.8285 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8865 - val_loss: 1.8335 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8860 - val_loss: 1.8386 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8852 - val_loss: 1.8437 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8837 - val_loss: 1.8447 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8836 - val_loss: 1.8458 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8834 - val_loss: 1.8468 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8831 - val_loss: 1.8470 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8832 - val_loss: 1.8472 - lr: 3.2000e-06 - 1s/epoch - 49ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8830 - val_loss: 1.8474 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8829 - val_loss: 1.8475 - lr: 6.4000e-07 - 1s/epoch - 49ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.8830 - val_loss: 1.8475 - lr: 6.4000e-07 - 1s/epoch - 48ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.8829 - val_loss: 1.8476 - lr: 6.4000e-07 - 1s/epoch - 50ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  49.3s\n",
      "7/7 - 3s - 3s/epoch - 386ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  52.0s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.3820 - val_loss: 0.3316 - lr: 0.0100 - 21s/epoch - 833ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3110 - val_loss: 0.3343 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3133 - val_loss: 0.2706 - lr: 0.0100 - 1s/epoch - 48ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5291 - val_loss: 0.3844 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.8996 - val_loss: 0.3267 - lr: 0.0100 - 1s/epoch - 48ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.5019 - val_loss: 0.9888 - lr: 0.0020 - 1s/epoch - 47ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8678 - val_loss: 0.7088 - lr: 0.0020 - 1s/epoch - 48ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 1.2783 - val_loss: 1.8078 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9523 - val_loss: 1.9241 - lr: 4.0000e-04 - 1s/epoch - 50ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.9272 - val_loss: 2.0303 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.9070 - val_loss: 2.1197 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8850 - val_loss: 2.1377 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8820 - val_loss: 2.1551 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8789 - val_loss: 2.1708 - lr: 8.0000e-05 - 1s/epoch - 50ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8744 - val_loss: 2.1737 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8734 - val_loss: 2.1766 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8728 - val_loss: 2.1794 - lr: 1.6000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8717 - val_loss: 2.1800 - lr: 3.2000e-06 - 1s/epoch - 47ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8716 - val_loss: 2.1805 - lr: 3.2000e-06 - 1s/epoch - 51ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8716 - val_loss: 2.1811 - lr: 3.2000e-06 - 1s/epoch - 49ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.8712 - val_loss: 2.1812 - lr: 6.4000e-07 - 1s/epoch - 50ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.8711 - val_loss: 2.1813 - lr: 6.4000e-07 - 1s/epoch - 49ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.8711 - val_loss: 2.1815 - lr: 6.4000e-07 - 1s/epoch - 51ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  50.6s\n",
      "7/7 - 3s - 3s/epoch - 395ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  53.4s\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 0.2463 - val_loss: 1.4940 - lr: 0.0100 - 22s/epoch - 865ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2349 - val_loss: 1.6524 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2385 - val_loss: 2.0302 - lr: 0.0100 - 1s/epoch - 57ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2424 - val_loss: 2.3539 - lr: 0.0100 - 1s/epoch - 60ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2430 - val_loss: 2.5252 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3098 - val_loss: 3.0552 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2414 - val_loss: 3.3713 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2177 - val_loss: 3.4787 - lr: 0.0020 - 1s/epoch - 58ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2114 - val_loss: 3.5010 - lr: 0.0020 - 1s/epoch - 54ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2101 - val_loss: 3.5026 - lr: 0.0020 - 1s/epoch - 51ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2100 - val_loss: 3.5015 - lr: 0.0020 - 1s/epoch - 52ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2100 - val_loss: 3.5010 - lr: 0.0020 - 1s/epoch - 52ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2102 - val_loss: 3.5011 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2102 - val_loss: 3.5017 - lr: 0.0020 - 1s/epoch - 51ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2082 - val_loss: 3.5366 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2067 - val_loss: 3.5737 - lr: 4.0000e-04 - 1s/epoch - 53ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2051 - val_loss: 3.6046 - lr: 4.0000e-04 - 1s/epoch - 53ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2039 - val_loss: 3.6295 - lr: 4.0000e-04 - 1s/epoch - 56ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2031 - val_loss: 3.6493 - lr: 4.0000e-04 - 1s/epoch - 52ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2024 - val_loss: 3.6648 - lr: 4.0000e-04 - 1s/epoch - 51ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 3.6771 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  50.8s\n",
      "7/7 - 3s - 3s/epoch - 424ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  53.7s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7627 - val_loss: 1.8313 - lr: 0.0100 - 20s/epoch - 818ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7560 - val_loss: 1.8453 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7545 - val_loss: 1.8537 - lr: 0.0100 - 980ms/epoch - 39ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7538 - val_loss: 1.8600 - lr: 0.0100 - 990ms/epoch - 40ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7532 - val_loss: 1.8650 - lr: 0.0100 - 990ms/epoch - 40ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7528 - val_loss: 1.8693 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7524 - val_loss: 1.8730 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7521 - val_loss: 1.8764 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7519 - val_loss: 1.8794 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7516 - val_loss: 1.8822 - lr: 0.0100 - 975ms/epoch - 39ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7514 - val_loss: 1.8847 - lr: 0.0100 - 986ms/epoch - 39ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7513 - val_loss: 1.8871 - lr: 0.0100 - 958ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7511 - val_loss: 1.8894 - lr: 0.0100 - 959ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7509 - val_loss: 1.8915 - lr: 0.0100 - 978ms/epoch - 39ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7508 - val_loss: 1.8935 - lr: 0.0100 - 979ms/epoch - 39ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7506 - val_loss: 1.8954 - lr: 0.0100 - 974ms/epoch - 39ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7505 - val_loss: 1.8972 - lr: 0.0100 - 986ms/epoch - 39ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7504 - val_loss: 1.8989 - lr: 0.0100 - 978ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7503 - val_loss: 1.9006 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7502 - val_loss: 1.9022 - lr: 0.0100 - 963ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7501 - val_loss: 1.9037 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.2s\n",
      "7/7 - 3s - 3s/epoch - 388ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  45.8s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7783 - val_loss: 1.8300 - lr: 0.0100 - 20s/epoch - 781ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7709 - val_loss: 1.8452 - lr: 0.0100 - 955ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7693 - val_loss: 1.8545 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7684 - val_loss: 1.8615 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7677 - val_loss: 1.8673 - lr: 0.0100 - 954ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7672 - val_loss: 1.8721 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7668 - val_loss: 1.8764 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7664 - val_loss: 1.8803 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7661 - val_loss: 1.8838 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7658 - val_loss: 1.8870 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7656 - val_loss: 1.8900 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7653 - val_loss: 1.8928 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7651 - val_loss: 1.8954 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7649 - val_loss: 1.8979 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7648 - val_loss: 1.9002 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7646 - val_loss: 1.9025 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7644 - val_loss: 1.9046 - lr: 0.0100 - 988ms/epoch - 40ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7643 - val_loss: 1.9067 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7641 - val_loss: 1.9086 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7640 - val_loss: 1.9105 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7638 - val_loss: 1.9124 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.0s\n",
      "7/7 - 3s - 3s/epoch - 383ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  43.6s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.8940 - val_loss: 1.8495 - lr: 0.0100 - 20s/epoch - 816ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8824 - val_loss: 1.8732 - lr: 0.0100 - 1s/epoch - 48ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8789 - val_loss: 1.8891 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8767 - val_loss: 1.9014 - lr: 0.0100 - 965ms/epoch - 39ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8749 - val_loss: 1.9118 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8735 - val_loss: 1.9209 - lr: 0.0100 - 958ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8723 - val_loss: 1.9290 - lr: 0.0100 - 981ms/epoch - 39ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8713 - val_loss: 1.9364 - lr: 0.0100 - 983ms/epoch - 39ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8703 - val_loss: 1.9433 - lr: 0.0100 - 988ms/epoch - 40ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8694 - val_loss: 1.9497 - lr: 0.0100 - 958ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8687 - val_loss: 1.9558 - lr: 0.0100 - 958ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8679 - val_loss: 1.9616 - lr: 0.0100 - 970ms/epoch - 39ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8672 - val_loss: 1.9671 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8666 - val_loss: 1.9724 - lr: 0.0100 - 970ms/epoch - 39ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8660 - val_loss: 1.9776 - lr: 0.0100 - 999ms/epoch - 40ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8654 - val_loss: 1.9826 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8648 - val_loss: 1.9875 - lr: 0.0100 - 1s/epoch - 46ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8643 - val_loss: 1.9924 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8637 - val_loss: 1.9971 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8632 - val_loss: 2.0018 - lr: 0.0100 - 977ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8627 - val_loss: 2.0065 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.0s\n",
      "7/7 - 3s - 3s/epoch - 473ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  46.3s\n",
      "Epoch 1/500\n",
      "25/25 - 31s - loss: 0.9511 - val_loss: 1.8686 - lr: 0.0100 - 31s/epoch - 1s/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9319 - val_loss: 1.9033 - lr: 0.0100 - 1s/epoch - 52ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9248 - val_loss: 1.9275 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.9199 - val_loss: 1.9470 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9160 - val_loss: 1.9637 - lr: 0.0100 - 1s/epoch - 56ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9127 - val_loss: 1.9787 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.9099 - val_loss: 1.9924 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.9073 - val_loss: 2.0053 - lr: 0.0100 - 973ms/epoch - 39ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9049 - val_loss: 2.0176 - lr: 0.0100 - 967ms/epoch - 39ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.9027 - val_loss: 2.0296 - lr: 0.0100 - 996ms/epoch - 40ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.9005 - val_loss: 2.0415 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8985 - val_loss: 2.0533 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8964 - val_loss: 2.0652 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8944 - val_loss: 2.0774 - lr: 0.0100 - 961ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8924 - val_loss: 2.0899 - lr: 0.0100 - 956ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8904 - val_loss: 2.1027 - lr: 0.0100 - 935ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8884 - val_loss: 2.1159 - lr: 0.0100 - 980ms/epoch - 39ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8865 - val_loss: 2.1295 - lr: 0.0100 - 1s/epoch - 45ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8845 - val_loss: 2.1434 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8825 - val_loss: 2.1575 - lr: 0.0100 - 968ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8805 - val_loss: 2.1719 - lr: 0.0100 - 963ms/epoch - 39ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  55.6s\n",
      "7/7 - 3s - 3s/epoch - 389ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  58.3s\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 0.6004 - val_loss: 1.9220 - lr: 0.0100 - 22s/epoch - 898ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5477 - val_loss: 2.0050 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5157 - val_loss: 2.0704 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4905 - val_loss: 2.1284 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4686 - val_loss: 2.1844 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4477 - val_loss: 2.2431 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.4262 - val_loss: 2.3096 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.4027 - val_loss: 2.3895 - lr: 0.0100 - 1s/epoch - 46ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3763 - val_loss: 2.4873 - lr: 0.0100 - 1s/epoch - 46ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3474 - val_loss: 2.6045 - lr: 0.0100 - 989ms/epoch - 40ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3174 - val_loss: 2.7384 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2888 - val_loss: 2.8814 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2640 - val_loss: 3.0233 - lr: 0.0100 - 983ms/epoch - 39ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2444 - val_loss: 3.1543 - lr: 0.0100 - 969ms/epoch - 39ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2301 - val_loss: 3.2674 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2202 - val_loss: 3.3599 - lr: 0.0100 - 976ms/epoch - 39ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2137 - val_loss: 3.4324 - lr: 0.0100 - 993ms/epoch - 40ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2095 - val_loss: 3.4876 - lr: 0.0100 - 971ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2068 - val_loss: 3.5291 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2050 - val_loss: 3.5600 - lr: 0.0100 - 972ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2038 - val_loss: 3.5831 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  46.2s\n",
      "7/7 - 4s - 4s/epoch - 542ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=50, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  50.0s\n",
      "Epoch 1/500\n",
      "25/25 - 31s - loss: 0.6069 - val_loss: 0.8363 - lr: 0.0100 - 31s/epoch - 1s/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7412 - val_loss: 0.7515 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6752 - val_loss: 0.5299 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4519 - val_loss: 0.2045 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1694 - val_loss: 0.1959 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0888 - val_loss: 0.1930 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0761 - val_loss: 0.1884 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0746 - val_loss: 0.1858 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0734 - val_loss: 0.1818 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0737 - val_loss: 0.1800 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0736 - val_loss: 0.1773 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0728 - val_loss: 0.1758 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0726 - val_loss: 0.1733 - lr: 0.0100 - 982ms/epoch - 39ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0729 - val_loss: 0.1724 - lr: 0.0100 - 953ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0735 - val_loss: 0.1725 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0736 - val_loss: 0.1696 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0796 - val_loss: 0.1646 - lr: 0.0020 - 927ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0689 - val_loss: 0.1641 - lr: 0.0020 - 889ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0672 - val_loss: 0.1639 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0661 - val_loss: 0.1637 - lr: 0.0020 - 983ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0662 - val_loss: 0.1633 - lr: 0.0020 - 921ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0663 - val_loss: 0.1634 - lr: 0.0020 - 901ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0660 - val_loss: 0.1627 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0649 - val_loss: 0.1628 - lr: 0.0020 - 927ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0647 - val_loss: 0.1630 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0650 - val_loss: 0.1627 - lr: 0.0020 - 945ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1626 - lr: 0.0020 - 965ms/epoch - 39ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0642 - val_loss: 0.1621 - lr: 0.0020 - 922ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1623 - lr: 0.0020 - 933ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1618 - lr: 0.0020 - 929ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0634 - val_loss: 0.1620 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0634 - val_loss: 0.1618 - lr: 0.0020 - 931ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0634 - val_loss: 0.1609 - lr: 0.0020 - 973ms/epoch - 39ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.1609 - lr: 0.0020 - 917ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0630 - val_loss: 0.1604 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.1601 - lr: 0.0020 - 916ms/epoch - 37ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1605 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.1601 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0622 - val_loss: 0.1597 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.1596 - lr: 0.0020 - 939ms/epoch - 38ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1593 - lr: 0.0020 - 923ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1593 - lr: 0.0020 - 904ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1590 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1588 - lr: 0.0020 - 919ms/epoch - 37ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1583 - lr: 0.0020 - 970ms/epoch - 39ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1579 - lr: 0.0020 - 1s/epoch - 47ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1582 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1584 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1583 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1580 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.1577 - lr: 0.0020 - 930ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1576 - lr: 0.0020 - 943ms/epoch - 38ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.1572 - lr: 4.0000e-04 - 947ms/epoch - 38ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.1570 - lr: 4.0000e-04 - 909ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.1568 - lr: 4.0000e-04 - 970ms/epoch - 39ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1568 - lr: 4.0000e-04 - 996ms/epoch - 40ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1568 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1568 - lr: 4.0000e-04 - 900ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1570 - lr: 4.0000e-04 - 956ms/epoch - 38ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1571 - lr: 4.0000e-04 - 897ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1572 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1572 - lr: 4.0000e-04 - 924ms/epoch - 37ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1573 - lr: 4.0000e-04 - 913ms/epoch - 37ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0590 - val_loss: 0.1573 - lr: 4.0000e-04 - 881ms/epoch - 35ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1573 - lr: 4.0000e-04 - 917ms/epoch - 37ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1573 - lr: 4.0000e-04 - 936ms/epoch - 37ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1572 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1572 - lr: 4.0000e-04 - 921ms/epoch - 37ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.1572 - lr: 8.0000e-05 - 926ms/epoch - 37ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.1572 - lr: 8.0000e-05 - 943ms/epoch - 38ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1572 - lr: 8.0000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0580 - val_loss: 0.1572 - lr: 8.0000e-05 - 939ms/epoch - 38ms/step\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.1572 - lr: 8.0000e-05 - 944ms/epoch - 38ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0576 - val_loss: 0.1572 - lr: 1.6000e-05 - 899ms/epoch - 36ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.1572 - lr: 1.6000e-05 - 957ms/epoch - 38ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.1572 - lr: 1.6000e-05 - 932ms/epoch - 37ms/step\n",
      "Epoch 77/500\n",
      "Restoring model weights from the end of the best epoch: 57.\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.1572 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 77: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.7min\n",
      "7/7 - 3s - 3s/epoch - 480ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.8min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.5735 - val_loss: 0.7091 - lr: 0.0100 - 20s/epoch - 817ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6200 - val_loss: 0.3793 - lr: 0.0100 - 957ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3137 - val_loss: 0.1925 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1107 - val_loss: 0.1958 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0763 - val_loss: 0.1895 - lr: 0.0100 - 878ms/epoch - 35ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0709 - val_loss: 0.1872 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0696 - val_loss: 0.1829 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0684 - val_loss: 0.1805 - lr: 0.0100 - 881ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0678 - val_loss: 0.1777 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0676 - val_loss: 0.1763 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0676 - val_loss: 0.1736 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0675 - val_loss: 0.1720 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0678 - val_loss: 0.1721 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0678 - val_loss: 0.1693 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0672 - val_loss: 0.1680 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0676 - val_loss: 0.1682 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0673 - val_loss: 0.1676 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0677 - val_loss: 0.1665 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0753 - val_loss: 0.1614 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0638 - val_loss: 0.1610 - lr: 0.0020 - 921ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1613 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1610 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0607 - val_loss: 0.1609 - lr: 0.0020 - 922ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.1603 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1601 - lr: 0.0020 - 873ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0603 - val_loss: 0.1596 - lr: 0.0020 - 880ms/epoch - 35ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.1601 - lr: 0.0020 - 917ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1597 - lr: 0.0020 - 914ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1589 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.1582 - lr: 0.0020 - 883ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1583 - lr: 0.0020 - 871ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1582 - lr: 0.0020 - 908ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1583 - lr: 0.0020 - 887ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.1581 - lr: 0.0020 - 954ms/epoch - 38ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0576 - val_loss: 0.1572 - lr: 0.0020 - 905ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.1572 - lr: 0.0020 - 871ms/epoch - 35ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.1568 - lr: 0.0020 - 886ms/epoch - 35ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.1569 - lr: 0.0020 - 873ms/epoch - 35ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1565 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.1558 - lr: 0.0020 - 922ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1562 - lr: 0.0020 - 923ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.1559 - lr: 0.0020 - 878ms/epoch - 35ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1558 - lr: 0.0020 - 854ms/epoch - 34ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1556 - lr: 0.0020 - 896ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1558 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.1552 - lr: 0.0020 - 888ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1548 - lr: 0.0020 - 935ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.1545 - lr: 0.0020 - 925ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1542 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1541 - lr: 4.0000e-04 - 901ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1541 - lr: 4.0000e-04 - 888ms/epoch - 36ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1541 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1541 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1542 - lr: 4.0000e-04 - 925ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1544 - lr: 4.0000e-04 - 907ms/epoch - 36ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1545 - lr: 4.0000e-04 - 895ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0542 - val_loss: 0.1546 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1548 - lr: 4.0000e-04 - 898ms/epoch - 36ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1547 - lr: 8.0000e-05 - 894ms/epoch - 36ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.1547 - lr: 8.0000e-05 - 877ms/epoch - 35ms/step\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1547 - lr: 8.0000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1547 - lr: 1.6000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1547 - lr: 1.6000e-05 - 858ms/epoch - 34ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1547 - lr: 1.6000e-05 - 914ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1547 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1547 - lr: 1.6000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1547 - lr: 3.2000e-06 - 921ms/epoch - 37ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1547 - lr: 3.2000e-06 - 949ms/epoch - 38ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1547 - lr: 3.2000e-06 - 892ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1547 - lr: 3.2000e-06 - 877ms/epoch - 35ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.1547 - lr: 3.2000e-06 - 878ms/epoch - 35ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1547 - lr: 3.2000e-06 - 888ms/epoch - 36ms/step\n",
      "Epoch 73/500\n",
      "Restoring model weights from the end of the best epoch: 53.\n",
      "25/25 - 1s - loss: 0.0529 - val_loss: 0.1547 - lr: 3.2000e-06 - 914ms/epoch - 37ms/step\n",
      "Epoch 73: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.5min\n",
      "7/7 - 4s - 4s/epoch - 512ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.5min\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 0.6290 - val_loss: 0.7304 - lr: 0.0100 - 22s/epoch - 884ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7083 - val_loss: 0.4299 - lr: 0.0100 - 986ms/epoch - 39ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3665 - val_loss: 0.1952 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0917 - val_loss: 0.1983 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0318 - val_loss: 0.1914 - lr: 0.0100 - 999ms/epoch - 40ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1871 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1834 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0274 - val_loss: 0.1822 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.1819 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0286 - val_loss: 0.1793 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0356 - val_loss: 0.1675 - lr: 0.0020 - 965ms/epoch - 39ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0318 - val_loss: 0.1644 - lr: 0.0020 - 910ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0314 - val_loss: 0.1643 - lr: 0.0020 - 912ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0313 - val_loss: 0.1640 - lr: 4.0000e-04 - 910ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0307 - val_loss: 0.1639 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1641 - lr: 4.0000e-04 - 895ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0298 - val_loss: 0.1641 - lr: 8.0000e-05 - 904ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0306 - val_loss: 0.1641 - lr: 8.0000e-05 - 930ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 0.1641 - lr: 8.0000e-05 - 886ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0302 - val_loss: 0.1641 - lr: 1.6000e-05 - 883ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1641 - lr: 1.6000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0293 - val_loss: 0.1641 - lr: 1.6000e-05 - 875ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0294 - val_loss: 0.1641 - lr: 3.2000e-06 - 907ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0302 - val_loss: 0.1641 - lr: 3.2000e-06 - 932ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1641 - lr: 3.2000e-06 - 924ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1641 - lr: 6.4000e-07 - 910ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0303 - val_loss: 0.1641 - lr: 6.4000e-07 - 887ms/epoch - 35ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 0.1641 - lr: 6.4000e-07 - 877ms/epoch - 35ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1641 - lr: 1.2800e-07 - 920ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1641 - lr: 1.2800e-07 - 922ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0294 - val_loss: 0.1641 - lr: 1.2800e-07 - 934ms/epoch - 37ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1641 - lr: 2.5600e-08 - 893ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1641 - lr: 2.5600e-08 - 877ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1641 - lr: 2.5600e-08 - 902ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "25/25 - 1s - loss: 0.0294 - val_loss: 0.1641 - lr: 5.1200e-09 - 911ms/epoch - 36ms/step\n",
      "Epoch 35: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  56.2s\n",
      "7/7 - 3s - 3s/epoch - 430ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time=  59.2s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.7202 - val_loss: 1.0172 - lr: 0.0100 - 21s/epoch - 831ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8345 - val_loss: 0.9255 - lr: 0.0100 - 997ms/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7173 - val_loss: 0.4997 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3385 - val_loss: 0.1905 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0808 - val_loss: 0.1854 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1799 - lr: 0.0100 - 991ms/epoch - 40ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0535 - val_loss: 0.1768 - lr: 0.0100 - 994ms/epoch - 40ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0524 - val_loss: 0.1747 - lr: 0.0100 - 949ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0516 - val_loss: 0.1735 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1704 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.1709 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0510 - val_loss: 0.1687 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0508 - val_loss: 0.1671 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1692 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.1637 - lr: 0.0020 - 982ms/epoch - 39ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1615 - lr: 0.0020 - 985ms/epoch - 39ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0500 - val_loss: 0.1605 - lr: 0.0020 - 949ms/epoch - 38ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0501 - val_loss: 0.1604 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1598 - lr: 0.0020 - 902ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0507 - val_loss: 0.1595 - lr: 0.0020 - 874ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.1593 - lr: 0.0020 - 940ms/epoch - 38ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1589 - lr: 0.0020 - 908ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0503 - val_loss: 0.1589 - lr: 0.0020 - 949ms/epoch - 38ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1584 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1584 - lr: 4.0000e-04 - 923ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.1584 - lr: 4.0000e-04 - 912ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1584 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0492 - val_loss: 0.1584 - lr: 4.0000e-04 - 964ms/epoch - 39ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1585 - lr: 4.0000e-04 - 969ms/epoch - 39ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1585 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0494 - val_loss: 0.1587 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1585 - lr: 4.0000e-04 - 890ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1586 - lr: 4.0000e-04 - 857ms/epoch - 34ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0490 - val_loss: 0.1586 - lr: 4.0000e-04 - 909ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1585 - lr: 4.0000e-04 - 920ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1585 - lr: 8.0000e-05 - 876ms/epoch - 35ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.1585 - lr: 8.0000e-05 - 871ms/epoch - 35ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1585 - lr: 8.0000e-05 - 906ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0486 - val_loss: 0.1585 - lr: 8.0000e-05 - 883ms/epoch - 35ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0482 - val_loss: 0.1585 - lr: 8.0000e-05 - 887ms/epoch - 35ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1585 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.1585 - lr: 8.0000e-05 - 938ms/epoch - 38ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0489 - val_loss: 0.1585 - lr: 8.0000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1585 - lr: 1.6000e-05 - 893ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1585 - lr: 1.6000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0487 - val_loss: 0.1585 - lr: 1.6000e-05 - 884ms/epoch - 35ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0484 - val_loss: 0.1585 - lr: 3.2000e-06 - 893ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.1585 - lr: 3.2000e-06 - 945ms/epoch - 38ms/step\n",
      "Epoch 48: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 3s - 3s/epoch - 469ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.2min\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.2895 - val_loss: 2.4242 - lr: 0.0100 - 21s/epoch - 836ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2066 - val_loss: 2.4128 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2014 - val_loss: 2.3747 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1972 - val_loss: 2.3200 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1918 - val_loss: 2.2430 - lr: 0.0100 - 958ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1852 - val_loss: 2.1355 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1764 - val_loss: 1.9878 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1651 - val_loss: 1.7956 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1496 - val_loss: 1.5491 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1309 - val_loss: 1.2671 - lr: 0.0100 - 866ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1101 - val_loss: 0.9928 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0915 - val_loss: 0.7790 - lr: 0.0100 - 982ms/epoch - 39ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0785 - val_loss: 0.6447 - lr: 0.0100 - 1s/epoch - 53ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0717 - val_loss: 0.5733 - lr: 0.0100 - 1s/epoch - 53ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0688 - val_loss: 0.5360 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0676 - val_loss: 0.5164 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0667 - val_loss: 0.5046 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0662 - val_loss: 0.4955 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0659 - val_loss: 0.4894 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0655 - val_loss: 0.4843 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.4782 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.4728 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0652 - val_loss: 0.4693 - lr: 0.0100 - 992ms/epoch - 40ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0651 - val_loss: 0.4649 - lr: 0.0100 - 978ms/epoch - 39ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0649 - val_loss: 0.4604 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0644 - val_loss: 0.4566 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0638 - val_loss: 0.4513 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0643 - val_loss: 0.4490 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0643 - val_loss: 0.4453 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.4421 - lr: 0.0100 - 949ms/epoch - 38ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0710 - val_loss: 0.4344 - lr: 0.0020 - 924ms/epoch - 37ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.4241 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0577 - val_loss: 0.4157 - lr: 0.0020 - 880ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.4086 - lr: 0.0020 - 920ms/epoch - 37ms/step\n",
      "Epoch 35/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.4028 - lr: 0.0020 - 934ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.3986 - lr: 0.0020 - 939ms/epoch - 38ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.3953 - lr: 0.0020 - 995ms/epoch - 40ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.3926 - lr: 0.0020 - 932ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.3904 - lr: 0.0020 - 991ms/epoch - 40ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.3902 - lr: 4.0000e-04 - 955ms/epoch - 38ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.3897 - lr: 4.0000e-04 - 899ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.3891 - lr: 4.0000e-04 - 1s/epoch - 53ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.3886 - lr: 4.0000e-04 - 1s/epoch - 47ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.3879 - lr: 4.0000e-04 - 1s/epoch - 55ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 2s - loss: 0.0555 - val_loss: 0.3874 - lr: 4.0000e-04 - 2s/epoch - 60ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.3868 - lr: 4.0000e-04 - 1s/epoch - 55ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3867 - lr: 8.0000e-05 - 1s/epoch - 56ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3866 - lr: 8.0000e-05 - 1s/epoch - 46ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.3865 - lr: 8.0000e-05 - 1s/epoch - 46ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.3863 - lr: 8.0000e-05 - 1s/epoch - 53ms/step\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3862 - lr: 8.0000e-05 - 1s/epoch - 53ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3862 - lr: 1.6000e-05 - 1s/epoch - 56ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3862 - lr: 1.6000e-05 - 1s/epoch - 54ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3862 - lr: 1.6000e-05 - 1s/epoch - 56ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.3861 - lr: 1.6000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 1.6000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 2s - loss: 0.0554 - val_loss: 0.3861 - lr: 1.6000e-05 - 2s/epoch - 61ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.3861 - lr: 1.6000e-05 - 1s/epoch - 55ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.3861 - lr: 3.2000e-06 - 1s/epoch - 54ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 3.2000e-06 - 1s/epoch - 52ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 3.2000e-06 - 1s/epoch - 51ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3861 - lr: 3.2000e-06 - 1s/epoch - 54ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3861 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3861 - lr: 6.4000e-07 - 979ms/epoch - 39ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3861 - lr: 6.4000e-07 - 920ms/epoch - 37ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3861 - lr: 1.2800e-07 - 964ms/epoch - 39ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3861 - lr: 1.2800e-07 - 886ms/epoch - 35ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3861 - lr: 1.2800e-07 - 893ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.3861 - lr: 2.5600e-08 - 893ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 2.5600e-08 - 940ms/epoch - 38ms/step\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.3861 - lr: 2.5600e-08 - 913ms/epoch - 37ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.3861 - lr: 5.1200e-09 - 905ms/epoch - 36ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 5.1200e-09 - 925ms/epoch - 37ms/step\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3861 - lr: 5.1200e-09 - 913ms/epoch - 37ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 1.0240e-09 - 1s/epoch - 45ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.3861 - lr: 1.0240e-09 - 1s/epoch - 48ms/step\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 1.0240e-09 - 1s/epoch - 44ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 2.0480e-10 - 933ms/epoch - 37ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.3861 - lr: 2.0480e-10 - 950ms/epoch - 38ms/step\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3861 - lr: 2.0480e-10 - 1s/epoch - 40ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.3861 - lr: 4.0960e-11 - 1s/epoch - 41ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.3861 - lr: 4.0960e-11 - 992ms/epoch - 40ms/step\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.3861 - lr: 4.0960e-11 - 1s/epoch - 42ms/step\n",
      "Epoch 84/500\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.3861 - lr: 8.1920e-12 - 954ms/epoch - 38ms/step\n",
      "Epoch 84: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.9min\n",
      "7/7 - 3s - 3s/epoch - 451ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.gradient_descent.SGD object at 0x000001BE9E76DF70>; total time= 1.9min\n",
      "Epoch 1/500\n",
      "25/25 - 23s - loss: 2.1556 - val_loss: 0.3224 - lr: 0.0100 - 23s/epoch - 909ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3530 - val_loss: 0.3246 - lr: 0.0100 - 1s/epoch - 46ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4011 - val_loss: 0.3073 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5151 - val_loss: 0.4202 - lr: 0.0100 - 1s/epoch - 46ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.4739 - val_loss: 0.3208 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.4577 - val_loss: 0.3068 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8017 - val_loss: 0.4187 - lr: 0.0020 - 1s/epoch - 45ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7339 - val_loss: 0.2447 - lr: 0.0020 - 1s/epoch - 47ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 1.2341 - val_loss: 0.2042 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.5937 - val_loss: 0.2912 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.3016 - val_loss: 0.3115 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1969 - val_loss: 0.2653 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1648 - val_loss: 0.2323 - lr: 4.0000e-04 - 1s/epoch - 46ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1562 - val_loss: 0.2652 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1542 - val_loss: 0.2587 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1521 - val_loss: 0.2655 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1444 - val_loss: 0.2849 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1371 - val_loss: 0.2654 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1238 - val_loss: 0.2653 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1240 - val_loss: 0.2623 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1180 - val_loss: 0.2634 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1108 - val_loss: 0.2728 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.1219 - val_loss: 0.2635 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.1145 - val_loss: 0.2584 - lr: 4.0000e-04 - 1s/epoch - 45ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.1066 - val_loss: 0.2704 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.1172 - val_loss: 0.2569 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.1043 - val_loss: 0.2658 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.1110 - val_loss: 0.2642 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 29/500\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "25/25 - 1s - loss: 0.1016 - val_loss: 0.2661 - lr: 4.0000e-04 - 1s/epoch - 47ms/step\n",
      "Epoch 29: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  56.0s\n",
      "7/7 - 3s - 3s/epoch - 420ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  58.9s\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 1.8082 - val_loss: 0.3088 - lr: 0.0100 - 22s/epoch - 862ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4510 - val_loss: 0.3046 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4650 - val_loss: 0.2662 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6453 - val_loss: 0.3497 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2947 - val_loss: 0.3234 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3907 - val_loss: 0.3327 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.3430 - val_loss: 0.3234 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.6425 - val_loss: 0.3498 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 1.8042 - val_loss: 0.5511 - lr: 0.0020 - 1s/epoch - 45ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8139 - val_loss: 0.4162 - lr: 0.0020 - 1s/epoch - 45ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.4892 - val_loss: 0.2518 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.3500 - val_loss: 0.2435 - lr: 4.0000e-04 - 1s/epoch - 46ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1953 - val_loss: 0.2397 - lr: 4.0000e-04 - 1s/epoch - 46ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1552 - val_loss: 0.2329 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1432 - val_loss: 0.2349 - lr: 4.0000e-04 - 1s/epoch - 46ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1398 - val_loss: 0.2402 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1337 - val_loss: 0.2440 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1306 - val_loss: 0.2361 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1275 - val_loss: 0.2477 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1227 - val_loss: 0.2419 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.1217 - val_loss: 0.2394 - lr: 4.0000e-04 - 1s/epoch - 46ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.1183 - val_loss: 0.2403 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.1190 - val_loss: 0.2390 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.1143 - val_loss: 0.2331 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.1203 - val_loss: 0.2358 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.1196 - val_loss: 0.2424 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.1142 - val_loss: 0.2288 - lr: 4.0000e-04 - 1s/epoch - 45ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.1171 - val_loss: 0.2393 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.1155 - val_loss: 0.2437 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.1154 - val_loss: 0.2171 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.1173 - val_loss: 0.2095 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.1074 - val_loss: 0.2065 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.1037 - val_loss: 0.2090 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.1012 - val_loss: 0.2071 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.1012 - val_loss: 0.2042 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0979 - val_loss: 0.2011 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0989 - val_loss: 0.1986 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0975 - val_loss: 0.2030 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0971 - val_loss: 0.2053 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0959 - val_loss: 0.2052 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0965 - val_loss: 0.1996 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0950 - val_loss: 0.2043 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0953 - val_loss: 0.2010 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0955 - val_loss: 0.1983 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0948 - val_loss: 0.1976 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0948 - val_loss: 0.2005 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0942 - val_loss: 0.2003 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0946 - val_loss: 0.1980 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0946 - val_loss: 0.2006 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0936 - val_loss: 0.1996 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0927 - val_loss: 0.1979 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0925 - val_loss: 0.1989 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0927 - val_loss: 0.2010 - lr: 8.0000e-05 - 1s/epoch - 46ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0928 - val_loss: 0.1973 - lr: 8.0000e-05 - 1s/epoch - 46ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0898 - val_loss: 0.1994 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0895 - val_loss: 0.2015 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0889 - val_loss: 0.2009 - lr: 8.0000e-05 - 1s/epoch - 50ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0901 - val_loss: 0.2026 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0894 - val_loss: 0.2022 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0877 - val_loss: 0.1983 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0891 - val_loss: 0.1966 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0881 - val_loss: 0.1991 - lr: 8.0000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0883 - val_loss: 0.2011 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0850 - val_loss: 0.1998 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0854 - val_loss: 0.1981 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0854 - val_loss: 0.1971 - lr: 1.6000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0845 - val_loss: 0.1962 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0842 - val_loss: 0.1955 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0842 - val_loss: 0.1955 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0840 - val_loss: 0.1957 - lr: 1.6000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0832 - val_loss: 0.1958 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0836 - val_loss: 0.1968 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0823 - val_loss: 0.1969 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0815 - val_loss: 0.1963 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0816 - val_loss: 0.1962 - lr: 1.6000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0792 - val_loss: 0.1976 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0803 - val_loss: 0.1977 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0790 - val_loss: 0.1963 - lr: 1.6000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0790 - val_loss: 0.1966 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0786 - val_loss: 0.1959 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0785 - val_loss: 0.1953 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0768 - val_loss: 0.1951 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0756 - val_loss: 0.1957 - lr: 1.6000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0747 - val_loss: 0.1956 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0761 - val_loss: 0.1948 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0746 - val_loss: 0.1957 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0738 - val_loss: 0.1949 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0742 - val_loss: 0.1950 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0720 - val_loss: 0.1951 - lr: 1.6000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0712 - val_loss: 0.1953 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0698 - val_loss: 0.1944 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0697 - val_loss: 0.1949 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0676 - val_loss: 0.1955 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0668 - val_loss: 0.1959 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0657 - val_loss: 0.1956 - lr: 1.6000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0644 - val_loss: 0.1958 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 97/500\n",
      "25/25 - 1s - loss: 0.0644 - val_loss: 0.1958 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.1949 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.1953 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1954 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 101/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.1953 - lr: 1.6000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1958 - lr: 1.6000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 103/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1960 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 104/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1958 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1965 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 106/500\n",
      "25/25 - 1s - loss: 0.0480 - val_loss: 0.1952 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 107/500\n",
      "25/25 - 1s - loss: 0.0453 - val_loss: 0.1961 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0438 - val_loss: 0.1961 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 109/500\n",
      "25/25 - 1s - loss: 0.0433 - val_loss: 0.1971 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 110/500\n",
      "25/25 - 1s - loss: 0.0413 - val_loss: 0.1977 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 111/500\n",
      "Restoring model weights from the end of the best epoch: 91.\n",
      "25/25 - 1s - loss: 0.0391 - val_loss: 0.1986 - lr: 1.6000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 111: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 2.4min\n",
      "7/7 - 3s - 3s/epoch - 418ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 2.4min\n",
      "Epoch 1/500\n",
      "25/25 - 23s - loss: 2.0562 - val_loss: 0.5090 - lr: 0.0100 - 23s/epoch - 902ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4284 - val_loss: 0.2960 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.4041 - val_loss: 0.3391 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4094 - val_loss: 0.3907 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4781 - val_loss: 0.3602 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.5042 - val_loss: 0.3329 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 2.0391 - val_loss: 0.6115 - lr: 0.0020 - 1s/epoch - 40ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8287 - val_loss: 0.3973 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8498 - val_loss: 1.8987 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 1.2367 - val_loss: 0.2273 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.4562 - val_loss: 0.2064 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1492 - val_loss: 0.2836 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1006 - val_loss: 0.2358 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0740 - val_loss: 0.3118 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.3100 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0694 - val_loss: 0.3080 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0601 - val_loss: 0.3124 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0612 - val_loss: 0.2875 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0793 - val_loss: 0.2671 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0749 - val_loss: 0.2683 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0662 - val_loss: 0.2275 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0495 - val_loss: 0.2263 - lr: 8.0000e-05 - 1s/epoch - 46ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0467 - val_loss: 0.2234 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0428 - val_loss: 0.2272 - lr: 8.0000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0440 - val_loss: 0.2182 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0420 - val_loss: 0.2187 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 2s - loss: 0.0414 - val_loss: 0.2216 - lr: 8.0000e-05 - 2s/epoch - 61ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0412 - val_loss: 0.2189 - lr: 8.0000e-05 - 1s/epoch - 56ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 2s - loss: 0.0398 - val_loss: 0.2131 - lr: 8.0000e-05 - 2s/epoch - 62ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0407 - val_loss: 0.2149 - lr: 8.0000e-05 - 1s/epoch - 57ms/step\n",
      "Epoch 31/500\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "25/25 - 2s - loss: 0.0386 - val_loss: 0.2144 - lr: 8.0000e-05 - 2s/epoch - 64ms/step\n",
      "Epoch 31: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  59.3s\n",
      "7/7 - 7s - 7s/epoch - 1s/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 - 23s - loss: 2.5679 - val_loss: 0.3209 - lr: 0.0100 - 23s/epoch - 929ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5835 - val_loss: 0.3034 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8222 - val_loss: 0.2772 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3753 - val_loss: 0.2929 - lr: 0.0100 - 1s/epoch - 45ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.3144 - val_loss: 0.3330 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4722 - val_loss: 0.3263 - lr: 0.0100 - 1s/epoch - 45ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.6958 - val_loss: 0.2789 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2575 - val_loss: 0.3238 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0920 - val_loss: 0.3243 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0827 - val_loss: 0.3263 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0866 - val_loss: 0.3282 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0920 - val_loss: 0.3279 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0917 - val_loss: 0.3242 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1260 - val_loss: 0.2667 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1040 - val_loss: 0.2529 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0991 - val_loss: 0.2497 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1064 - val_loss: 0.2482 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1009 - val_loss: 0.2472 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0979 - val_loss: 0.2466 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0960 - val_loss: 0.2465 - lr: 8.0000e-05 - 1s/epoch - 43ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0955 - val_loss: 0.2464 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0951 - val_loss: 0.2464 - lr: 8.0000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0946 - val_loss: 0.2464 - lr: 1.6000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0946 - val_loss: 0.2463 - lr: 1.6000e-05 - 1s/epoch - 44ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0945 - val_loss: 0.2463 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2000e-06 - 1s/epoch - 43ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2000e-06 - 1s/epoch - 46ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 6.4000e-07 - 1s/epoch - 44ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 6.4000e-07 - 1s/epoch - 42ms/step\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.2800e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.2800e-07 - 1s/epoch - 44ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.2800e-07 - 1s/epoch - 45ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 2.5600e-08 - 1s/epoch - 43ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 2.5600e-08 - 1s/epoch - 44ms/step\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 2.5600e-08 - 1s/epoch - 44ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 5.1200e-09 - 1s/epoch - 43ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 5.1200e-09 - 1s/epoch - 42ms/step\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 5.1200e-09 - 1s/epoch - 43ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.0240e-09 - 1s/epoch - 42ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.0240e-09 - 1s/epoch - 43ms/step\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.0240e-09 - 1s/epoch - 43ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 2.0480e-10 - 1s/epoch - 44ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 2.0480e-10 - 1s/epoch - 44ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0943 - val_loss: 0.2463 - lr: 2.0480e-10 - 1s/epoch - 43ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 4.0960e-11 - 1s/epoch - 42ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 4.0960e-11 - 1s/epoch - 42ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 8.19199916235469e-12.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 4.0960e-11 - 1s/epoch - 42ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0943 - val_loss: 0.2463 - lr: 8.1920e-12 - 1s/epoch - 42ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 8.1920e-12 - 1s/epoch - 46ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.6383998324709382e-12.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 8.1920e-12 - 1s/epoch - 42ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.6384e-12 - 1s/epoch - 42ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.6384e-12 - 1s/epoch - 42ms/step\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.2767996215737895e-13.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 1.6384e-12 - 1s/epoch - 43ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2768e-13 - 1s/epoch - 43ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2768e-13 - 1s/epoch - 44ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.553599351567796e-14.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 3.2768e-13 - 1s/epoch - 42ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 6.5536e-14 - 1s/epoch - 42ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 6.5536e-14 - 1s/epoch - 43ms/step\n",
      "Epoch 61/500\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.310719924523668e-14.\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 6.5536e-14 - 1s/epoch - 44ms/step\n",
      "Epoch 61: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.5min\n",
      "7/7 - 3s - 3s/epoch - 430ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time= 1.6min\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 1.6796 - val_loss: 0.7167 - lr: 0.0100 - 22s/epoch - 865ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1975 - val_loss: 0.9866 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1904 - val_loss: 0.2826 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2816 - val_loss: 1.1039 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1992 - val_loss: 1.0782 - lr: 0.0100 - 1s/epoch - 46ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2055 - val_loss: 1.2520 - lr: 0.0100 - 1s/epoch - 45ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.5295 - val_loss: 2.3539 - lr: 0.0020 - 1s/epoch - 43ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2211 - val_loss: 2.3576 - lr: 0.0020 - 1s/epoch - 45ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2138 - val_loss: 2.3148 - lr: 0.0020 - 1s/epoch - 42ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3454 - val_loss: 2.7409 - lr: 4.0000e-04 - 1s/epoch - 44ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2652 - val_loss: 3.0457 - lr: 4.0000e-04 - 1s/epoch - 43ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.2302 - val_loss: 3.2053 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2269 - val_loss: 3.2662 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2219 - val_loss: 3.3165 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.2178 - val_loss: 3.3610 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2158 - val_loss: 3.3704 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2150 - val_loss: 3.3799 - lr: 1.6000e-05 - 1s/epoch - 45ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.2145 - val_loss: 3.3890 - lr: 1.6000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2140 - val_loss: 3.3909 - lr: 3.2000e-06 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2138 - val_loss: 3.3928 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.2136 - val_loss: 3.3946 - lr: 3.2000e-06 - 1s/epoch - 42ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.2136 - val_loss: 3.3949 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "25/25 - 1s - loss: 0.2136 - val_loss: 3.3953 - lr: 6.4000e-07 - 1s/epoch - 43ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  47.5s\n",
      "7/7 - 4s - 4s/epoch - 519ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x000001BE9E76D3A0>; total time=  51.1s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.5033 - val_loss: 0.6226 - lr: 0.0100 - 21s/epoch - 855ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8054 - val_loss: 0.6898 - lr: 0.0100 - 932ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.1255 - val_loss: 0.5631 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 2.1566 - val_loss: 1.0700 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 1.0477 - val_loss: 1.3464 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9210 - val_loss: 1.7537 - lr: 0.0020 - 873ms/epoch - 35ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8300 - val_loss: 1.9286 - lr: 0.0020 - 879ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7588 - val_loss: 1.9409 - lr: 4.0000e-04 - 912ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7579 - val_loss: 1.9516 - lr: 4.0000e-04 - 907ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7572 - val_loss: 1.9608 - lr: 4.0000e-04 - 925ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7487 - val_loss: 1.9624 - lr: 8.0000e-05 - 909ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7486 - val_loss: 1.9640 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7485 - val_loss: 1.9656 - lr: 8.0000e-05 - 886ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7469 - val_loss: 1.9660 - lr: 1.6000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7469 - val_loss: 1.9663 - lr: 1.6000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7469 - val_loss: 1.9666 - lr: 1.6000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7465 - val_loss: 1.9666 - lr: 3.2000e-06 - 922ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7465 - val_loss: 1.9667 - lr: 3.2000e-06 - 902ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7465 - val_loss: 1.9668 - lr: 3.2000e-06 - 889ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7465 - val_loss: 1.9668 - lr: 6.4000e-07 - 911ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.7465 - val_loss: 1.9668 - lr: 6.4000e-07 - 898ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.7465 - val_loss: 1.9668 - lr: 6.4000e-07 - 923ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "25/25 - 1s - loss: 0.7464 - val_loss: 1.9668 - lr: 1.2800e-07 - 926ms/epoch - 37ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.6s\n",
      "7/7 - 3s - 3s/epoch - 404ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  46.4s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.9114 - val_loss: 0.3285 - lr: 0.0100 - 21s/epoch - 822ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.5852 - val_loss: 2.0275 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7948 - val_loss: 2.0197 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7826 - val_loss: 2.0014 - lr: 0.0100 - 879ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7819 - val_loss: 1.9990 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7806 - val_loss: 2.0006 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7795 - val_loss: 2.0034 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7786 - val_loss: 2.0067 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7779 - val_loss: 2.0102 - lr: 0.0100 - 879ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7773 - val_loss: 2.0135 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7768 - val_loss: 2.0165 - lr: 0.0100 - 872ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7764 - val_loss: 2.0192 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7761 - val_loss: 2.0215 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7758 - val_loss: 2.0235 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7756 - val_loss: 2.0252 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7754 - val_loss: 2.0267 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7752 - val_loss: 2.0279 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7751 - val_loss: 2.0290 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7750 - val_loss: 2.0298 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7749 - val_loss: 2.0306 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7748 - val_loss: 2.0312 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.8s\n",
      "7/7 - 3s - 3s/epoch - 398ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  43.6s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7068 - val_loss: 0.3552 - lr: 0.0100 - 20s/epoch - 803ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.8559 - val_loss: 1.7205 - lr: 0.0100 - 911ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0479 - val_loss: 2.1515 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9464 - val_loss: 2.2324 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8623 - val_loss: 2.2371 - lr: 0.0020 - 928ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8627 - val_loss: 2.2460 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8624 - val_loss: 2.2536 - lr: 0.0020 - 886ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8483 - val_loss: 2.2548 - lr: 4.0000e-04 - 881ms/epoch - 35ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8482 - val_loss: 2.2563 - lr: 4.0000e-04 - 878ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8482 - val_loss: 2.2578 - lr: 4.0000e-04 - 896ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8454 - val_loss: 2.2581 - lr: 8.0000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8454 - val_loss: 2.2583 - lr: 8.0000e-05 - 933ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8454 - val_loss: 2.2586 - lr: 8.0000e-05 - 881ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8448 - val_loss: 2.2587 - lr: 1.6000e-05 - 893ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8448 - val_loss: 2.2587 - lr: 1.6000e-05 - 878ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8448 - val_loss: 2.2588 - lr: 1.6000e-05 - 882ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8447 - val_loss: 2.2588 - lr: 3.2000e-06 - 873ms/epoch - 35ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8447 - val_loss: 2.2588 - lr: 3.2000e-06 - 904ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8447 - val_loss: 2.2588 - lr: 3.2000e-06 - 905ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8446 - val_loss: 2.2588 - lr: 6.4000e-07 - 893ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8446 - val_loss: 2.2588 - lr: 6.4000e-07 - 906ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.3s\n",
      "7/7 - 3s - 3s/epoch - 463ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  43.5s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 1.8240 - val_loss: 1.0945 - lr: 0.0100 - 20s/epoch - 804ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.6338 - val_loss: 2.4779 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9755 - val_loss: 2.5106 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.9224 - val_loss: 2.3682 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9252 - val_loss: 2.3637 - lr: 0.0100 - 878ms/epoch - 35ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9207 - val_loss: 2.3809 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.9134 - val_loss: 2.3891 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.9078 - val_loss: 2.3982 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9026 - val_loss: 2.4097 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8979 - val_loss: 2.4227 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8935 - val_loss: 2.4364 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8896 - val_loss: 2.4503 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8861 - val_loss: 2.4641 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8830 - val_loss: 2.4775 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8803 - val_loss: 2.4902 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8779 - val_loss: 2.5021 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8759 - val_loss: 2.5131 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8741 - val_loss: 2.5232 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8726 - val_loss: 2.5324 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8713 - val_loss: 2.5407 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8702 - val_loss: 2.5481 - lr: 0.0100 - 875ms/epoch - 35ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.4s\n",
      "7/7 - 3s - 3s/epoch - 409ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  43.2s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 1.5151 - val_loss: 1.0453 - lr: 0.0100 - 20s/epoch - 787ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3642 - val_loss: 1.3540 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2561 - val_loss: 1.2848 - lr: 0.0100 - 868ms/epoch - 35ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2620 - val_loss: 1.2583 - lr: 0.0100 - 860ms/epoch - 34ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2578 - val_loss: 1.2315 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2568 - val_loss: 1.2046 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2124 - val_loss: 1.2025 - lr: 0.0020 - 931ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2129 - val_loss: 1.2055 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2099 - val_loss: 1.2058 - lr: 0.0020 - 889ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2089 - val_loss: 1.2059 - lr: 0.0020 - 885ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2090 - val_loss: 1.2056 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2094 - val_loss: 1.2042 - lr: 0.0020 - 889ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2098 - val_loss: 1.2025 - lr: 0.0020 - 884ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2005 - val_loss: 1.2027 - lr: 4.0000e-04 - 940ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2006 - val_loss: 1.2028 - lr: 4.0000e-04 - 888ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2006 - val_loss: 1.2029 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.2006 - val_loss: 1.2030 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 1.2030 - lr: 8.0000e-05 - 921ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 1.2030 - lr: 8.0000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 1.2030 - lr: 8.0000e-05 - 895ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.1985 - val_loss: 1.2030 - lr: 8.0000e-05 - 911ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.7s\n",
      "7/7 - 3s - 3s/epoch - 396ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001BE9E76D280>; total time=  43.4s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7552 - val_loss: 1.7253 - lr: 0.0100 - 20s/epoch - 800ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7433 - val_loss: 1.6967 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7292 - val_loss: 1.6594 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7097 - val_loss: 1.6050 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6807 - val_loss: 1.5192 - lr: 0.0100 - 929ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.6323 - val_loss: 1.3744 - lr: 0.0100 - 953ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.5492 - val_loss: 1.1255 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.4081 - val_loss: 0.7455 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2307 - val_loss: 0.3810 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1135 - val_loss: 0.2149 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0778 - val_loss: 0.1702 - lr: 0.0100 - 931ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0678 - val_loss: 0.1597 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0655 - val_loss: 0.1575 - lr: 0.0100 - 963ms/epoch - 39ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1573 - lr: 0.0100 - 981ms/epoch - 39ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.1575 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0627 - val_loss: 0.1575 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0622 - val_loss: 0.1575 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1576 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1578 - lr: 0.0100 - 968ms/epoch - 39ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0613 - val_loss: 0.1577 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1579 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0610 - val_loss: 0.1575 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0611 - val_loss: 0.1576 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.1572 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0609 - val_loss: 0.1574 - lr: 0.0100 - 965ms/epoch - 39ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1574 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0605 - val_loss: 0.1574 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.1573 - lr: 0.0100 - 971ms/epoch - 39ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.1572 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.1573 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.1568 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.1565 - lr: 0.0100 - 981ms/epoch - 39ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0596 - val_loss: 0.1566 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.1566 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0592 - val_loss: 0.1566 - lr: 0.0020 - 938ms/epoch - 38ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.1566 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1565 - lr: 0.0020 - 952ms/epoch - 38ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1565 - lr: 0.0020 - 932ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1563 - lr: 0.0020 - 964ms/epoch - 39ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1563 - lr: 0.0020 - 974ms/epoch - 39ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1563 - lr: 0.0020 - 918ms/epoch - 37ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1561 - lr: 0.0020 - 950ms/epoch - 38ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1560 - lr: 0.0020 - 913ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1560 - lr: 0.0020 - 949ms/epoch - 38ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1559 - lr: 0.0020 - 966ms/epoch - 39ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1560 - lr: 0.0020 - 965ms/epoch - 39ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0584 - val_loss: 0.1559 - lr: 0.0020 - 919ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1559 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1558 - lr: 0.0020 - 943ms/epoch - 38ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.1558 - lr: 4.0000e-04 - 930ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1557 - lr: 4.0000e-04 - 953ms/epoch - 38ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1557 - lr: 4.0000e-04 - 947ms/epoch - 38ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.1557 - lr: 4.0000e-04 - 947ms/epoch - 38ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0584 - val_loss: 0.1557 - lr: 4.0000e-04 - 931ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1557 - lr: 4.0000e-04 - 949ms/epoch - 38ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1557 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1556 - lr: 4.0000e-04 - 934ms/epoch - 37ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1556 - lr: 4.0000e-04 - 950ms/epoch - 38ms/step\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 4.0000e-04 - 934ms/epoch - 37ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1556 - lr: 8.0000e-05 - 936ms/epoch - 37ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 8.0000e-05 - 886ms/epoch - 35ms/step\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.1556 - lr: 8.0000e-05 - 926ms/epoch - 37ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 1.6000e-05 - 936ms/epoch - 37ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1556 - lr: 1.6000e-05 - 922ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1556 - lr: 1.6000e-05 - 964ms/epoch - 39ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1556 - lr: 3.2000e-06 - 928ms/epoch - 37ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1556 - lr: 3.2000e-06 - 944ms/epoch - 38ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 3.2000e-06 - 928ms/epoch - 37ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1556 - lr: 6.4000e-07 - 906ms/epoch - 36ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1556 - lr: 6.4000e-07 - 902ms/epoch - 36ms/step\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 6.4000e-07 - 943ms/epoch - 38ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 1.2800e-07 - 957ms/epoch - 38ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.1556 - lr: 1.2800e-07 - 950ms/epoch - 38ms/step\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0584 - val_loss: 0.1556 - lr: 1.2800e-07 - 952ms/epoch - 38ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 2.5600e-08 - 919ms/epoch - 37ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0584 - val_loss: 0.1556 - lr: 2.5600e-08 - 930ms/epoch - 37ms/step\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.1556 - lr: 2.5600e-08 - 938ms/epoch - 38ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0581 - val_loss: 0.1556 - lr: 5.1200e-09 - 974ms/epoch - 39ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1556 - lr: 5.1200e-09 - 915ms/epoch - 37ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1556 - lr: 5.1200e-09 - 928ms/epoch - 37ms/step\n",
      "Epoch 81/500\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.1556 - lr: 5.1200e-09 - 945ms/epoch - 38ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.6min\n",
      "7/7 - 3s - 3s/epoch - 479ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.7min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7692 - val_loss: 1.7154 - lr: 0.0100 - 20s/epoch - 813ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7489 - val_loss: 1.6643 - lr: 0.0100 - 998ms/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7209 - val_loss: 1.5864 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6759 - val_loss: 1.4532 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.5955 - val_loss: 1.2099 - lr: 0.0100 - 955ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4479 - val_loss: 0.7948 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2373 - val_loss: 0.3574 - lr: 0.0100 - 977ms/epoch - 39ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1004 - val_loss: 0.1858 - lr: 0.0100 - 984ms/epoch - 39ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0681 - val_loss: 0.1544 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1504 - lr: 0.0100 - 932ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.1508 - lr: 0.0100 - 941ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.1518 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1525 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.1534 - lr: 0.0100 - 985ms/epoch - 39ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.1537 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1541 - lr: 0.0100 - 964ms/epoch - 39ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.1543 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1548 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1553 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1552 - lr: 0.0100 - 931ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1554 - lr: 0.0100 - 962ms/epoch - 38ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1551 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1555 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1556 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1556 - lr: 0.0100 - 941ms/epoch - 38ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1552 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1551 - lr: 0.0100 - 975ms/epoch - 39ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1554 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1561 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1550 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 30: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  50.0s\n",
      "7/7 - 3s - 3s/epoch - 406ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time=  52.8s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.8820 - val_loss: 1.6908 - lr: 0.0100 - 20s/epoch - 812ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8494 - val_loss: 1.6219 - lr: 0.0100 - 997ms/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7994 - val_loss: 1.4985 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7024 - val_loss: 1.2417 - lr: 0.0100 - 984ms/epoch - 39ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4961 - val_loss: 0.7357 - lr: 0.0100 - 983ms/epoch - 39ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1961 - val_loss: 0.2720 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1699 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0368 - val_loss: 0.1588 - lr: 0.0100 - 932ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1589 - lr: 0.0100 - 961ms/epoch - 38ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0287 - val_loss: 0.1607 - lr: 0.0100 - 953ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0273 - val_loss: 0.1624 - lr: 0.0100 - 996ms/epoch - 40ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0277 - val_loss: 0.1638 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1645 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1648 - lr: 0.0100 - 1s/epoch - 47ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1651 - lr: 0.0100 - 1s/epoch - 45ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0268 - val_loss: 0.1653 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0272 - val_loss: 0.1655 - lr: 0.0100 - 1s/epoch - 46ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0267 - val_loss: 0.1655 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0271 - val_loss: 0.1648 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1645 - lr: 0.0100 - 1s/epoch - 43ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0269 - val_loss: 0.1649 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0265 - val_loss: 0.1650 - lr: 0.0020 - 1s/epoch - 55ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0263 - val_loss: 0.1650 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.1649 - lr: 0.0020 - 1s/epoch - 56ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.1648 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0259 - val_loss: 0.1647 - lr: 0.0020 - 966ms/epoch - 39ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0261 - val_loss: 0.1645 - lr: 0.0020 - 1s/epoch - 41ms/step\n",
      "Epoch 28/500\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "25/25 - 1s - loss: 0.0262 - val_loss: 0.1643 - lr: 0.0020 - 976ms/epoch - 39ms/step\n",
      "Epoch 28: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  51.4s\n",
      "7/7 - 4s - 4s/epoch - 519ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time=  55.0s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.9369 - val_loss: 1.6920 - lr: 0.0100 - 21s/epoch - 829ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8990 - val_loss: 1.6193 - lr: 0.0100 - 981ms/epoch - 39ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8323 - val_loss: 1.4675 - lr: 0.0100 - 979ms/epoch - 39ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.6846 - val_loss: 1.1026 - lr: 0.0100 - 971ms/epoch - 39ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.3684 - val_loss: 0.4663 - lr: 0.0100 - 941ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1079 - val_loss: 0.1995 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0716 - val_loss: 0.1647 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1576 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1566 - lr: 0.0100 - 963ms/epoch - 39ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1574 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0550 - val_loss: 0.1585 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1597 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.1605 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0528 - val_loss: 0.1609 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0530 - val_loss: 0.1620 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0527 - val_loss: 0.1623 - lr: 0.0100 - 950ms/epoch - 38ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1625 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.1621 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.1620 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.1631 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.1627 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1621 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1626 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0517 - val_loss: 0.1617 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0512 - val_loss: 0.1613 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0502 - val_loss: 0.1611 - lr: 0.0020 - 945ms/epoch - 38ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0505 - val_loss: 0.1612 - lr: 0.0020 - 970ms/epoch - 39ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0507 - val_loss: 0.1609 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "25/25 - 1s - loss: 0.0499 - val_loss: 0.1607 - lr: 0.0020 - 984ms/epoch - 39ms/step\n",
      "Epoch 29: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  50.4s\n",
      "7/7 - 3s - 3s/epoch - 410ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time=  53.3s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.6463 - val_loss: 1.7699 - lr: 0.0100 - 21s/epoch - 846ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6250 - val_loss: 1.7587 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5979 - val_loss: 1.7383 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5573 - val_loss: 1.6952 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4840 - val_loss: 1.5936 - lr: 0.0100 - 968ms/epoch - 39ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.3489 - val_loss: 1.3627 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.1784 - val_loss: 1.0666 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1019 - val_loss: 0.9169 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0875 - val_loss: 0.8386 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0831 - val_loss: 0.7747 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0796 - val_loss: 0.7175 - lr: 0.0100 - 881ms/epoch - 35ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0764 - val_loss: 0.6658 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0735 - val_loss: 0.6201 - lr: 0.0100 - 932ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0718 - val_loss: 0.5782 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0691 - val_loss: 0.5408 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0677 - val_loss: 0.5061 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.4760 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0640 - val_loss: 0.4486 - lr: 0.0100 - 979ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.4257 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0622 - val_loss: 0.4038 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.3838 - lr: 0.0100 - 971ms/epoch - 39ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.3682 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0597 - val_loss: 0.3538 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.3404 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.3287 - lr: 0.0100 - 955ms/epoch - 38ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0593 - val_loss: 0.3193 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0585 - val_loss: 0.3103 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.3012 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.2949 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0580 - val_loss: 0.2880 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.2821 - lr: 0.0100 - 966ms/epoch - 39ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.2774 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0572 - val_loss: 0.2723 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.2689 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.2655 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.2621 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0576 - val_loss: 0.2592 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.2559 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.2539 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.2509 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.2489 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0571 - val_loss: 0.2473 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.2446 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.2431 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.2416 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.2400 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.2373 - lr: 0.0020 - 931ms/epoch - 37ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.2352 - lr: 0.0020 - 932ms/epoch - 37ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.2335 - lr: 0.0020 - 899ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.2321 - lr: 0.0020 - 915ms/epoch - 37ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.2310 - lr: 0.0020 - 941ms/epoch - 38ms/step\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.2301 - lr: 0.0020 - 930ms/epoch - 37ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2299 - lr: 4.0000e-04 - 925ms/epoch - 37ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2297 - lr: 4.0000e-04 - 922ms/epoch - 37ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2295 - lr: 4.0000e-04 - 898ms/epoch - 36ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.2293 - lr: 4.0000e-04 - 892ms/epoch - 36ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2291 - lr: 4.0000e-04 - 941ms/epoch - 38ms/step\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0565 - val_loss: 0.2289 - lr: 4.0000e-04 - 951ms/epoch - 38ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2288 - lr: 8.0000e-05 - 921ms/epoch - 37ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2288 - lr: 8.0000e-05 - 936ms/epoch - 37ms/step\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.2288 - lr: 8.0000e-05 - 1s/epoch - 42ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2287 - lr: 1.6000e-05 - 961ms/epoch - 38ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2287 - lr: 1.6000e-05 - 945ms/epoch - 38ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2287 - lr: 1.6000e-05 - 926ms/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/500\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.2287 - lr: 1.6000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.2287 - lr: 3.2000e-06 - 896ms/epoch - 36ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2287 - lr: 3.2000e-06 - 948ms/epoch - 38ms/step\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2287 - lr: 3.2000e-06 - 899ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2287 - lr: 6.4000e-07 - 916ms/epoch - 37ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.2287 - lr: 6.4000e-07 - 972ms/epoch - 39ms/step\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.2287 - lr: 6.4000e-07 - 905ms/epoch - 36ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2287 - lr: 1.2800e-07 - 926ms/epoch - 37ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2287 - lr: 1.2800e-07 - 896ms/epoch - 36ms/step\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.2287 - lr: 1.2800e-07 - 917ms/epoch - 37ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2287 - lr: 2.5600e-08 - 906ms/epoch - 36ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.2287 - lr: 2.5600e-08 - 950ms/epoch - 38ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.2287 - lr: 2.5600e-08 - 896ms/epoch - 36ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2287 - lr: 2.5600e-08 - 893ms/epoch - 36ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.2287 - lr: 2.5600e-08 - 902ms/epoch - 36ms/step\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2287 - lr: 2.5600e-08 - 894ms/epoch - 36ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0557 - val_loss: 0.2287 - lr: 5.1200e-09 - 942ms/epoch - 38ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.2287 - lr: 5.1200e-09 - 919ms/epoch - 37ms/step\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.2287 - lr: 5.1200e-09 - 941ms/epoch - 38ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2287 - lr: 1.0240e-09 - 888ms/epoch - 36ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.2287 - lr: 1.0240e-09 - 890ms/epoch - 36ms/step\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 2.0479997697719911e-10.\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.2287 - lr: 1.0240e-09 - 918ms/epoch - 37ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0562 - val_loss: 0.2287 - lr: 2.0480e-10 - 908ms/epoch - 36ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.2287 - lr: 2.0480e-10 - 894ms/epoch - 36ms/step\n",
      "Epoch 89/500\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.095999650566285e-11.\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2287 - lr: 2.0480e-10 - 870ms/epoch - 35ms/step\n",
      "Epoch 90/500\n",
      "Restoring model weights from the end of the best epoch: 70.\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.2287 - lr: 4.0960e-11 - 945ms/epoch - 38ms/step\n",
      "Epoch 90: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.8min\n",
      "7/7 - 3s - 3s/epoch - 447ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adadelta.Adadelta object at 0x000001BE9E76D370>; total time= 1.8min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.4078 - val_loss: 0.2098 - lr: 0.0100 - 20s/epoch - 790ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.3430 - val_loss: 0.2188 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0756 - val_loss: 0.2039 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0683 - val_loss: 0.1997 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0667 - val_loss: 0.1953 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0669 - val_loss: 0.1939 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0686 - val_loss: 0.1862 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0688 - val_loss: 0.1855 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0809 - val_loss: 0.1767 - lr: 0.0020 - 987ms/epoch - 39ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0720 - val_loss: 0.1707 - lr: 0.0020 - 960ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0690 - val_loss: 0.1668 - lr: 0.0020 - 934ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0675 - val_loss: 0.1689 - lr: 4.0000e-04 - 951ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0664 - val_loss: 0.1700 - lr: 4.0000e-04 - 924ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0658 - val_loss: 0.1705 - lr: 4.0000e-04 - 903ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0655 - val_loss: 0.1705 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1699 - lr: 4.0000e-04 - 923ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0645 - val_loss: 0.1695 - lr: 4.0000e-04 - 975ms/epoch - 39ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0646 - val_loss: 0.1688 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.1683 - lr: 4.0000e-04 - 913ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0638 - val_loss: 0.1678 - lr: 4.0000e-04 - 993ms/epoch - 40ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1670 - lr: 4.0000e-04 - 909ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.1665 - lr: 4.0000e-04 - 911ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1659 - lr: 4.0000e-04 - 918ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1654 - lr: 4.0000e-04 - 909ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1651 - lr: 4.0000e-04 - 942ms/epoch - 38ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.1651 - lr: 8.0000e-05 - 923ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1652 - lr: 8.0000e-05 - 916ms/epoch - 37ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0627 - val_loss: 0.1652 - lr: 8.0000e-05 - 876ms/epoch - 35ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1653 - lr: 8.0000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0622 - val_loss: 0.1653 - lr: 8.0000e-05 - 879ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0619 - val_loss: 0.1653 - lr: 8.0000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1653 - lr: 8.0000e-05 - 922ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1654 - lr: 8.0000e-05 - 902ms/epoch - 36ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1654 - lr: 8.0000e-05 - 921ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0627 - val_loss: 0.1653 - lr: 8.0000e-05 - 981ms/epoch - 39ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1654 - lr: 1.6000e-05 - 938ms/epoch - 38ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.1654 - lr: 1.6000e-05 - 944ms/epoch - 38ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0622 - val_loss: 0.1654 - lr: 1.6000e-05 - 909ms/epoch - 36ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1654 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1654 - lr: 1.6000e-05 - 903ms/epoch - 36ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1654 - lr: 3.2000e-06 - 888ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0617 - val_loss: 0.1654 - lr: 3.2000e-06 - 968ms/epoch - 39ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0621 - val_loss: 0.1654 - lr: 3.2000e-06 - 935ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0624 - val_loss: 0.1654 - lr: 6.4000e-07 - 957ms/epoch - 38ms/step\n",
      "Epoch 45/500\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "25/25 - 1s - loss: 0.0623 - val_loss: 0.1654 - lr: 6.4000e-07 - 997ms/epoch - 40ms/step\n",
      "Epoch 45: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.0min\n",
      "7/7 - 3s - 3s/epoch - 395ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.1min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.5373 - val_loss: 0.3560 - lr: 0.0100 - 19s/epoch - 754ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2449 - val_loss: 0.2261 - lr: 0.0100 - 931ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0733 - val_loss: 0.2136 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.2058 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0618 - val_loss: 0.1968 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0622 - val_loss: 0.1984 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0626 - val_loss: 0.1910 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0633 - val_loss: 0.1868 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0763 - val_loss: 0.1773 - lr: 0.0020 - 924ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0669 - val_loss: 0.1714 - lr: 0.0020 - 964ms/epoch - 39ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0636 - val_loss: 0.1682 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0632 - val_loss: 0.1699 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.1708 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0615 - val_loss: 0.1710 - lr: 4.0000e-04 - 942ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0604 - val_loss: 0.1710 - lr: 4.0000e-04 - 943ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1702 - lr: 4.0000e-04 - 1s/epoch - 40ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0595 - val_loss: 0.1693 - lr: 4.0000e-04 - 902ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0594 - val_loss: 0.1685 - lr: 4.0000e-04 - 925ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.1677 - lr: 4.0000e-04 - 920ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0586 - val_loss: 0.1669 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.1663 - lr: 4.0000e-04 - 945ms/epoch - 38ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0587 - val_loss: 0.1659 - lr: 4.0000e-04 - 895ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1654 - lr: 4.0000e-04 - 882ms/epoch - 35ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0582 - val_loss: 0.1651 - lr: 4.0000e-04 - 876ms/epoch - 35ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.1645 - lr: 4.0000e-04 - 904ms/epoch - 36ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0576 - val_loss: 0.1641 - lr: 4.0000e-04 - 897ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.1637 - lr: 4.0000e-04 - 881ms/epoch - 35ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.1634 - lr: 4.0000e-04 - 923ms/epoch - 37ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0574 - val_loss: 0.1630 - lr: 4.0000e-04 - 896ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1626 - lr: 4.0000e-04 - 921ms/epoch - 37ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0575 - val_loss: 0.1621 - lr: 4.0000e-04 - 853ms/epoch - 34ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1617 - lr: 4.0000e-04 - 918ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1614 - lr: 4.0000e-04 - 874ms/epoch - 35ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0568 - val_loss: 0.1611 - lr: 4.0000e-04 - 905ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.1610 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1610 - lr: 8.0000e-05 - 959ms/epoch - 38ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0567 - val_loss: 0.1611 - lr: 8.0000e-05 - 899ms/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1611 - lr: 8.0000e-05 - 943ms/epoch - 38ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.1612 - lr: 8.0000e-05 - 916ms/epoch - 37ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1612 - lr: 8.0000e-05 - 950ms/epoch - 38ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.1613 - lr: 8.0000e-05 - 878ms/epoch - 35ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1613 - lr: 8.0000e-05 - 896ms/epoch - 36ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1613 - lr: 8.0000e-05 - 936ms/epoch - 37ms/step\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1613 - lr: 8.0000e-05 - 905ms/epoch - 36ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.1613 - lr: 1.6000e-05 - 1s/epoch - 41ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1613 - lr: 1.6000e-05 - 923ms/epoch - 37ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1613 - lr: 1.6000e-05 - 907ms/epoch - 36ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0553 - val_loss: 0.1613 - lr: 1.6000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0556 - val_loss: 0.1613 - lr: 1.6000e-05 - 928ms/epoch - 37ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1613 - lr: 1.6000e-05 - 976ms/epoch - 39ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1613 - lr: 1.6000e-05 - 1s/epoch - 40ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0560 - val_loss: 0.1613 - lr: 1.6000e-05 - 952ms/epoch - 38ms/step\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0561 - val_loss: 0.1613 - lr: 1.6000e-05 - 912ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1613 - lr: 3.2000e-06 - 910ms/epoch - 36ms/step\n",
      "Epoch 55/500\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "25/25 - 1s - loss: 0.0555 - val_loss: 0.1613 - lr: 3.2000e-06 - 970ms/epoch - 39ms/step\n",
      "Epoch 55: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.2min\n",
      "7/7 - 3s - 3s/epoch - 426ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 1.2min\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.5064 - val_loss: 0.3072 - lr: 0.0100 - 20s/epoch - 783ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2173 - val_loss: 0.2233 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.2054 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0258 - val_loss: 0.2036 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0276 - val_loss: 0.2028 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0279 - val_loss: 0.1945 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1913 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0436 - val_loss: 0.1765 - lr: 0.0020 - 894ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0370 - val_loss: 0.1732 - lr: 0.0020 - 893ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0352 - val_loss: 0.1706 - lr: 0.0020 - 943ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0363 - val_loss: 0.1728 - lr: 4.0000e-04 - 889ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0350 - val_loss: 0.1734 - lr: 4.0000e-04 - 886ms/epoch - 35ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0344 - val_loss: 0.1735 - lr: 4.0000e-04 - 907ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0339 - val_loss: 0.1737 - lr: 8.0000e-05 - 873ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0341 - val_loss: 0.1739 - lr: 8.0000e-05 - 880ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0337 - val_loss: 0.1741 - lr: 8.0000e-05 - 906ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0335 - val_loss: 0.1741 - lr: 1.6000e-05 - 915ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0330 - val_loss: 0.1741 - lr: 1.6000e-05 - 911ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.1742 - lr: 1.6000e-05 - 891ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0331 - val_loss: 0.1742 - lr: 3.2000e-06 - 916ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0329 - val_loss: 0.1742 - lr: 3.2000e-06 - 890ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0336 - val_loss: 0.1742 - lr: 3.2000e-06 - 918ms/epoch - 37ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0335 - val_loss: 0.1742 - lr: 6.4000e-07 - 924ms/epoch - 37ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.1742 - lr: 6.4000e-07 - 891ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0332 - val_loss: 0.1742 - lr: 6.4000e-07 - 861ms/epoch - 34ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0334 - val_loss: 0.1742 - lr: 1.2800e-07 - 918ms/epoch - 37ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0331 - val_loss: 0.1742 - lr: 1.2800e-07 - 909ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0336 - val_loss: 0.1742 - lr: 1.2800e-07 - 888ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0331 - val_loss: 0.1742 - lr: 2.5600e-08 - 902ms/epoch - 36ms/step\n",
      "Epoch 30/500\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.1742 - lr: 2.5600e-08 - 939ms/epoch - 38ms/step\n",
      "Epoch 30: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  47.9s\n",
      "7/7 - 3s - 3s/epoch - 435ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time=  50.9s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.5784 - val_loss: 0.5234 - lr: 0.0100 - 19s/epoch - 778ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1671 - val_loss: 0.2296 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0588 - val_loss: 0.2134 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0569 - val_loss: 0.2148 - lr: 0.0100 - 854ms/epoch - 34ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.0534 - val_loss: 0.2107 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0523 - val_loss: 0.2077 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0521 - val_loss: 0.2050 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0520 - val_loss: 0.2009 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0518 - val_loss: 0.1986 - lr: 0.0100 - 882ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.1920 - lr: 0.0100 - 882ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.0531 - val_loss: 0.1945 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.1902 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0599 - val_loss: 0.1800 - lr: 0.0020 - 929ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0583 - val_loss: 0.1751 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.1723 - lr: 0.0020 - 907ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0563 - val_loss: 0.1740 - lr: 4.0000e-04 - 891ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0559 - val_loss: 0.1746 - lr: 4.0000e-04 - 915ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0552 - val_loss: 0.1748 - lr: 4.0000e-04 - 907ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1750 - lr: 8.0000e-05 - 933ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1751 - lr: 8.0000e-05 - 901ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1752 - lr: 8.0000e-05 - 867ms/epoch - 35ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0539 - val_loss: 0.1752 - lr: 1.6000e-05 - 880ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1752 - lr: 1.6000e-05 - 890ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0548 - val_loss: 0.1753 - lr: 1.6000e-05 - 900ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0549 - val_loss: 0.1753 - lr: 3.2000e-06 - 874ms/epoch - 35ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1753 - lr: 3.2000e-06 - 898ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0541 - val_loss: 0.1753 - lr: 3.2000e-06 - 907ms/epoch - 36ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1753 - lr: 6.4000e-07 - 896ms/epoch - 36ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1753 - lr: 6.4000e-07 - 845ms/epoch - 34ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1753 - lr: 6.4000e-07 - 889ms/epoch - 36ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0543 - val_loss: 0.1753 - lr: 1.2800e-07 - 883ms/epoch - 35ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.1753 - lr: 1.2800e-07 - 904ms/epoch - 36ms/step\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.1753 - lr: 1.2800e-07 - 916ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0544 - val_loss: 0.1753 - lr: 2.5600e-08 - 896ms/epoch - 36ms/step\n",
      "Epoch 35/500\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "25/25 - 1s - loss: 0.0547 - val_loss: 0.1753 - lr: 2.5600e-08 - 897ms/epoch - 36ms/step\n",
      "Epoch 35: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  52.1s\n",
      "7/7 - 3s - 3s/epoch - 388ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time=  54.8s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.2076 - val_loss: 2.1096 - lr: 0.0100 - 19s/epoch - 742ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1924 - val_loss: 2.1357 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1750 - val_loss: 1.9868 - lr: 0.0100 - 877ms/epoch - 35ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.1517 - val_loss: 1.6286 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1195 - val_loss: 1.0881 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.0875 - val_loss: 0.6769 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.0718 - val_loss: 0.5391 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.0673 - val_loss: 0.5023 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.0657 - val_loss: 0.4845 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.0645 - val_loss: 0.4728 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 11/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 1s - loss: 0.0639 - val_loss: 0.4619 - lr: 0.0100 - 964ms/epoch - 39ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.4503 - lr: 0.0100 - 903ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.0629 - val_loss: 0.4424 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0620 - val_loss: 0.4343 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0614 - val_loss: 0.4226 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0616 - val_loss: 0.4159 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0606 - val_loss: 0.4081 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0602 - val_loss: 0.3987 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0600 - val_loss: 0.3934 - lr: 0.0100 - 881ms/epoch - 35ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0598 - val_loss: 0.3863 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0591 - val_loss: 0.3793 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0589 - val_loss: 0.3733 - lr: 0.0100 - 866ms/epoch - 35ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0584 - val_loss: 0.3680 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0578 - val_loss: 0.3578 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0573 - val_loss: 0.3535 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0579 - val_loss: 0.3483 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0570 - val_loss: 0.3405 - lr: 0.0100 - 871ms/epoch - 35ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.3393 - lr: 0.0100 - 872ms/epoch - 35ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0566 - val_loss: 0.3315 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0564 - val_loss: 0.3278 - lr: 0.0100 - 881ms/epoch - 35ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0558 - val_loss: 0.3221 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 32/500\n",
      "25/25 - 1s - loss: 0.0554 - val_loss: 0.3156 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 33/500\n",
      "25/25 - 1s - loss: 0.0551 - val_loss: 0.3126 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 34/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.3063 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 35/500\n",
      "25/25 - 1s - loss: 0.0545 - val_loss: 0.3024 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 36/500\n",
      "25/25 - 1s - loss: 0.0540 - val_loss: 0.2990 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 37/500\n",
      "25/25 - 1s - loss: 0.0536 - val_loss: 0.2920 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 38/500\n",
      "25/25 - 1s - loss: 0.0533 - val_loss: 0.2896 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 39/500\n",
      "25/25 - 1s - loss: 0.0532 - val_loss: 0.2859 - lr: 0.0100 - 870ms/epoch - 35ms/step\n",
      "Epoch 40/500\n",
      "25/25 - 1s - loss: 0.0526 - val_loss: 0.2821 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 41/500\n",
      "25/25 - 1s - loss: 0.0525 - val_loss: 0.2780 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 42/500\n",
      "25/25 - 1s - loss: 0.0522 - val_loss: 0.2751 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 43/500\n",
      "25/25 - 1s - loss: 0.0519 - val_loss: 0.2710 - lr: 0.0100 - 873ms/epoch - 35ms/step\n",
      "Epoch 44/500\n",
      "25/25 - 1s - loss: 0.0515 - val_loss: 0.2686 - lr: 0.0100 - 882ms/epoch - 35ms/step\n",
      "Epoch 45/500\n",
      "25/25 - 1s - loss: 0.0514 - val_loss: 0.2628 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 46/500\n",
      "25/25 - 1s - loss: 0.0507 - val_loss: 0.2602 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 47/500\n",
      "25/25 - 1s - loss: 0.0504 - val_loss: 0.2563 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 48/500\n",
      "25/25 - 1s - loss: 0.0506 - val_loss: 0.2551 - lr: 0.0100 - 877ms/epoch - 35ms/step\n",
      "Epoch 49/500\n",
      "25/25 - 1s - loss: 0.0497 - val_loss: 0.2508 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 50/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.2501 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 51/500\n",
      "25/25 - 1s - loss: 0.0493 - val_loss: 0.2468 - lr: 0.0100 - 971ms/epoch - 39ms/step\n",
      "Epoch 52/500\n",
      "25/25 - 1s - loss: 0.0488 - val_loss: 0.2422 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 53/500\n",
      "25/25 - 1s - loss: 0.0485 - val_loss: 0.2370 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 54/500\n",
      "25/25 - 1s - loss: 0.0483 - val_loss: 0.2351 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 55/500\n",
      "25/25 - 1s - loss: 0.0478 - val_loss: 0.2322 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 56/500\n",
      "25/25 - 1s - loss: 0.0479 - val_loss: 0.2291 - lr: 0.0100 - 931ms/epoch - 37ms/step\n",
      "Epoch 57/500\n",
      "25/25 - 1s - loss: 0.0475 - val_loss: 0.2274 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 58/500\n",
      "25/25 - 1s - loss: 0.0469 - val_loss: 0.2237 - lr: 0.0100 - 956ms/epoch - 38ms/step\n",
      "Epoch 59/500\n",
      "25/25 - 1s - loss: 0.0471 - val_loss: 0.2212 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 60/500\n",
      "25/25 - 1s - loss: 0.0465 - val_loss: 0.2159 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 61/500\n",
      "25/25 - 1s - loss: 0.0463 - val_loss: 0.2134 - lr: 0.0100 - 874ms/epoch - 35ms/step\n",
      "Epoch 62/500\n",
      "25/25 - 1s - loss: 0.0459 - val_loss: 0.2132 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 63/500\n",
      "25/25 - 1s - loss: 0.0460 - val_loss: 0.2100 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 64/500\n",
      "25/25 - 1s - loss: 0.0452 - val_loss: 0.2068 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 65/500\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2041 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 66/500\n",
      "25/25 - 1s - loss: 0.0448 - val_loss: 0.2027 - lr: 0.0100 - 949ms/epoch - 38ms/step\n",
      "Epoch 67/500\n",
      "25/25 - 1s - loss: 0.0444 - val_loss: 0.1992 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 68/500\n",
      "25/25 - 1s - loss: 0.0441 - val_loss: 0.1978 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 69/500\n",
      "25/25 - 1s - loss: 0.0435 - val_loss: 0.1947 - lr: 0.0100 - 860ms/epoch - 34ms/step\n",
      "Epoch 70/500\n",
      "25/25 - 1s - loss: 0.0431 - val_loss: 0.1944 - lr: 0.0100 - 882ms/epoch - 35ms/step\n",
      "Epoch 71/500\n",
      "25/25 - 1s - loss: 0.0431 - val_loss: 0.1894 - lr: 0.0100 - 926ms/epoch - 37ms/step\n",
      "Epoch 72/500\n",
      "25/25 - 1s - loss: 0.0429 - val_loss: 0.1854 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 73/500\n",
      "25/25 - 1s - loss: 0.0423 - val_loss: 0.1858 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 74/500\n",
      "25/25 - 1s - loss: 0.0424 - val_loss: 0.1833 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 75/500\n",
      "25/25 - 1s - loss: 0.0420 - val_loss: 0.1804 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 76/500\n",
      "25/25 - 1s - loss: 0.0413 - val_loss: 0.1799 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 77/500\n",
      "25/25 - 1s - loss: 0.0411 - val_loss: 0.1780 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 78/500\n",
      "25/25 - 1s - loss: 0.0408 - val_loss: 0.1759 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 79/500\n",
      "25/25 - 1s - loss: 0.0403 - val_loss: 0.1710 - lr: 0.0100 - 868ms/epoch - 35ms/step\n",
      "Epoch 80/500\n",
      "25/25 - 1s - loss: 0.0405 - val_loss: 0.1678 - lr: 0.0100 - 900ms/epoch - 36ms/step\n",
      "Epoch 81/500\n",
      "25/25 - 1s - loss: 0.0400 - val_loss: 0.1664 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 82/500\n",
      "25/25 - 1s - loss: 0.0396 - val_loss: 0.1654 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 83/500\n",
      "25/25 - 1s - loss: 0.0391 - val_loss: 0.1639 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 84/500\n",
      "25/25 - 1s - loss: 0.0388 - val_loss: 0.1617 - lr: 0.0100 - 970ms/epoch - 39ms/step\n",
      "Epoch 85/500\n",
      "25/25 - 1s - loss: 0.0385 - val_loss: 0.1587 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 86/500\n",
      "25/25 - 1s - loss: 0.0385 - val_loss: 0.1564 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 87/500\n",
      "25/25 - 1s - loss: 0.0379 - val_loss: 0.1545 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 88/500\n",
      "25/25 - 1s - loss: 0.0374 - val_loss: 0.1521 - lr: 0.0100 - 886ms/epoch - 35ms/step\n",
      "Epoch 89/500\n",
      "25/25 - 1s - loss: 0.0372 - val_loss: 0.1499 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 90/500\n",
      "25/25 - 1s - loss: 0.0368 - val_loss: 0.1487 - lr: 0.0100 - 941ms/epoch - 38ms/step\n",
      "Epoch 91/500\n",
      "25/25 - 1s - loss: 0.0371 - val_loss: 0.1448 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 92/500\n",
      "25/25 - 1s - loss: 0.0361 - val_loss: 0.1433 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 93/500\n",
      "25/25 - 1s - loss: 0.0360 - val_loss: 0.1414 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 94/500\n",
      "25/25 - 1s - loss: 0.0357 - val_loss: 0.1399 - lr: 0.0100 - 896ms/epoch - 36ms/step\n",
      "Epoch 95/500\n",
      "25/25 - 1s - loss: 0.0353 - val_loss: 0.1372 - lr: 0.0100 - 893ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/500\n",
      "25/25 - 1s - loss: 0.0350 - val_loss: 0.1359 - lr: 0.0100 - 881ms/epoch - 35ms/step\n",
      "Epoch 97/500\n",
      "25/25 - 1s - loss: 0.0347 - val_loss: 0.1354 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 98/500\n",
      "25/25 - 1s - loss: 0.0346 - val_loss: 0.1340 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 99/500\n",
      "25/25 - 1s - loss: 0.0346 - val_loss: 0.1309 - lr: 0.0100 - 983ms/epoch - 39ms/step\n",
      "Epoch 100/500\n",
      "25/25 - 1s - loss: 0.0343 - val_loss: 0.1299 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 101/500\n",
      "25/25 - 1s - loss: 0.0338 - val_loss: 0.1282 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 102/500\n",
      "25/25 - 1s - loss: 0.0335 - val_loss: 0.1283 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 103/500\n",
      "25/25 - 1s - loss: 0.0334 - val_loss: 0.1263 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 104/500\n",
      "25/25 - 1s - loss: 0.0333 - val_loss: 0.1235 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 105/500\n",
      "25/25 - 1s - loss: 0.0330 - val_loss: 0.1228 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 106/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.1203 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 107/500\n",
      "25/25 - 1s - loss: 0.0329 - val_loss: 0.1213 - lr: 0.0100 - 858ms/epoch - 34ms/step\n",
      "Epoch 108/500\n",
      "25/25 - 1s - loss: 0.0324 - val_loss: 0.1197 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 109/500\n",
      "25/25 - 1s - loss: 0.0326 - val_loss: 0.1167 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 110/500\n",
      "25/25 - 1s - loss: 0.0323 - val_loss: 0.1193 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 111/500\n",
      "25/25 - 1s - loss: 0.0327 - val_loss: 0.1174 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 112/500\n",
      "25/25 - 1s - loss: 0.0319 - val_loss: 0.1154 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 113/500\n",
      "25/25 - 1s - loss: 0.0319 - val_loss: 0.1151 - lr: 0.0100 - 882ms/epoch - 35ms/step\n",
      "Epoch 114/500\n",
      "25/25 - 1s - loss: 0.0315 - val_loss: 0.1149 - lr: 0.0100 - 872ms/epoch - 35ms/step\n",
      "Epoch 115/500\n",
      "25/25 - 1s - loss: 0.0319 - val_loss: 0.1143 - lr: 0.0100 - 877ms/epoch - 35ms/step\n",
      "Epoch 116/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1138 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 117/500\n",
      "25/25 - 1s - loss: 0.0311 - val_loss: 0.1137 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 118/500\n",
      "25/25 - 1s - loss: 0.0312 - val_loss: 0.1117 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 119/500\n",
      "25/25 - 1s - loss: 0.0308 - val_loss: 0.1104 - lr: 0.0100 - 882ms/epoch - 35ms/step\n",
      "Epoch 120/500\n",
      "25/25 - 1s - loss: 0.0308 - val_loss: 0.1108 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 121/500\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1107 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 122/500\n",
      "25/25 - 1s - loss: 0.0309 - val_loss: 0.1103 - lr: 0.0100 - 885ms/epoch - 35ms/step\n",
      "Epoch 123/500\n",
      "25/25 - 1s - loss: 0.0305 - val_loss: 0.1091 - lr: 0.0100 - 853ms/epoch - 34ms/step\n",
      "Epoch 124/500\n",
      "25/25 - 1s - loss: 0.0303 - val_loss: 0.1076 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 125/500\n",
      "25/25 - 1s - loss: 0.0305 - val_loss: 0.1083 - lr: 0.0100 - 867ms/epoch - 35ms/step\n",
      "Epoch 126/500\n",
      "25/25 - 1s - loss: 0.0304 - val_loss: 0.1090 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 127/500\n",
      "25/25 - 1s - loss: 0.0303 - val_loss: 0.1086 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 128/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1085 - lr: 0.0100 - 923ms/epoch - 37ms/step\n",
      "Epoch 129/500\n",
      "25/25 - 1s - loss: 0.0304 - val_loss: 0.1071 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 130/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1068 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 131/500\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1073 - lr: 0.0100 - 995ms/epoch - 40ms/step\n",
      "Epoch 132/500\n",
      "25/25 - 1s - loss: 0.0303 - val_loss: 0.1064 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 133/500\n",
      "25/25 - 1s - loss: 0.0299 - val_loss: 0.1068 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 134/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.1058 - lr: 0.0100 - 949ms/epoch - 38ms/step\n",
      "Epoch 135/500\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1075 - lr: 0.0100 - 918ms/epoch - 37ms/step\n",
      "Epoch 136/500\n",
      "25/25 - 1s - loss: 0.0294 - val_loss: 0.1052 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 137/500\n",
      "25/25 - 1s - loss: 0.0297 - val_loss: 0.1051 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 138/500\n",
      "25/25 - 1s - loss: 0.0292 - val_loss: 0.1059 - lr: 0.0100 - 909ms/epoch - 36ms/step\n",
      "Epoch 139/500\n",
      "25/25 - 1s - loss: 0.0296 - val_loss: 0.1053 - lr: 0.0100 - 941ms/epoch - 38ms/step\n",
      "Epoch 140/500\n",
      "25/25 - 1s - loss: 0.0295 - val_loss: 0.1050 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 141/500\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.1041 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 142/500\n",
      "25/25 - 1s - loss: 0.0290 - val_loss: 0.1052 - lr: 0.0100 - 916ms/epoch - 37ms/step\n",
      "Epoch 143/500\n",
      "25/25 - 1s - loss: 0.0288 - val_loss: 0.1051 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 144/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1045 - lr: 0.0100 - 978ms/epoch - 39ms/step\n",
      "Epoch 145/500\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.1048 - lr: 0.0100 - 937ms/epoch - 37ms/step\n",
      "Epoch 146/500\n",
      "25/25 - 1s - loss: 0.0290 - val_loss: 0.1056 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 147/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1045 - lr: 0.0100 - 1s/epoch - 44ms/step\n",
      "Epoch 148/500\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.0286 - val_loss: 0.1047 - lr: 0.0100 - 978ms/epoch - 39ms/step\n",
      "Epoch 149/500\n",
      "25/25 - 1s - loss: 0.0329 - val_loss: 0.1013 - lr: 0.0020 - 877ms/epoch - 35ms/step\n",
      "Epoch 150/500\n",
      "25/25 - 1s - loss: 0.0293 - val_loss: 0.1012 - lr: 0.0020 - 891ms/epoch - 36ms/step\n",
      "Epoch 151/500\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.0293 - val_loss: 0.1007 - lr: 0.0020 - 853ms/epoch - 34ms/step\n",
      "Epoch 152/500\n",
      "25/25 - 1s - loss: 0.0300 - val_loss: 0.1016 - lr: 4.0000e-04 - 875ms/epoch - 35ms/step\n",
      "Epoch 153/500\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.1022 - lr: 4.0000e-04 - 870ms/epoch - 35ms/step\n",
      "Epoch 154/500\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.1024 - lr: 4.0000e-04 - 853ms/epoch - 34ms/step\n",
      "Epoch 155/500\n",
      "25/25 - 1s - loss: 0.0288 - val_loss: 0.1026 - lr: 8.0000e-05 - 840ms/epoch - 34ms/step\n",
      "Epoch 156/500\n",
      "25/25 - 1s - loss: 0.0288 - val_loss: 0.1027 - lr: 8.0000e-05 - 878ms/epoch - 35ms/step\n",
      "Epoch 157/500\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.0290 - val_loss: 0.1028 - lr: 8.0000e-05 - 926ms/epoch - 37ms/step\n",
      "Epoch 158/500\n",
      "25/25 - 1s - loss: 0.0287 - val_loss: 0.1028 - lr: 1.6000e-05 - 875ms/epoch - 35ms/step\n",
      "Epoch 159/500\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.1028 - lr: 1.6000e-05 - 896ms/epoch - 36ms/step\n",
      "Epoch 160/500\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.0286 - val_loss: 0.1029 - lr: 1.6000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 161/500\n",
      "25/25 - 1s - loss: 0.0283 - val_loss: 0.1029 - lr: 3.2000e-06 - 885ms/epoch - 35ms/step\n",
      "Epoch 162/500\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.1029 - lr: 3.2000e-06 - 865ms/epoch - 35ms/step\n",
      "Epoch 163/500\n",
      "25/25 - 1s - loss: 0.0282 - val_loss: 0.1029 - lr: 3.2000e-06 - 907ms/epoch - 36ms/step\n",
      "Epoch 164/500\n",
      "25/25 - 1s - loss: 0.0290 - val_loss: 0.1029 - lr: 3.2000e-06 - 873ms/epoch - 35ms/step\n",
      "Epoch 165/500\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.1029 - lr: 3.2000e-06 - 857ms/epoch - 34ms/step\n",
      "Epoch 166/500\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.0283 - val_loss: 0.1029 - lr: 3.2000e-06 - 842ms/epoch - 34ms/step\n",
      "Epoch 167/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1029 - lr: 6.4000e-07 - 849ms/epoch - 34ms/step\n",
      "Epoch 168/500\n",
      "25/25 - 1s - loss: 0.0291 - val_loss: 0.1029 - lr: 6.4000e-07 - 846ms/epoch - 34ms/step\n",
      "Epoch 169/500\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.0288 - val_loss: 0.1029 - lr: 6.4000e-07 - 848ms/epoch - 34ms/step\n",
      "Epoch 170/500\n",
      "25/25 - 1s - loss: 0.0289 - val_loss: 0.1029 - lr: 1.2800e-07 - 871ms/epoch - 35ms/step\n",
      "Epoch 171/500\n",
      "Restoring model weights from the end of the best epoch: 151.\n",
      "25/25 - 1s - loss: 0.0285 - val_loss: 0.1029 - lr: 1.2800e-07 - 918ms/epoch - 37ms/step\n",
      "Epoch 171: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 2.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 3s - 3s/epoch - 385ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adagrad.Adagrad object at 0x000001BE9E76D160>; total time= 3.0min\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.5061 - val_loss: 0.8009 - lr: 0.0100 - 19s/epoch - 766ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 2.7384 - val_loss: 2.0276 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9793 - val_loss: 1.6089 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.9533 - val_loss: 2.0119 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7646 - val_loss: 2.0010 - lr: 0.0020 - 906ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7666 - val_loss: 2.0006 - lr: 0.0020 - 923ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7671 - val_loss: 2.0010 - lr: 0.0020 - 900ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7496 - val_loss: 2.0011 - lr: 4.0000e-04 - 906ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7498 - val_loss: 2.0020 - lr: 4.0000e-04 - 916ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7499 - val_loss: 2.0029 - lr: 4.0000e-04 - 894ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7461 - val_loss: 2.0031 - lr: 8.0000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7461 - val_loss: 2.0033 - lr: 8.0000e-05 - 906ms/epoch - 36ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7461 - val_loss: 2.0035 - lr: 8.0000e-05 - 932ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7454 - val_loss: 2.0035 - lr: 1.6000e-05 - 920ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7453 - val_loss: 2.0036 - lr: 1.6000e-05 - 885ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7454 - val_loss: 2.0036 - lr: 1.6000e-05 - 910ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7452 - val_loss: 2.0036 - lr: 3.2000e-06 - 910ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7451 - val_loss: 2.0036 - lr: 3.2000e-06 - 882ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7452 - val_loss: 2.0037 - lr: 3.2000e-06 - 957ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7451 - val_loss: 2.0037 - lr: 6.4000e-07 - 872ms/epoch - 35ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7451 - val_loss: 2.0037 - lr: 6.4000e-07 - 905ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.3s\n",
      "7/7 - 3s - 3s/epoch - 383ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  43.0s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 1.7281 - val_loss: 0.4915 - lr: 0.0100 - 19s/epoch - 779ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.7258 - val_loss: 2.2525 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8446 - val_loss: 1.9474 - lr: 0.0100 - 870ms/epoch - 35ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8793 - val_loss: 1.9039 - lr: 0.0100 - 901ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8784 - val_loss: 1.9353 - lr: 0.0100 - 983ms/epoch - 39ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.8643 - val_loss: 1.9393 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7810 - val_loss: 1.9462 - lr: 0.0020 - 884ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7825 - val_loss: 1.9674 - lr: 0.0020 - 918ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7813 - val_loss: 1.9845 - lr: 0.0020 - 884ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7805 - val_loss: 1.9967 - lr: 0.0020 - 890ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7799 - val_loss: 2.0051 - lr: 0.0020 - 898ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7795 - val_loss: 2.0108 - lr: 0.0020 - 915ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7796 - val_loss: 2.0145 - lr: 0.0020 - 880ms/epoch - 35ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7797 - val_loss: 2.0169 - lr: 0.0020 - 902ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.7796 - val_loss: 2.0184 - lr: 0.0020 - 904ms/epoch - 36ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7624 - val_loss: 2.0190 - lr: 4.0000e-04 - 919ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7626 - val_loss: 2.0205 - lr: 4.0000e-04 - 1s/epoch - 42ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7627 - val_loss: 2.0219 - lr: 4.0000e-04 - 1s/epoch - 41ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7627 - val_loss: 2.0232 - lr: 4.0000e-04 - 931ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7587 - val_loss: 2.0235 - lr: 8.0000e-05 - 897ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7588 - val_loss: 2.0239 - lr: 8.0000e-05 - 920ms/epoch - 37ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.1s\n",
      "7/7 - 3s - 3s/epoch - 446ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  43.1s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 1.6929 - val_loss: 0.9749 - lr: 0.0100 - 20s/epoch - 797ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 1.6065 - val_loss: 2.1096 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 1.0093 - val_loss: 2.2317 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.9565 - val_loss: 2.1623 - lr: 0.0100 - 876ms/epoch - 35ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9549 - val_loss: 2.1724 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9409 - val_loss: 2.1845 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.9278 - val_loss: 2.1859 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.9193 - val_loss: 2.1850 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9131 - val_loss: 2.1852 - lr: 0.0100 - 887ms/epoch - 35ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.9077 - val_loss: 2.1861 - lr: 0.0100 - 899ms/epoch - 36ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.9028 - val_loss: 2.1856 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8987 - val_loss: 2.1842 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8959 - val_loss: 2.1828 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8938 - val_loss: 2.1825 - lr: 0.0100 - 876ms/epoch - 35ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8920 - val_loss: 2.1831 - lr: 0.0100 - 871ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8904 - val_loss: 2.1846 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8885 - val_loss: 2.1864 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8860 - val_loss: 2.1866 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8811 - val_loss: 2.1787 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8771 - val_loss: 2.1708 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8770 - val_loss: 2.1714 - lr: 0.0100 - 914ms/epoch - 37ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  40.1s\n",
      "7/7 - 3s - 3s/epoch - 403ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  42.8s\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 - 20s - loss: 0.4566 - val_loss: 0.8092 - lr: 0.0100 - 20s/epoch - 790ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 2.3455 - val_loss: 0.4041 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3025 - val_loss: 0.4396 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.5180 - val_loss: 0.3690 - lr: 0.0100 - 892ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2241 - val_loss: 0.2461 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4116 - val_loss: 0.2464 - lr: 0.0100 - 889ms/epoch - 36ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2049 - val_loss: 0.3112 - lr: 0.0100 - 893ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1613 - val_loss: 0.2594 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1253 - val_loss: 0.2797 - lr: 0.0100 - 906ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.1190 - val_loss: 0.2474 - lr: 0.0100 - 880ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1178 - val_loss: 0.2533 - lr: 0.0100 - 890ms/epoch - 36ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.1171 - val_loss: 0.2513 - lr: 0.0100 - 874ms/epoch - 35ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.1170 - val_loss: 0.2518 - lr: 0.0100 - 898ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.1168 - val_loss: 0.2518 - lr: 0.0100 - 915ms/epoch - 37ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.1167 - val_loss: 0.2519 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.1165 - val_loss: 0.2519 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.1165 - val_loss: 0.2520 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.1163 - val_loss: 0.2521 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.1164 - val_loss: 0.2522 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.1165 - val_loss: 0.2523 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1164 - val_loss: 0.2523 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0993 - val_loss: 0.2513 - lr: 0.0020 - 910ms/epoch - 36ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0976 - val_loss: 0.2499 - lr: 0.0020 - 903ms/epoch - 36ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0966 - val_loss: 0.2492 - lr: 0.0020 - 909ms/epoch - 36ms/step\n",
      "Epoch 25/500\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "25/25 - 1s - loss: 0.0963 - val_loss: 0.2489 - lr: 0.0020 - 897ms/epoch - 36ms/step\n",
      "Epoch 25: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.7s\n",
      "7/7 - 3s - 3s/epoch - 455ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  46.8s\n",
      "Epoch 1/500\n",
      "25/25 - 23s - loss: 0.6639 - val_loss: 2.2918 - lr: 0.0100 - 23s/epoch - 903ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.4417 - val_loss: 3.3466 - lr: 0.0100 - 917ms/epoch - 37ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.3596 - val_loss: 3.8094 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2402 - val_loss: 3.7110 - lr: 0.0100 - 888ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2461 - val_loss: 3.6906 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.2428 - val_loss: 3.6928 - lr: 0.0100 - 928ms/epoch - 37ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2399 - val_loss: 3.6807 - lr: 0.0100 - 895ms/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2385 - val_loss: 3.6772 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.2372 - val_loss: 3.6754 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2359 - val_loss: 3.6738 - lr: 0.0100 - 883ms/epoch - 35ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2349 - val_loss: 3.6722 - lr: 0.0100 - 854ms/epoch - 34ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2339 - val_loss: 3.6720 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2332 - val_loss: 3.6717 - lr: 0.0100 - 894ms/epoch - 36ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2325 - val_loss: 3.6739 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2318 - val_loss: 3.6731 - lr: 0.0100 - 884ms/epoch - 35ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2315 - val_loss: 3.6742 - lr: 0.0100 - 873ms/epoch - 35ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2306 - val_loss: 3.6750 - lr: 0.0100 - 974ms/epoch - 39ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2304 - val_loss: 3.6760 - lr: 0.0100 - 868ms/epoch - 35ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2299 - val_loss: 3.6759 - lr: 0.0100 - 913ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2295 - val_loss: 3.6783 - lr: 0.0100 - 905ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2292 - val_loss: 3.6817 - lr: 0.0100 - 891ms/epoch - 36ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  43.2s\n",
      "7/7 - 3s - 3s/epoch - 390ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.adamax.Adamax object at 0x000001BE9E76D2E0>; total time=  45.9s\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 0.5998 - val_loss: 0.3112 - lr: 0.0100 - 22s/epoch - 868ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.6486 - val_loss: 0.4137 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5739 - val_loss: 0.3790 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4916 - val_loss: 0.3387 - lr: 0.0100 - 1s/epoch - 47ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.6401 - val_loss: 0.3046 - lr: 0.0100 - 1s/epoch - 48ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.6969 - val_loss: 0.2678 - lr: 0.0100 - 1s/epoch - 47ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.7572 - val_loss: 0.2474 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 1.7351 - val_loss: 1.0888 - lr: 0.0020 - 1s/epoch - 48ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9132 - val_loss: 1.4756 - lr: 0.0020 - 1s/epoch - 47ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8177 - val_loss: 1.5024 - lr: 0.0020 - 1s/epoch - 47ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8112 - val_loss: 1.5873 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7967 - val_loss: 1.6687 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.7844 - val_loss: 1.7316 - lr: 4.0000e-04 - 1s/epoch - 47ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7676 - val_loss: 1.7452 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7661 - val_loss: 1.7586 - lr: 8.0000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.7646 - val_loss: 1.7713 - lr: 8.0000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7612 - val_loss: 1.7739 - lr: 1.6000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7609 - val_loss: 1.7765 - lr: 1.6000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7606 - val_loss: 1.7790 - lr: 1.6000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7600 - val_loss: 1.7795 - lr: 3.2000e-06 - 1s/epoch - 47ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.7599 - val_loss: 1.7800 - lr: 3.2000e-06 - 1s/epoch - 47ms/step\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7599 - val_loss: 1.7805 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.7597 - val_loss: 1.7806 - lr: 6.4000e-07 - 1s/epoch - 47ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.7597 - val_loss: 1.7807 - lr: 6.4000e-07 - 1s/epoch - 48ms/step\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.7597 - val_loss: 1.7808 - lr: 6.4000e-07 - 1s/epoch - 47ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.7597 - val_loss: 1.7808 - lr: 1.2800e-07 - 1s/epoch - 47ms/step\n",
      "Epoch 27/500\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "25/25 - 1s - loss: 0.7597 - val_loss: 1.7809 - lr: 1.2800e-07 - 1s/epoch - 49ms/step\n",
      "Epoch 27: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  55.1s\n",
      "7/7 - 3s - 3s/epoch - 393ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  57.8s\n",
      "Epoch 1/500\n",
      "25/25 - 22s - loss: 0.8708 - val_loss: 0.3389 - lr: 0.0100 - 22s/epoch - 872ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1674 - val_loss: 0.3574 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.2867 - val_loss: 0.2515 - lr: 0.0100 - 1s/epoch - 53ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.3739 - val_loss: 0.3583 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.8449 - val_loss: 0.4751 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 1.3257 - val_loss: 0.9375 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 1.0154 - val_loss: 1.2587 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8943 - val_loss: 1.4808 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8288 - val_loss: 1.5215 - lr: 4.0000e-04 - 1s/epoch - 50ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8199 - val_loss: 1.5606 - lr: 4.0000e-04 - 1s/epoch - 47ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8122 - val_loss: 1.5973 - lr: 4.0000e-04 - 1s/epoch - 47ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8026 - val_loss: 1.6048 - lr: 8.0000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8013 - val_loss: 1.6122 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8000 - val_loss: 1.6196 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7979 - val_loss: 1.6211 - lr: 1.6000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7977 - val_loss: 1.6226 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.7975 - val_loss: 1.6241 - lr: 1.6000e-05 - 1s/epoch - 51ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7972 - val_loss: 1.6244 - lr: 3.2000e-06 - 1s/epoch - 52ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7970 - val_loss: 1.6247 - lr: 3.2000e-06 - 1s/epoch - 49ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.7970 - val_loss: 1.6250 - lr: 3.2000e-06 - 1s/epoch - 49ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.7970 - val_loss: 1.6251 - lr: 6.4000e-07 - 1s/epoch - 50ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.7969 - val_loss: 1.6251 - lr: 6.4000e-07 - 1s/epoch - 48ms/step\n",
      "Epoch 23/500\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "25/25 - 1s - loss: 0.7969 - val_loss: 1.6252 - lr: 6.4000e-07 - 1s/epoch - 51ms/step\n",
      "Epoch 23: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  51.0s\n",
      "7/7 - 3s - 3s/epoch - 416ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  53.9s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.7567 - val_loss: 0.4755 - lr: 0.0100 - 21s/epoch - 855ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5367 - val_loss: 0.3704 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.6540 - val_loss: 0.3903 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 1.1706 - val_loss: 1.1825 - lr: 0.0100 - 1s/epoch - 48ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 1.0393 - val_loss: 1.7587 - lr: 0.0100 - 1s/epoch - 52ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9049 - val_loss: 1.8241 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8934 - val_loss: 1.8768 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.8853 - val_loss: 1.9213 - lr: 0.0020 - 1s/epoch - 51ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8736 - val_loss: 1.9302 - lr: 4.0000e-04 - 1s/epoch - 55ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8724 - val_loss: 1.9388 - lr: 4.0000e-04 - 1s/epoch - 53ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.8713 - val_loss: 1.9473 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8691 - val_loss: 1.9490 - lr: 8.0000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8689 - val_loss: 1.9507 - lr: 8.0000e-05 - 1s/epoch - 51ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.8687 - val_loss: 1.9524 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8682 - val_loss: 1.9527 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8682 - val_loss: 1.9531 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.8681 - val_loss: 1.9534 - lr: 1.6000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8680 - val_loss: 1.9535 - lr: 3.2000e-06 - 1s/epoch - 51ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8680 - val_loss: 1.9535 - lr: 3.2000e-06 - 1s/epoch - 51ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.8680 - val_loss: 1.9536 - lr: 3.2000e-06 - 1s/epoch - 52ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.8680 - val_loss: 1.9536 - lr: 6.4000e-07 - 1s/epoch - 54ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "25/25 - 1s - loss: 0.8680 - val_loss: 1.9536 - lr: 6.4000e-07 - 1s/epoch - 55ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  51.2s\n",
      "7/7 - 3s - 3s/epoch - 475ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  54.5s\n",
      "Epoch 1/500\n",
      "25/25 - 23s - loss: 1.2973 - val_loss: 0.2872 - lr: 0.0100 - 23s/epoch - 935ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.1829 - val_loss: 0.3351 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.0962 - val_loss: 0.3342 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.0960 - val_loss: 0.3376 - lr: 0.0100 - 1s/epoch - 48ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.1014 - val_loss: 0.3370 - lr: 0.0100 - 1s/epoch - 48ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.1061 - val_loss: 0.3317 - lr: 0.0100 - 1s/epoch - 51ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.1098 - val_loss: 0.3225 - lr: 0.0100 - 1s/epoch - 52ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.1475 - val_loss: 0.2544 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.1052 - val_loss: 0.2467 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.1058 - val_loss: 0.2467 - lr: 0.0020 - 1s/epoch - 49ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.1056 - val_loss: 0.2464 - lr: 4.0000e-04 - 1s/epoch - 52ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.0999 - val_loss: 0.2462 - lr: 4.0000e-04 - 1s/epoch - 51ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.0974 - val_loss: 0.2463 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.0944 - val_loss: 0.2463 - lr: 8.0000e-05 - 1s/epoch - 54ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.0941 - val_loss: 0.2464 - lr: 8.0000e-05 - 1s/epoch - 52ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.0939 - val_loss: 0.2464 - lr: 8.0000e-05 - 1s/epoch - 52ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.0937 - val_loss: 0.2465 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.0936 - val_loss: 0.2465 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.0935 - val_loss: 0.2466 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.0934 - val_loss: 0.2467 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 21/500\n",
      "25/25 - 1s - loss: 0.0933 - val_loss: 0.2467 - lr: 8.0000e-05 - 1s/epoch - 50ms/step\n",
      "Epoch 22/500\n",
      "25/25 - 1s - loss: 0.0932 - val_loss: 0.2468 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 23/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2468 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 24/500\n",
      "25/25 - 1s - loss: 0.0932 - val_loss: 0.2469 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 25/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2470 - lr: 8.0000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 26/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2470 - lr: 8.0000e-05 - 1s/epoch - 49ms/step\n",
      "Epoch 27/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2471 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 28/500\n",
      "25/25 - 1s - loss: 0.0931 - val_loss: 0.2471 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 29/500\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2472 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 30/500\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2472 - lr: 8.0000e-05 - 1s/epoch - 52ms/step\n",
      "Epoch 31/500\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2473 - lr: 8.0000e-05 - 1s/epoch - 55ms/step\n",
      "Epoch 32/500\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "25/25 - 1s - loss: 0.0930 - val_loss: 0.2473 - lr: 8.0000e-05 - 1s/epoch - 52ms/step\n",
      "Epoch 32: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n",
      "7/7 - 3s - 3s/epoch - 397ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time= 1.1min\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.5875 - val_loss: 2.0862 - lr: 0.0100 - 21s/epoch - 844ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.2086 - val_loss: 0.7180 - lr: 0.0100 - 1s/epoch - 49ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.1900 - val_loss: 1.1900 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.2182 - val_loss: 1.3481 - lr: 0.0100 - 1s/epoch - 47ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.2506 - val_loss: 2.4666 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "25/25 - 1s - loss: 0.2450 - val_loss: 2.6617 - lr: 0.0100 - 1s/epoch - 50ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.2972 - val_loss: 3.0210 - lr: 0.0020 - 1s/epoch - 50ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.2458 - val_loss: 3.3252 - lr: 0.0020 - 1s/epoch - 48ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "25/25 - 1s - loss: 0.2205 - val_loss: 3.4450 - lr: 0.0020 - 1s/epoch - 48ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.2112 - val_loss: 3.4888 - lr: 4.0000e-04 - 1s/epoch - 48ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2090 - val_loss: 3.5342 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "25/25 - 1s - loss: 0.2068 - val_loss: 3.5727 - lr: 4.0000e-04 - 1s/epoch - 49ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2037 - val_loss: 3.5812 - lr: 8.0000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2034 - val_loss: 3.5901 - lr: 8.0000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "25/25 - 1s - loss: 0.2030 - val_loss: 3.5987 - lr: 8.0000e-05 - 1s/epoch - 51ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2023 - val_loss: 3.6005 - lr: 1.6000e-05 - 1s/epoch - 48ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 2s - loss: 0.2022 - val_loss: 3.6024 - lr: 1.6000e-05 - 2s/epoch - 65ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "25/25 - 1s - loss: 0.2022 - val_loss: 3.6042 - lr: 1.6000e-05 - 1s/epoch - 47ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2021 - val_loss: 3.6046 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2020 - val_loss: 3.6050 - lr: 3.2000e-06 - 1s/epoch - 47ms/step\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "25/25 - 1s - loss: 0.2020 - val_loss: 3.6054 - lr: 3.2000e-06 - 1s/epoch - 48ms/step\n",
      "Epoch 22/500\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "25/25 - 1s - loss: 0.2019 - val_loss: 3.6055 - lr: 6.4000e-07 - 1s/epoch - 51ms/step\n",
      "Epoch 22: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  50.4s\n",
      "7/7 - 3s - 3s/epoch - 384ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.nadam.Nadam object at 0x000001BE9E76D0A0>; total time=  53.0s\n",
      "Epoch 1/500\n",
      "25/25 - 21s - loss: 0.7637 - val_loss: 1.8311 - lr: 0.0100 - 21s/epoch - 826ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7560 - val_loss: 1.8451 - lr: 0.0100 - 972ms/epoch - 39ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7546 - val_loss: 1.8537 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7538 - val_loss: 1.8600 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7532 - val_loss: 1.8651 - lr: 0.0100 - 975ms/epoch - 39ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7528 - val_loss: 1.8695 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7524 - val_loss: 1.8733 - lr: 0.0100 - 941ms/epoch - 38ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7521 - val_loss: 1.8767 - lr: 0.0100 - 929ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7519 - val_loss: 1.8798 - lr: 0.0100 - 907ms/epoch - 36ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7516 - val_loss: 1.8826 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7514 - val_loss: 1.8853 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7512 - val_loss: 1.8877 - lr: 0.0100 - 966ms/epoch - 39ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7511 - val_loss: 1.8900 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7509 - val_loss: 1.8922 - lr: 0.0100 - 911ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7507 - val_loss: 1.8943 - lr: 0.0100 - 944ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7506 - val_loss: 1.8962 - lr: 0.0100 - 954ms/epoch - 38ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7505 - val_loss: 1.8981 - lr: 0.0100 - 972ms/epoch - 39ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7504 - val_loss: 1.8999 - lr: 0.0100 - 981ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7502 - val_loss: 1.9016 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7501 - val_loss: 1.9032 - lr: 0.0100 - 952ms/epoch - 38ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7500 - val_loss: 1.9048 - lr: 0.0100 - 970ms/epoch - 39ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.8s\n",
      "7/7 - 3s - 3s/epoch - 414ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  44.6s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.7786 - val_loss: 1.8300 - lr: 0.0100 - 20s/epoch - 788ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.7709 - val_loss: 1.8452 - lr: 0.0100 - 1s/epoch - 42ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.7693 - val_loss: 1.8546 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.7684 - val_loss: 1.8617 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.7677 - val_loss: 1.8675 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.7672 - val_loss: 1.8725 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.7668 - val_loss: 1.8768 - lr: 0.0100 - 955ms/epoch - 38ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.7664 - val_loss: 1.8807 - lr: 0.0100 - 982ms/epoch - 39ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.7661 - val_loss: 1.8843 - lr: 0.0100 - 993ms/epoch - 40ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.7658 - val_loss: 1.8876 - lr: 0.0100 - 934ms/epoch - 37ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.7656 - val_loss: 1.8906 - lr: 0.0100 - 978ms/epoch - 39ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.7653 - val_loss: 1.8935 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.7651 - val_loss: 1.8961 - lr: 0.0100 - 942ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.7649 - val_loss: 1.8987 - lr: 0.0100 - 946ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.7647 - val_loss: 1.9011 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.7645 - val_loss: 1.9034 - lr: 0.0100 - 940ms/epoch - 38ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.7644 - val_loss: 1.9056 - lr: 0.0100 - 912ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.7642 - val_loss: 1.9077 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.7641 - val_loss: 1.9097 - lr: 0.0100 - 950ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.7639 - val_loss: 1.9117 - lr: 0.0100 - 919ms/epoch - 37ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.7638 - val_loss: 1.9136 - lr: 0.0100 - 998ms/epoch - 40ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.8s\n",
      "7/7 - 3s - 3s/epoch - 388ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  44.5s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.8947 - val_loss: 1.8494 - lr: 0.0100 - 19s/epoch - 777ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.8824 - val_loss: 1.8734 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.8789 - val_loss: 1.8895 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.8766 - val_loss: 1.9021 - lr: 0.0100 - 910ms/epoch - 36ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.8749 - val_loss: 1.9127 - lr: 0.0100 - 921ms/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.8734 - val_loss: 1.9220 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.8722 - val_loss: 1.9304 - lr: 0.0100 - 943ms/epoch - 38ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.8711 - val_loss: 1.9381 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.8701 - val_loss: 1.9453 - lr: 0.0100 - 935ms/epoch - 37ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.8692 - val_loss: 1.9521 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8684 - val_loss: 1.9585 - lr: 0.0100 - 925ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8676 - val_loss: 1.9648 - lr: 0.0100 - 985ms/epoch - 39ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8669 - val_loss: 1.9708 - lr: 0.0100 - 968ms/epoch - 39ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8662 - val_loss: 1.9767 - lr: 0.0100 - 904ms/epoch - 36ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8655 - val_loss: 1.9824 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8649 - val_loss: 1.9881 - lr: 0.0100 - 922ms/epoch - 37ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8643 - val_loss: 1.9937 - lr: 0.0100 - 897ms/epoch - 36ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8637 - val_loss: 1.9993 - lr: 0.0100 - 979ms/epoch - 39ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8631 - val_loss: 2.0049 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8625 - val_loss: 2.0104 - lr: 0.0100 - 902ms/epoch - 36ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8619 - val_loss: 2.0160 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.3s\n",
      "7/7 - 3s - 3s/epoch - 393ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  44.0s\n",
      "Epoch 1/500\n",
      "25/25 - 19s - loss: 0.9508 - val_loss: 1.8689 - lr: 0.0100 - 19s/epoch - 772ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.9318 - val_loss: 1.9038 - lr: 0.0100 - 974ms/epoch - 39ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.9247 - val_loss: 1.9284 - lr: 0.0100 - 991ms/epoch - 40ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.9197 - val_loss: 1.9483 - lr: 0.0100 - 999ms/epoch - 40ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.9157 - val_loss: 1.9655 - lr: 0.0100 - 954ms/epoch - 38ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.9124 - val_loss: 1.9810 - lr: 0.0100 - 964ms/epoch - 39ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.9094 - val_loss: 1.9955 - lr: 0.0100 - 930ms/epoch - 37ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.9067 - val_loss: 2.0094 - lr: 0.0100 - 952ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.9041 - val_loss: 2.0229 - lr: 0.0100 - 977ms/epoch - 39ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.9017 - val_loss: 2.0364 - lr: 0.0100 - 938ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.8993 - val_loss: 2.0501 - lr: 0.0100 - 968ms/epoch - 39ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.8969 - val_loss: 2.0642 - lr: 0.0100 - 951ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.8946 - val_loss: 2.0787 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.8922 - val_loss: 2.0939 - lr: 0.0100 - 1s/epoch - 40ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.8898 - val_loss: 2.1098 - lr: 0.0100 - 982ms/epoch - 39ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.8873 - val_loss: 2.1264 - lr: 0.0100 - 1s/epoch - 41ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.8849 - val_loss: 2.1437 - lr: 0.0100 - 957ms/epoch - 38ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.8824 - val_loss: 2.1616 - lr: 0.0100 - 957ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.8800 - val_loss: 2.1799 - lr: 0.0100 - 920ms/epoch - 37ms/step\n",
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.8776 - val_loss: 2.1984 - lr: 0.0100 - 965ms/epoch - 39ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.8753 - val_loss: 2.2171 - lr: 0.0100 - 980ms/epoch - 39ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.7s\n",
      "7/7 - 3s - 3s/epoch - 402ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  44.5s\n",
      "Epoch 1/500\n",
      "25/25 - 20s - loss: 0.6003 - val_loss: 1.9226 - lr: 0.0100 - 20s/epoch - 792ms/step\n",
      "Epoch 2/500\n",
      "25/25 - 1s - loss: 0.5472 - val_loss: 2.0069 - lr: 0.0100 - 995ms/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "25/25 - 1s - loss: 0.5144 - val_loss: 2.0749 - lr: 0.0100 - 933ms/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "25/25 - 1s - loss: 0.4878 - val_loss: 2.1382 - lr: 0.0100 - 924ms/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "25/25 - 1s - loss: 0.4633 - val_loss: 2.2044 - lr: 0.0100 - 969ms/epoch - 39ms/step\n",
      "Epoch 6/500\n",
      "25/25 - 1s - loss: 0.4378 - val_loss: 2.2816 - lr: 0.0100 - 945ms/epoch - 38ms/step\n",
      "Epoch 7/500\n",
      "25/25 - 1s - loss: 0.4091 - val_loss: 2.3792 - lr: 0.0100 - 961ms/epoch - 38ms/step\n",
      "Epoch 8/500\n",
      "25/25 - 1s - loss: 0.3757 - val_loss: 2.5053 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 9/500\n",
      "25/25 - 1s - loss: 0.3383 - val_loss: 2.6617 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 10/500\n",
      "25/25 - 1s - loss: 0.3006 - val_loss: 2.8394 - lr: 0.0100 - 957ms/epoch - 38ms/step\n",
      "Epoch 11/500\n",
      "25/25 - 1s - loss: 0.2676 - val_loss: 3.0188 - lr: 0.0100 - 927ms/epoch - 37ms/step\n",
      "Epoch 12/500\n",
      "25/25 - 1s - loss: 0.2428 - val_loss: 3.1787 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 13/500\n",
      "25/25 - 1s - loss: 0.2263 - val_loss: 3.3065 - lr: 0.0100 - 936ms/epoch - 37ms/step\n",
      "Epoch 14/500\n",
      "25/25 - 1s - loss: 0.2164 - val_loss: 3.4006 - lr: 0.0100 - 948ms/epoch - 38ms/step\n",
      "Epoch 15/500\n",
      "25/25 - 1s - loss: 0.2107 - val_loss: 3.4666 - lr: 0.0100 - 939ms/epoch - 38ms/step\n",
      "Epoch 16/500\n",
      "25/25 - 1s - loss: 0.2073 - val_loss: 3.5120 - lr: 0.0100 - 908ms/epoch - 36ms/step\n",
      "Epoch 17/500\n",
      "25/25 - 1s - loss: 0.2054 - val_loss: 3.5432 - lr: 0.0100 - 953ms/epoch - 38ms/step\n",
      "Epoch 18/500\n",
      "25/25 - 1s - loss: 0.2042 - val_loss: 3.5650 - lr: 0.0100 - 960ms/epoch - 38ms/step\n",
      "Epoch 19/500\n",
      "25/25 - 1s - loss: 0.2035 - val_loss: 3.5805 - lr: 0.0100 - 1s/epoch - 42ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "25/25 - 1s - loss: 0.2029 - val_loss: 3.5921 - lr: 0.0100 - 956ms/epoch - 38ms/step\n",
      "Epoch 21/500\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "25/25 - 1s - loss: 0.2026 - val_loss: 3.6008 - lr: 0.0100 - 947ms/epoch - 38ms/step\n",
      "Epoch 21: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total=  41.8s\n",
      "7/7 - 3s - 3s/epoch - 391ms/step\n",
      "[CV] END model__dropoutFoward=0.1, model__layers=5, model__n_lstm=100, model__optimizer=<keras.optimizers.optimizer_v2.ftrl.Ftrl object at 0x000001BE9E76D100>; total time=  44.5s\n",
      "Epoch 1/500\n",
      "31/31 - 18s - loss: 0.8221 - val_loss: 1.7446 - lr: 0.0100 - 18s/epoch - 575ms/step\n",
      "Epoch 2/500\n",
      "31/31 - 1s - loss: 0.7702 - val_loss: 1.6358 - lr: 0.0100 - 978ms/epoch - 32ms/step\n",
      "Epoch 3/500\n",
      "31/31 - 1s - loss: 0.6957 - val_loss: 1.4491 - lr: 0.0100 - 935ms/epoch - 30ms/step\n",
      "Epoch 4/500\n",
      "31/31 - 1s - loss: 0.5562 - val_loss: 1.0839 - lr: 0.0100 - 994ms/epoch - 32ms/step\n",
      "Epoch 5/500\n",
      "31/31 - 1s - loss: 0.3167 - val_loss: 0.5585 - lr: 0.0100 - 967ms/epoch - 31ms/step\n",
      "Epoch 6/500\n",
      "31/31 - 1s - loss: 0.1236 - val_loss: 0.2761 - lr: 0.0100 - 1s/epoch - 33ms/step\n",
      "Epoch 7/500\n",
      "31/31 - 1s - loss: 0.0772 - val_loss: 0.2009 - lr: 0.0100 - 1s/epoch - 33ms/step\n",
      "Epoch 8/500\n",
      "31/31 - 1s - loss: 0.0653 - val_loss: 0.1746 - lr: 0.0100 - 1s/epoch - 33ms/step\n",
      "Epoch 9/500\n",
      "31/31 - 1s - loss: 0.0595 - val_loss: 0.1616 - lr: 0.0100 - 980ms/epoch - 32ms/step\n",
      "Epoch 10/500\n",
      "31/31 - 1s - loss: 0.0552 - val_loss: 0.1550 - lr: 0.0100 - 1s/epoch - 33ms/step\n",
      "Epoch 11/500\n",
      "31/31 - 1s - loss: 0.0525 - val_loss: 0.1519 - lr: 0.0100 - 1s/epoch - 33ms/step\n",
      "Epoch 12/500\n",
      "31/31 - 1s - loss: 0.0515 - val_loss: 0.1505 - lr: 0.0100 - 1s/epoch - 35ms/step\n",
      "Epoch 13/500\n",
      "31/31 - 1s - loss: 0.0505 - val_loss: 0.1499 - lr: 0.0100 - 1s/epoch - 33ms/step\n",
      "Epoch 14/500\n",
      "31/31 - 1s - loss: 0.0500 - val_loss: 0.1497 - lr: 0.0100 - 987ms/epoch - 32ms/step\n",
      "Epoch 15/500\n",
      "31/31 - 1s - loss: 0.0494 - val_loss: 0.1497 - lr: 0.0100 - 977ms/epoch - 32ms/step\n",
      "Epoch 16/500\n",
      "31/31 - 1s - loss: 0.0492 - val_loss: 0.1499 - lr: 0.0100 - 987ms/epoch - 32ms/step\n",
      "Epoch 17/500\n",
      "31/31 - 1s - loss: 0.0492 - val_loss: 0.1499 - lr: 0.0100 - 1s/epoch - 32ms/step\n",
      "Epoch 18/500\n",
      "31/31 - 1s - loss: 0.0488 - val_loss: 0.1500 - lr: 0.0100 - 1s/epoch - 33ms/step\n",
      "Epoch 19/500\n",
      "31/31 - 1s - loss: 0.0487 - val_loss: 0.1500 - lr: 0.0100 - 947ms/epoch - 31ms/step\n",
      "Epoch 20/500\n",
      "31/31 - 1s - loss: 0.0486 - val_loss: 0.1502 - lr: 0.0100 - 955ms/epoch - 31ms/step\n",
      "Epoch 21/500\n",
      "31/31 - 1s - loss: 0.0485 - val_loss: 0.1501 - lr: 0.0100 - 938ms/epoch - 30ms/step\n",
      "Epoch 22/500\n",
      "31/31 - 1s - loss: 0.0486 - val_loss: 0.1498 - lr: 0.0100 - 972ms/epoch - 31ms/step\n",
      "Epoch 23/500\n",
      "31/31 - 1s - loss: 0.0482 - val_loss: 0.1503 - lr: 0.0100 - 940ms/epoch - 30ms/step\n",
      "Epoch 24/500\n",
      "31/31 - 1s - loss: 0.0483 - val_loss: 0.1499 - lr: 0.0100 - 976ms/epoch - 31ms/step\n",
      "Epoch 25/500\n",
      "31/31 - 1s - loss: 0.0480 - val_loss: 0.1497 - lr: 0.0100 - 989ms/epoch - 32ms/step\n",
      "Epoch 26/500\n",
      "31/31 - 1s - loss: 0.0481 - val_loss: 0.1499 - lr: 0.0100 - 975ms/epoch - 31ms/step\n",
      "Epoch 27/500\n",
      "31/31 - 1s - loss: 0.0475 - val_loss: 0.1498 - lr: 0.0100 - 960ms/epoch - 31ms/step\n",
      "Epoch 28/500\n",
      "31/31 - 1s - loss: 0.0476 - val_loss: 0.1494 - lr: 0.0100 - 934ms/epoch - 30ms/step\n",
      "Epoch 29/500\n",
      "31/31 - 1s - loss: 0.0477 - val_loss: 0.1493 - lr: 0.0100 - 954ms/epoch - 31ms/step\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "31/31 - 1s - loss: 0.0476 - val_loss: 0.1493 - lr: 0.0100 - 986ms/epoch - 32ms/step\n",
      "Epoch 31/500\n",
      "31/31 - 1s - loss: 0.0471 - val_loss: 0.1495 - lr: 0.0020 - 998ms/epoch - 32ms/step\n",
      "Epoch 32/500\n",
      "31/31 - 1s - loss: 0.0468 - val_loss: 0.1496 - lr: 0.0020 - 979ms/epoch - 32ms/step\n",
      "Epoch 33/500\n",
      "31/31 - 1s - loss: 0.0471 - val_loss: 0.1498 - lr: 0.0020 - 975ms/epoch - 31ms/step\n",
      "Epoch 34/500\n",
      "31/31 - 1s - loss: 0.0471 - val_loss: 0.1498 - lr: 0.0020 - 944ms/epoch - 30ms/step\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "31/31 - 1s - loss: 0.0472 - val_loss: 0.1498 - lr: 0.0020 - 950ms/epoch - 31ms/step\n",
      "Epoch 36/500\n",
      "31/31 - 1s - loss: 0.0468 - val_loss: 0.1498 - lr: 4.0000e-04 - 988ms/epoch - 32ms/step\n",
      "Epoch 37/500\n",
      "31/31 - 1s - loss: 0.0468 - val_loss: 0.1498 - lr: 4.0000e-04 - 1s/epoch - 36ms/step\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "31/31 - 1s - loss: 0.0469 - val_loss: 0.1498 - lr: 4.0000e-04 - 970ms/epoch - 31ms/step\n",
      "Epoch 39/500\n",
      "31/31 - 1s - loss: 0.0467 - val_loss: 0.1498 - lr: 8.0000e-05 - 970ms/epoch - 31ms/step\n",
      "Epoch 40/500\n",
      "31/31 - 1s - loss: 0.0471 - val_loss: 0.1498 - lr: 8.0000e-05 - 972ms/epoch - 31ms/step\n",
      "Epoch 41/500\n",
      "31/31 - 1s - loss: 0.0470 - val_loss: 0.1498 - lr: 8.0000e-05 - 966ms/epoch - 31ms/step\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "31/31 - 1s - loss: 0.0467 - val_loss: 0.1498 - lr: 8.0000e-05 - 989ms/epoch - 32ms/step\n",
      "Epoch 43/500\n",
      "31/31 - 1s - loss: 0.0467 - val_loss: 0.1498 - lr: 1.6000e-05 - 1s/epoch - 33ms/step\n",
      "Epoch 44/500\n",
      "31/31 - 1s - loss: 0.0470 - val_loss: 0.1498 - lr: 1.6000e-05 - 984ms/epoch - 32ms/step\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "31/31 - 1s - loss: 0.0467 - val_loss: 0.1498 - lr: 1.6000e-05 - 1s/epoch - 34ms/step\n",
      "Epoch 46/500\n",
      "31/31 - 1s - loss: 0.0470 - val_loss: 0.1498 - lr: 3.2000e-06 - 971ms/epoch - 31ms/step\n",
      "Epoch 47/500\n",
      "31/31 - 1s - loss: 0.0469 - val_loss: 0.1498 - lr: 3.2000e-06 - 1s/epoch - 32ms/step\n",
      "Epoch 48/500\n",
      "31/31 - 1s - loss: 0.0465 - val_loss: 0.1498 - lr: 3.2000e-06 - 985ms/epoch - 32ms/step\n",
      "Epoch 49/500\n",
      "31/31 - 1s - loss: 0.0469 - val_loss: 0.1498 - lr: 3.2000e-06 - 1s/epoch - 33ms/step\n",
      "Epoch 50/500\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "31/31 - 1s - loss: 0.0468 - val_loss: 0.1498 - lr: 3.2000e-06 - 992ms/epoch - 32ms/step\n",
      "Epoch 50: early stopping\n",
      "[Pipeline] ............. (step 1 of 1) Processing model, total= 1.1min\n"
     ]
    }
   ],
   "source": [
    "# Treinamento\n",
    "\n",
    "fit_params = {\n",
    "    'model__batch_size': batch_size,\n",
    "    'model__epochs': epochs,\n",
    "    'model__verbose': verbose,\n",
    "    'model__validation_data': (X_teste, y_teste),\n",
    "    'model__shuffle': False,\n",
    "    'model__validation_steps': None,\n",
    "    'model__validation_freq': 1,\n",
    "}\n",
    "\n",
    "grid_result = grid.fit(X_treino, y_treino, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>model__dropoutFoward</th>\n",
       "      <th>model__layers</th>\n",
       "      <th>model__n_lstm</th>\n",
       "      <th>model__optimizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.232667</td>\n",
       "      <td>52.675592</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adadelta.Adadel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.235127</td>\n",
       "      <td>73.736135</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adagrad.Adagrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.241221</td>\n",
       "      <td>86.276316</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adagrad.Adagrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.260211</td>\n",
       "      <td>70.981013</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adadelta.Adadel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.263071</td>\n",
       "      <td>94.735343</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adagrad.Adagrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.264119</td>\n",
       "      <td>82.093122</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adagrad.Adagrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.282791</td>\n",
       "      <td>64.739633</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.gradient_descen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.304580</td>\n",
       "      <td>96.789059</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adadelta.Adadel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.317017</td>\n",
       "      <td>85.266875</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.gradient_descen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.318183</td>\n",
       "      <td>94.010459</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adadelta.Adadel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.322897</td>\n",
       "      <td>69.721895</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.gradient_descen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.378020</td>\n",
       "      <td>68.002068</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.gradient_descen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.558713</td>\n",
       "      <td>41.991449</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adamax.Adamax o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.601692</td>\n",
       "      <td>68.167079</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.rmsprop.RMSprop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.619014</td>\n",
       "      <td>57.150544</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adamax.Adamax o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.655501</td>\n",
       "      <td>79.501801</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.rmsprop.RMSprop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.702631</td>\n",
       "      <td>46.142523</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adamax.Adamax o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.762921</td>\n",
       "      <td>57.150261</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.rmsprop.RMSprop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.852024</td>\n",
       "      <td>45.793454</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.ftrl.Ftrl objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.852044</td>\n",
       "      <td>34.965003</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.ftrl.Ftrl objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.852093</td>\n",
       "      <td>35.438123</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.ftrl.Ftrl objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.852093</td>\n",
       "      <td>41.676276</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.ftrl.Ftrl objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.360497</td>\n",
       "      <td>57.132952</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.rmsprop.RMSprop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.457188</td>\n",
       "      <td>45.897721</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.nadam.Nadam obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.509947</td>\n",
       "      <td>41.460106</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adamax.Adamax o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.520282</td>\n",
       "      <td>43.906808</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.nadam.Nadam obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.525304</td>\n",
       "      <td>35.709610</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adam.Adam objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.600085</td>\n",
       "      <td>41.160402</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adam.Adam objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.606317</td>\n",
       "      <td>54.604867</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.nadam.Nadam obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1.680644</td>\n",
       "      <td>34.756444</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adam.Adam objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1.969450</td>\n",
       "      <td>50.160298</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.nadam.Nadam obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-2.291920</td>\n",
       "      <td>40.064779</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;keras.optimizers.optimizer_v2.adam.Adam objec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score  mean_fit_time  model__dropoutFoward  \\\n",
       "rank_test_score                                                         \n",
       "1                      -0.232667      52.675592                   0.1   \n",
       "2                      -0.235127      73.736135                   0.1   \n",
       "3                      -0.241221      86.276316                   0.1   \n",
       "4                      -0.260211      70.981013                   0.1   \n",
       "5                      -0.263071      94.735343                   0.1   \n",
       "6                      -0.264119      82.093122                   0.1   \n",
       "7                      -0.282791      64.739633                   0.1   \n",
       "8                      -0.304580      96.789059                   0.1   \n",
       "9                      -0.317017      85.266875                   0.1   \n",
       "10                     -0.318183      94.010459                   0.1   \n",
       "11                     -0.322897      69.721895                   0.1   \n",
       "12                     -0.378020      68.002068                   0.1   \n",
       "13                     -0.558713      41.991449                   0.1   \n",
       "14                     -0.601692      68.167079                   0.1   \n",
       "15                     -0.619014      57.150544                   0.1   \n",
       "16                     -0.655501      79.501801                   0.1   \n",
       "17                     -0.702631      46.142523                   0.1   \n",
       "18                     -0.762921      57.150261                   0.1   \n",
       "19                     -0.852024      45.793454                   0.1   \n",
       "20                     -0.852044      34.965003                   0.1   \n",
       "21                     -0.852093      35.438123                   0.1   \n",
       "22                     -0.852093      41.676276                   0.1   \n",
       "23                     -1.360497      57.132952                   0.1   \n",
       "24                     -1.457188      45.897721                   0.1   \n",
       "25                     -1.509947      41.460106                   0.1   \n",
       "26                     -1.520282      43.906808                   0.1   \n",
       "27                     -1.525304      35.709610                   0.1   \n",
       "28                     -1.600085      41.160402                   0.1   \n",
       "29                     -1.606317      54.604867                   0.1   \n",
       "30                     -1.680644      34.756444                   0.1   \n",
       "31                     -1.969450      50.160298                   0.1   \n",
       "32                     -2.291920      40.064779                   0.1   \n",
       "\n",
       "                 model__layers  model__n_lstm  \\\n",
       "rank_test_score                                 \n",
       "1                            4            100   \n",
       "2                            4            100   \n",
       "3                            4             50   \n",
       "4                            5            100   \n",
       "5                            5             50   \n",
       "6                            5            100   \n",
       "7                            4            100   \n",
       "8                            4             50   \n",
       "9                            5            100   \n",
       "10                           5             50   \n",
       "11                           4             50   \n",
       "12                           5             50   \n",
       "13                           4            100   \n",
       "14                           4             50   \n",
       "15                           5             50   \n",
       "16                           5            100   \n",
       "17                           4             50   \n",
       "18                           4            100   \n",
       "19                           5             50   \n",
       "20                           4             50   \n",
       "21                           4            100   \n",
       "22                           5            100   \n",
       "23                           5             50   \n",
       "24                           4             50   \n",
       "25                           5            100   \n",
       "26                           4            100   \n",
       "27                           4            100   \n",
       "28                           5            100   \n",
       "29                           5            100   \n",
       "30                           4             50   \n",
       "31                           5             50   \n",
       "32                           5             50   \n",
       "\n",
       "                                                  model__optimizer  \n",
       "rank_test_score                                                     \n",
       "1                <keras.optimizers.optimizer_v2.adadelta.Adadel...  \n",
       "2                <keras.optimizers.optimizer_v2.adagrad.Adagrad...  \n",
       "3                <keras.optimizers.optimizer_v2.adagrad.Adagrad...  \n",
       "4                <keras.optimizers.optimizer_v2.adadelta.Adadel...  \n",
       "5                <keras.optimizers.optimizer_v2.adagrad.Adagrad...  \n",
       "6                <keras.optimizers.optimizer_v2.adagrad.Adagrad...  \n",
       "7                <keras.optimizers.optimizer_v2.gradient_descen...  \n",
       "8                <keras.optimizers.optimizer_v2.adadelta.Adadel...  \n",
       "9                <keras.optimizers.optimizer_v2.gradient_descen...  \n",
       "10               <keras.optimizers.optimizer_v2.adadelta.Adadel...  \n",
       "11               <keras.optimizers.optimizer_v2.gradient_descen...  \n",
       "12               <keras.optimizers.optimizer_v2.gradient_descen...  \n",
       "13               <keras.optimizers.optimizer_v2.adamax.Adamax o...  \n",
       "14               <keras.optimizers.optimizer_v2.rmsprop.RMSprop...  \n",
       "15               <keras.optimizers.optimizer_v2.adamax.Adamax o...  \n",
       "16               <keras.optimizers.optimizer_v2.rmsprop.RMSprop...  \n",
       "17               <keras.optimizers.optimizer_v2.adamax.Adamax o...  \n",
       "18               <keras.optimizers.optimizer_v2.rmsprop.RMSprop...  \n",
       "19               <keras.optimizers.optimizer_v2.ftrl.Ftrl objec...  \n",
       "20               <keras.optimizers.optimizer_v2.ftrl.Ftrl objec...  \n",
       "21               <keras.optimizers.optimizer_v2.ftrl.Ftrl objec...  \n",
       "22               <keras.optimizers.optimizer_v2.ftrl.Ftrl objec...  \n",
       "23               <keras.optimizers.optimizer_v2.rmsprop.RMSprop...  \n",
       "24               <keras.optimizers.optimizer_v2.nadam.Nadam obj...  \n",
       "25               <keras.optimizers.optimizer_v2.adamax.Adamax o...  \n",
       "26               <keras.optimizers.optimizer_v2.nadam.Nadam obj...  \n",
       "27               <keras.optimizers.optimizer_v2.adam.Adam objec...  \n",
       "28               <keras.optimizers.optimizer_v2.adam.Adam objec...  \n",
       "29               <keras.optimizers.optimizer_v2.nadam.Nadam obj...  \n",
       "30               <keras.optimizers.optimizer_v2.adam.Adam objec...  \n",
       "31               <keras.optimizers.optimizer_v2.nadam.Nadam obj...  \n",
       "32               <keras.optimizers.optimizer_v2.adam.Adam objec...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resultado do SearchGridCV\n",
    "\n",
    "pd.concat([\n",
    "           pd.DataFrame(grid.cv_results_)[['rank_test_score', 'mean_test_score', 'mean_fit_time']],\n",
    "           pd.DataFrame(grid.cv_results_['params'])\n",
    "          ],\n",
    "           axis=1,\n",
    "           join='inner').set_index('rank_test_score').sort_values('rank_test_score')\n",
    "\n",
    "# Ranckeamento segundo métrica do GridSearchCV: neg_root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid.best_params_\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__dropoutFoward': 0.1,\n",
       " 'model__layers': 4,\n",
       " 'model__n_lstm': 100,\n",
       " 'model__optimizer': <keras.optimizers.optimizer_v2.adadelta.Adadelta at 0x1be9e76d370>}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 - 0s - loss: 0.0468 - 420ms/epoch - 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.046799056231975555"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative mean square error - Função score do Modelo Keras encapsulado\n",
    "best_model.score(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 0.1493 - 230ms/epoch - 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.1492612510919571"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative mean square error - Função score do Modelo Keras encapsulado\n",
    "best_model.score(X_teste, y_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTableFrame(ativo, table = SP500_close):\n",
    "    \n",
    "    TimeframeTable = generatorTimeframeTable(table, ativo)\n",
    "    \n",
    "    index_data = TimeframeTable.index\n",
    "    \n",
    "    scaler = createTrainScaler(TimeframeTable)\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "    TimeframeTable = scaler.fit_transform(TimeframeTable)\n",
    "\n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    # index_data = TimeframeTable.index\n",
    "    # TimeframeTable = scaler.transform(TimeframeTable)\n",
    "    # TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    \n",
    "    for day in forecast:\n",
    "        \n",
    "        current_info = TimeframeTable.iloc[-1, 1:].to_numpy()\n",
    "        \n",
    "        standardCurrentInfo = current_info.reshape(1, steps, 1).astype('float32')\n",
    "        \n",
    "        #current_forecast = best_model.predict(scaler.transform(standardCurrentInfo), verbose=False).reshape(1,)\n",
    "        \n",
    "        current_forecast = best_model.predict(standardCurrentInfo, verbose=False).reshape(1,)\n",
    "        \n",
    "        new_line = np.concatenate((current_info, current_forecast), axis = 0)\n",
    "        \n",
    "        TimeframeTable = pd.concat([TimeframeTable,\n",
    "                                    pd.DataFrame(new_line.reshape(1, -1),\n",
    "                                                 columns = nameColumns,\n",
    "                                                 index = [day])], axis = 0)\n",
    "\n",
    "    index_data = TimeframeTable.index   \n",
    "    TimeframeTable = scaler.inverse_transform(TimeframeTable)\n",
    "    TimeframeTable = pd.DataFrame(TimeframeTable, columns = nameColumns, index = index_data)\n",
    "    TimeframeTable.index = pd.to_datetime(TimeframeTable.index)\n",
    "    \n",
    "    return TimeframeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeframeSP500  = fillTableFrame('SP500', table = SP500_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria tabela de Log-Retorno vazia \n",
    "\n",
    "tableRetLog = pd.DataFrame(index = TimeframeSP500.index,\n",
    "                           columns = ativos).reset_index().rename(columns={'index': 'Dia'})\n",
    "\n",
    "tableRetLog['Dia'] = tableRetLog['Dia'].apply(lambda date: date.strftime('%d/%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popula tabela de log-Retorno e gera gráficos\n",
    "\n",
    "lengthTable = len(tableRetLog)\n",
    "\n",
    "for ativo in ativos:\n",
    "    \n",
    "    TimeframeSPAux = fillTableFrame(ativo)\n",
    "    \n",
    "    #-----------Graphic------------------------------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(TimeframeSPAux.index[:-steps], TimeframeSPAux.iloc[:-steps, -1], linewidth=3.0, c = 'b')\n",
    "    ax.plot(TimeframeSPAux.index[-steps:], TimeframeSPAux.iloc[-steps:, -1], linewidth=3.0, c = 'c', ls = '-')\n",
    "    ax.legend(['Atual', 'Previsão'])\n",
    "    ax.set_title('Preço de Fechamento - {}'.format(ativo))\n",
    "    ax.set(xlabel='Tempo (ano)', ylabel='Preço ($)')\n",
    "    plt.savefig('./graphics/{}-{}.jpg'.format(datetime.now().strftime('%d-%B-%Ih%Mmin'), ativo))\n",
    "    plt.close(fig)\n",
    "    #------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    for n in range(len(forecast)):\n",
    "        tableRetLog.loc[lengthTable-n-1, ativo] = \\\n",
    "        np.log(TimeframeSPAux.iloc[lengthTable-n-1, -1] / TimeframeSPAux.iloc[lengthTable-n-21, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera tabela Entregável do Log-Retorno no padrão\n",
    "\n",
    "tableRetLog.iloc[-len(forecast):, :] \\\n",
    "           .to_csv(path_or_buf = 'LogRetorno/predicao.csv'.format(datetime.now().strftime('%d-%B-%Ih%Mmin')),\n",
    "            index = False,\n",
    "            decimal = '.',\n",
    "            sep=','\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('LogRetorno/predicao.csv', index_col = 'Dia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>WYNN</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dia</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24/10</th>\n",
       "      <td>0.048149</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>0.045011</td>\n",
       "      <td>0.023157</td>\n",
       "      <td>-0.026665</td>\n",
       "      <td>-0.008896</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>0.047552</td>\n",
       "      <td>0.063878</td>\n",
       "      <td>0.125644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075248</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.007230</td>\n",
       "      <td>0.156383</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>0.011389</td>\n",
       "      <td>-0.017414</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.060544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25/10</th>\n",
       "      <td>0.067481</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>0.054616</td>\n",
       "      <td>0.005838</td>\n",
       "      <td>-0.019651</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>0.063450</td>\n",
       "      <td>0.077172</td>\n",
       "      <td>0.125249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049394</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>0.190064</td>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>-0.001310</td>\n",
       "      <td>0.050950</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.085346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26/10</th>\n",
       "      <td>0.079404</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.051816</td>\n",
       "      <td>0.024647</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>-0.002755</td>\n",
       "      <td>0.031393</td>\n",
       "      <td>0.078179</td>\n",
       "      <td>0.090939</td>\n",
       "      <td>0.123845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110472</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.192256</td>\n",
       "      <td>0.041479</td>\n",
       "      <td>0.039998</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.047553</td>\n",
       "      <td>0.027065</td>\n",
       "      <td>0.093721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/10</th>\n",
       "      <td>0.094164</td>\n",
       "      <td>0.098989</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>-0.027595</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0.048856</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>0.097574</td>\n",
       "      <td>0.100733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174184</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.232807</td>\n",
       "      <td>0.066234</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0.034935</td>\n",
       "      <td>0.083853</td>\n",
       "      <td>0.047616</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/10</th>\n",
       "      <td>0.098261</td>\n",
       "      <td>0.136520</td>\n",
       "      <td>0.067396</td>\n",
       "      <td>0.042115</td>\n",
       "      <td>-0.028037</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>0.049220</td>\n",
       "      <td>0.070539</td>\n",
       "      <td>0.100977</td>\n",
       "      <td>0.088302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.073031</td>\n",
       "      <td>0.217569</td>\n",
       "      <td>0.070534</td>\n",
       "      <td>0.050433</td>\n",
       "      <td>0.050920</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.064189</td>\n",
       "      <td>0.093316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31/10</th>\n",
       "      <td>0.103635</td>\n",
       "      <td>0.162285</td>\n",
       "      <td>0.077183</td>\n",
       "      <td>0.037029</td>\n",
       "      <td>-0.015301</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.053414</td>\n",
       "      <td>0.073602</td>\n",
       "      <td>0.104160</td>\n",
       "      <td>0.092305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073583</td>\n",
       "      <td>0.032562</td>\n",
       "      <td>0.092751</td>\n",
       "      <td>0.225905</td>\n",
       "      <td>0.075937</td>\n",
       "      <td>0.060228</td>\n",
       "      <td>0.058516</td>\n",
       "      <td>0.091099</td>\n",
       "      <td>0.075903</td>\n",
       "      <td>0.096663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/11</th>\n",
       "      <td>0.104386</td>\n",
       "      <td>0.124179</td>\n",
       "      <td>0.058873</td>\n",
       "      <td>0.026058</td>\n",
       "      <td>-0.019358</td>\n",
       "      <td>0.016926</td>\n",
       "      <td>0.038342</td>\n",
       "      <td>0.083099</td>\n",
       "      <td>0.101484</td>\n",
       "      <td>0.067426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078088</td>\n",
       "      <td>0.047656</td>\n",
       "      <td>0.070526</td>\n",
       "      <td>0.235903</td>\n",
       "      <td>0.075874</td>\n",
       "      <td>0.068558</td>\n",
       "      <td>0.057314</td>\n",
       "      <td>0.082636</td>\n",
       "      <td>0.083440</td>\n",
       "      <td>0.084043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02/11</th>\n",
       "      <td>0.084560</td>\n",
       "      <td>0.083870</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.036575</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.005353</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.075805</td>\n",
       "      <td>0.075589</td>\n",
       "      <td>0.037396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076795</td>\n",
       "      <td>0.032747</td>\n",
       "      <td>0.034086</td>\n",
       "      <td>0.220444</td>\n",
       "      <td>0.048134</td>\n",
       "      <td>0.062771</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.068031</td>\n",
       "      <td>0.060213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03/11</th>\n",
       "      <td>0.090115</td>\n",
       "      <td>0.121043</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>0.084326</td>\n",
       "      <td>-0.026029</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>0.013023</td>\n",
       "      <td>0.077125</td>\n",
       "      <td>0.086152</td>\n",
       "      <td>0.032508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116172</td>\n",
       "      <td>0.077882</td>\n",
       "      <td>0.034624</td>\n",
       "      <td>0.241107</td>\n",
       "      <td>0.072326</td>\n",
       "      <td>0.068038</td>\n",
       "      <td>0.029546</td>\n",
       "      <td>0.066030</td>\n",
       "      <td>0.089727</td>\n",
       "      <td>0.064505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04/11</th>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.134954</td>\n",
       "      <td>0.070723</td>\n",
       "      <td>0.112947</td>\n",
       "      <td>0.034925</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.023602</td>\n",
       "      <td>0.090555</td>\n",
       "      <td>0.087690</td>\n",
       "      <td>0.040041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137661</td>\n",
       "      <td>0.092073</td>\n",
       "      <td>0.049706</td>\n",
       "      <td>0.268044</td>\n",
       "      <td>0.077483</td>\n",
       "      <td>0.072906</td>\n",
       "      <td>0.051207</td>\n",
       "      <td>0.072115</td>\n",
       "      <td>0.099755</td>\n",
       "      <td>0.078678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07/11</th>\n",
       "      <td>0.057955</td>\n",
       "      <td>0.143150</td>\n",
       "      <td>0.026665</td>\n",
       "      <td>0.081249</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>-0.013104</td>\n",
       "      <td>-0.015641</td>\n",
       "      <td>0.061235</td>\n",
       "      <td>0.056953</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113641</td>\n",
       "      <td>0.060797</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>0.238791</td>\n",
       "      <td>0.043383</td>\n",
       "      <td>0.043744</td>\n",
       "      <td>0.023664</td>\n",
       "      <td>0.029571</td>\n",
       "      <td>0.067239</td>\n",
       "      <td>0.055843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08/11</th>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.057381</td>\n",
       "      <td>-0.000575</td>\n",
       "      <td>0.053638</td>\n",
       "      <td>-0.022898</td>\n",
       "      <td>-0.037839</td>\n",
       "      <td>-0.050787</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.020155</td>\n",
       "      <td>-0.030802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023612</td>\n",
       "      <td>0.038493</td>\n",
       "      <td>-0.037277</td>\n",
       "      <td>0.213163</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>0.019392</td>\n",
       "      <td>-0.017811</td>\n",
       "      <td>-0.010018</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.031687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09/11</th>\n",
       "      <td>0.009220</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.048722</td>\n",
       "      <td>-0.032741</td>\n",
       "      <td>-0.036045</td>\n",
       "      <td>-0.052902</td>\n",
       "      <td>0.023032</td>\n",
       "      <td>0.019683</td>\n",
       "      <td>-0.037867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.063798</td>\n",
       "      <td>-0.076451</td>\n",
       "      <td>0.203195</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>-0.025844</td>\n",
       "      <td>-0.013895</td>\n",
       "      <td>0.029775</td>\n",
       "      <td>0.033157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/11</th>\n",
       "      <td>0.014027</td>\n",
       "      <td>0.072642</td>\n",
       "      <td>0.018248</td>\n",
       "      <td>0.053448</td>\n",
       "      <td>-0.011473</td>\n",
       "      <td>-0.018275</td>\n",
       "      <td>-0.063516</td>\n",
       "      <td>0.034275</td>\n",
       "      <td>0.038753</td>\n",
       "      <td>-0.039057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022237</td>\n",
       "      <td>0.097164</td>\n",
       "      <td>-0.104683</td>\n",
       "      <td>0.231172</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>0.027610</td>\n",
       "      <td>-0.009171</td>\n",
       "      <td>-0.010834</td>\n",
       "      <td>0.046225</td>\n",
       "      <td>0.047828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/11</th>\n",
       "      <td>0.052919</td>\n",
       "      <td>0.118587</td>\n",
       "      <td>0.044120</td>\n",
       "      <td>0.089341</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.022970</td>\n",
       "      <td>0.042991</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>-0.003862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004171</td>\n",
       "      <td>0.123264</td>\n",
       "      <td>-0.093394</td>\n",
       "      <td>0.267301</td>\n",
       "      <td>0.047253</td>\n",
       "      <td>0.039127</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>0.040527</td>\n",
       "      <td>0.083738</td>\n",
       "      <td>0.085510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/11</th>\n",
       "      <td>0.067589</td>\n",
       "      <td>0.132659</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>0.086262</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>-0.001997</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>0.052316</td>\n",
       "      <td>0.088223</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130292</td>\n",
       "      <td>0.112577</td>\n",
       "      <td>-0.068658</td>\n",
       "      <td>0.280155</td>\n",
       "      <td>0.054841</td>\n",
       "      <td>0.050688</td>\n",
       "      <td>0.026295</td>\n",
       "      <td>0.074390</td>\n",
       "      <td>0.092122</td>\n",
       "      <td>0.099357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/11</th>\n",
       "      <td>0.073554</td>\n",
       "      <td>0.119881</td>\n",
       "      <td>0.008332</td>\n",
       "      <td>0.095925</td>\n",
       "      <td>-0.022077</td>\n",
       "      <td>-0.030898</td>\n",
       "      <td>0.038351</td>\n",
       "      <td>0.049841</td>\n",
       "      <td>0.111273</td>\n",
       "      <td>0.017460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203927</td>\n",
       "      <td>0.116188</td>\n",
       "      <td>-0.056801</td>\n",
       "      <td>0.278789</td>\n",
       "      <td>0.055855</td>\n",
       "      <td>0.076749</td>\n",
       "      <td>0.015177</td>\n",
       "      <td>0.112191</td>\n",
       "      <td>0.109512</td>\n",
       "      <td>0.100145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/11</th>\n",
       "      <td>0.077019</td>\n",
       "      <td>0.088208</td>\n",
       "      <td>0.010516</td>\n",
       "      <td>0.099804</td>\n",
       "      <td>-0.011755</td>\n",
       "      <td>-0.010574</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.071206</td>\n",
       "      <td>0.127324</td>\n",
       "      <td>0.017324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220780</td>\n",
       "      <td>0.147760</td>\n",
       "      <td>-0.058549</td>\n",
       "      <td>0.285598</td>\n",
       "      <td>0.080865</td>\n",
       "      <td>0.077257</td>\n",
       "      <td>0.028776</td>\n",
       "      <td>0.073590</td>\n",
       "      <td>0.127997</td>\n",
       "      <td>0.106775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/11</th>\n",
       "      <td>0.062752</td>\n",
       "      <td>0.063479</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.066728</td>\n",
       "      <td>-0.033722</td>\n",
       "      <td>-0.032769</td>\n",
       "      <td>-0.017579</td>\n",
       "      <td>0.052281</td>\n",
       "      <td>0.101633</td>\n",
       "      <td>-0.008540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249778</td>\n",
       "      <td>0.116641</td>\n",
       "      <td>-0.091321</td>\n",
       "      <td>0.253042</td>\n",
       "      <td>0.070802</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.049287</td>\n",
       "      <td>0.087996</td>\n",
       "      <td>0.090893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/11</th>\n",
       "      <td>0.083688</td>\n",
       "      <td>0.062861</td>\n",
       "      <td>0.022805</td>\n",
       "      <td>0.101716</td>\n",
       "      <td>-0.034669</td>\n",
       "      <td>-0.022067</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.072646</td>\n",
       "      <td>0.124639</td>\n",
       "      <td>0.019042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265408</td>\n",
       "      <td>0.135124</td>\n",
       "      <td>-0.063077</td>\n",
       "      <td>0.273459</td>\n",
       "      <td>0.100135</td>\n",
       "      <td>0.072713</td>\n",
       "      <td>0.032303</td>\n",
       "      <td>0.102413</td>\n",
       "      <td>0.101796</td>\n",
       "      <td>0.116777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              A       AAL       AAP      AAPL      ABBV       ABC      ABMD  \\\n",
       "Dia                                                                           \n",
       "24/10  0.048149 -0.010545  0.045011  0.023157 -0.026665 -0.008896  0.017794   \n",
       "25/10  0.067481  0.007024  0.054616  0.005838 -0.019651  0.000113  0.010771   \n",
       "26/10  0.079404  0.061174  0.051816  0.024647 -0.008805 -0.002755  0.031393   \n",
       "27/10  0.094164  0.098989  0.053097  0.028871 -0.027595 -0.010829  0.048856   \n",
       "28/10  0.098261  0.136520  0.067396  0.042115 -0.028037  0.007716  0.049220   \n",
       "31/10  0.103635  0.162285  0.077183  0.037029 -0.015301  0.012167  0.053414   \n",
       "01/11  0.104386  0.124179  0.058873  0.026058 -0.019358  0.016926  0.038342   \n",
       "02/11  0.084560  0.083870  0.030134  0.036575 -0.039192 -0.005353  0.011733   \n",
       "03/11  0.090115  0.121043  0.050526  0.084326 -0.026029  0.011289  0.013023   \n",
       "04/11  0.097656  0.134954  0.070723  0.112947  0.034925  0.016394  0.023602   \n",
       "07/11  0.057955  0.143150  0.026665  0.081249  0.003899 -0.013104 -0.015641   \n",
       "08/11  0.018100  0.057381 -0.000575  0.053638 -0.022898 -0.037839 -0.050787   \n",
       "09/11  0.009220  0.061539  0.001647  0.048722 -0.032741 -0.036045 -0.052902   \n",
       "10/11  0.014027  0.072642  0.018248  0.053448 -0.011473 -0.018275 -0.063516   \n",
       "11/11  0.052919  0.118587  0.044120  0.089341 -0.001248 -0.000229 -0.022970   \n",
       "14/11  0.067589  0.132659  0.018290  0.086262  0.001218 -0.001997  0.019336   \n",
       "15/11  0.073554  0.119881  0.008332  0.095925 -0.022077 -0.030898  0.038351   \n",
       "16/11  0.077019  0.088208  0.010516  0.099804 -0.011755 -0.010574  0.042481   \n",
       "17/11  0.062752  0.063479  0.000385  0.066728 -0.033722 -0.032769 -0.017579   \n",
       "18/11  0.083688  0.062861  0.022805  0.101716 -0.034669 -0.022067  0.018421   \n",
       "\n",
       "            ABT       ACN      ADBE  ...      WYNN       XEL       XOM  \\\n",
       "Dia                                  ...                                 \n",
       "24/10  0.047552  0.063878  0.125644  ...  0.075248 -0.006957 -0.007230   \n",
       "25/10  0.063450  0.077172  0.125249  ...  0.049394  0.004400 -0.000111   \n",
       "26/10  0.078179  0.090939  0.123845  ...  0.110472  0.014440  0.015321   \n",
       "27/10  0.071071  0.097574  0.100733  ...  0.174184  0.009915  0.019015   \n",
       "28/10  0.070539  0.100977  0.088302  ...  0.185200  0.016522  0.073031   \n",
       "31/10  0.073602  0.104160  0.092305  ...  0.073583  0.032562  0.092751   \n",
       "01/11  0.083099  0.101484  0.067426  ...  0.078088  0.047656  0.070526   \n",
       "02/11  0.075805  0.075589  0.037396  ...  0.076795  0.032747  0.034086   \n",
       "03/11  0.077125  0.086152  0.032508  ...  0.116172  0.077882  0.034624   \n",
       "04/11  0.090555  0.087690  0.040041  ...  0.137661  0.092073  0.049706   \n",
       "07/11  0.061235  0.056953  0.002961  ...  0.113641  0.060797 -0.001629   \n",
       "08/11  0.025216  0.020155 -0.030802  ...  0.023612  0.038493 -0.037277   \n",
       "09/11  0.023032  0.019683 -0.037867  ...  0.003346  0.063798 -0.076451   \n",
       "10/11  0.034275  0.038753 -0.039057  ... -0.022237  0.097164 -0.104683   \n",
       "11/11  0.042991  0.077900 -0.003862  ... -0.004171  0.123264 -0.093394   \n",
       "14/11  0.052316  0.088223  0.009767  ...  0.130292  0.112577 -0.068658   \n",
       "15/11  0.049841  0.111273  0.017460  ...  0.203927  0.116188 -0.056801   \n",
       "16/11  0.071206  0.127324  0.017324  ...  0.220780  0.147760 -0.058549   \n",
       "17/11  0.052281  0.101633 -0.008540  ...  0.249778  0.116641 -0.091321   \n",
       "18/11  0.072646  0.124639  0.019042  ...  0.265408  0.135124 -0.063077   \n",
       "\n",
       "           XRAY       XYL       YUM       ZBH      ZBRA      ZION       ZTS  \n",
       "Dia                                                                          \n",
       "24/10  0.156383  0.011237  0.011389 -0.017414  0.038575  0.007273  0.060544  \n",
       "25/10  0.190064  0.027721  0.017930 -0.001310  0.050950  0.017857  0.085346  \n",
       "26/10  0.192256  0.041479  0.039998  0.014659  0.047553  0.027065  0.093721  \n",
       "27/10  0.232807  0.066234  0.052910  0.034935  0.083853  0.047616  0.101100  \n",
       "28/10  0.217569  0.070534  0.050433  0.050920  0.093764  0.064189  0.093316  \n",
       "31/10  0.225905  0.075937  0.060228  0.058516  0.091099  0.075903  0.096663  \n",
       "01/11  0.235903  0.075874  0.068558  0.057314  0.082636  0.083440  0.084043  \n",
       "02/11  0.220444  0.048134  0.062771  0.028708  0.051775  0.068031  0.060213  \n",
       "03/11  0.241107  0.072326  0.068038  0.029546  0.066030  0.089727  0.064505  \n",
       "04/11  0.268044  0.077483  0.072906  0.051207  0.072115  0.099755  0.078678  \n",
       "07/11  0.238791  0.043383  0.043744  0.023664  0.029571  0.067239  0.055843  \n",
       "08/11  0.213163  0.006978  0.019392 -0.017811 -0.010018  0.017007  0.031687  \n",
       "09/11  0.203195  0.009509  0.008488 -0.025844 -0.013895  0.029775  0.033157  \n",
       "10/11  0.231172  0.019316  0.027610 -0.009171 -0.010834  0.046225  0.047828  \n",
       "11/11  0.267301  0.047253  0.039127  0.012772  0.040527  0.083738  0.085510  \n",
       "14/11  0.280155  0.054841  0.050688  0.026295  0.074390  0.092122  0.099357  \n",
       "15/11  0.278789  0.055855  0.076749  0.015177  0.112191  0.109512  0.100145  \n",
       "16/11  0.285598  0.080865  0.077257  0.028776  0.073590  0.127997  0.106775  \n",
       "17/11  0.253042  0.070802  0.041983  0.007850  0.049287  0.087996  0.090893  \n",
       "18/11  0.273459  0.100135  0.072713  0.032303  0.102413  0.101796  0.116777  \n",
       "\n",
       "[20 rows x 503 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resíduos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
